---
title: "MIT RLM(递归语言模型)-克服LLM上下文窗口限制和性能衰减"
date: "2026-01-20T22:18:36+08:00"
draft: false
tags: ["paper", "research"]
categories: ["papers"]
description: "MIT 递归语言模型（RLM）模型，旨在攻克大语言模型在处理超长文本时的上下文衰减与窗口限制难题。"
cover:
  image: "/images/optimized/png/RLM.webp"
  alt: "RLM 示意图"
wordCount: 4845
readingTime: 13
---

## 摘要

这篇研究介绍了一种名为递归语言模型（RLM）的新型推理策略，旨在攻克大语言模型在处理超长文本时的上下文衰减与窗口限制难题。RLM的核心思想是将长提示词视为外部环境而非直接输入，并将其加载至 Python REPL 环境中。通过这种方式，模型能够以编程化手段对文本进行拆解、筛选和递归调用，从而突破数百万词元的限制。实验表明，RLM 在长文本推理任务中的表现显著优于传统的总结代理或检索工具，且成本效益极高。该方法实现了推理侧的计算扩展，为通向无限上下文和复杂长程任务处理开辟了新路径。



{{< pdf-slide src="/pdf/RLM.pdf" title="Recursive Language Models" >}}

## 闪卡回顾

{{< flashcards >}}
{{< flashcard q="在大型语言模型（LLM）的语境中，“上下文窗口”（context window）指的是什么？" >}}它指模型在单次处理中能够接收和处理的最大输入信息量，通常以词元（tokens）为单位计算。{{< /flashcard >}}
{{< flashcard q="“上下文腐烂”（context rot）这一现象描述了什么问题？" >}}它描述了即使在模型的物理上下文窗口限制内，随着输入长度的增加，模型的性能和推理质量也会迅速下降的现象。{{< /flashcard >}}
{{< flashcard q="为什么像“上下文压缩”（context compaction）这样的传统长上下文处理方法被认为是“有损的”？" >}}因为这类方法通过总结来压缩信息，此过程不可避免地会丢失原始上下文中的细节和质量。{{< /flashcard >}}
{{< flashcard q="递归语言模型（RLM）处理长提示的核心思想是什么？" >}}其核心思想是将长提示视为外部环境的一部分，让语言模型以编程方式与之交互，而不是直接将其全部输入神经网络。{{< /flashcard >}}
{{< flashcard q="在递归语言模型（RLM）的架构中，长提示通常被加载到一个 _____ 环境中，并被赋值给一个变量。" >}}Python REPL（读取-求值-打印循环）{{< /flashcard >}}
{{< flashcard q="RLM框架中的“递归”部分指的是模型的哪种能力？" >}}它指的是模型能够对其认为相关的上下文片段再次调用自身，进行更深层次、更集中的查询和分析的能力。{{< /flashcard >}}
{{< flashcard q="根据视频描述，RLM方法与常见的RAG（检索增强生成）有何相似之处？" >}}两者都涉及到在给定上下文中进行搜索，但RLM是在一个为当前任务创建的动态环境中对整个提示进行内部搜索。{{< /flashcard >}}
{{< flashcard q="“大海捞针”（Needle-in-a-Haystack）基准测试是如何评估长上下文模型的？" >}}它通过将一个特定的信息片段（“针”）嵌入到大量无关文本（“草堆”）中，然后要求模型准确地找回该信息。{{< /flashcard >}}
{{< flashcard q="OOLONG基准测试的设计目的是评估模型的哪两种核心能力？" >}}它旨在评估模型的长推理能力和对输入文本块进行语义转换及聚合的能力。{{< /flashcard >}}
{{< flashcard q="OOLONG-Pairs基准测试与OOLONG有何不同，其处理成本如何随输入长度变化？" >}}它特别要求聚合输入数据块的“对”，因此其处理成本相对于输入长度呈二次方级（quadratically）增长。{{< /flashcard >}}
{{< flashcard q="BrowseComp-Plus是一个什么样的基准测试？" >}}它是一个多跳问答（multi-hop question-answering）基准测试，要求模型在多个不同文档中推理以找到答案。{{< /flashcard >}}
{{< flashcard q="LongBench-v2 CodeQA基准测试专注于评估模型在哪一方面的能力？" >}}它专注于评估模型对大型代码库的理解能力，例如理解不同函数的功能和它们之间的调用关系。{{< /flashcard >}}
{{< flashcard q="研究得出的第一个关键观察是，RLM的性能和扩展能力如何？" >}}RLM可以成功扩展到处理千万级词元的输入，并且在长上下文任务上的表现显著优于基础模型和常见的代理框架。{{< /flashcard >}}
{{< flashcard q="在RLM中，_____ 对于处理长输入是必要的，而 _____ 对于处理信息密集的输入提供了强大的优势。" >}}REPL环境，递归子调用{{< /flashcard >}}
{{< flashcard q="与基础语言模型相比，RLM的性能如何随着输入长度和问题复杂度的增加而变化？" >}}基础模型的性能会随着长度和复杂度的增加而显著下降，而RLM的性能扩展性更好，下降速度要慢得多。{{< /flashcard >}}
{{< flashcard q="RLM的推理成本与基础模型相比有何特点？" >}}其平均成本与基础模型相当或更低，但由于处理轨迹长度不同而表现出高方差，可能导致少数情况下成本激增。{{< /flashcard >}}
{{< flashcard q="“RLM是一种模型无关的推理策略”这句话意味着什么？" >}}这意味着该策略可以应用于任何现有的语言模型，而无需改变模型的核心架构或进行重新训练。{{< /flashcard >}}
{{< flashcard q="RLM在与长上下文交互时，展现出的一种关键“涌现行为”是什么？" >}}它能够通过执行代码（如正则表达式搜索）来探测和过滤上下文，而无需将全部内容加载到其核心推理空间中。{{< /flashcard >}}
{{< flashcard q="当面对一个极长的输出任务时，RLM如何生成超出单个模型上下文窗口限制的答案？" >}}它可以通过在REPL环境中将子问题的答案存储在变量中，并最后将这些变量拼接成最终的、可能非常长的输出。{{< /flashcard >}}
{{< flashcard q="在研究中，将哪两个模型用于评估RLM方法？" >}}一个前沿的闭源模型（GPT-5）和一个前沿的开源模型（Qwen3-Coder-480B-A35B）。{{< /flashcard >}}
{{< flashcard q="RLM性能的一个权衡点是什么？" >}}在处理较短的输入上下文时，基础模型的性能可能略优于RLM，这表明在何时使用RLM存在一个权衡点。{{< /flashcard >}}
{{< flashcard q="与将整个输入上下文提供给模型的“摘要基线”（summarization baseline）相比，RLM的成本效益如何？" >}}RLM的成本最高可便宜三倍，同时在所有任务上保持更强的性能，因为它能选择性地查看上下文。{{< /flashcard >}}
{{< flashcard q="研究人员在论文中将RLM的灵感来源与计算机科学中的什么概念类比？" >}}核外算法（out-of-core algorithms），这类算法通过巧妙管理数据加载，用较小的主内存处理远大于内存的数据集。{{< /flashcard >}}
{{< flashcard q="物理上下文窗口和有效上下文窗口之间有什么区别？" >}}物理窗口是模型支持的最大词元数，而有效窗口是模型在特定任务上能够保持高性能的实际上下文长度，通常远小于物理窗口。{{< /flashcard >}}
{{< flashcard q="在RLM框架中，模型如何处理它认为是“相关”的上下文片段？" >}}模型会以编程方式构建针对该片段的子任务，并递归地调用自身来解决这些子任务。{{< /flashcard >}}
{{< flashcard q="在BrowseComp-Plus任务中，RLM(GPT-5)相比其摘要和检索基线，性能和成本表现如何？" >}}它的性能比基线高出29%以上，而平均成本却更低。{{< /flashcard >}}
{{< flashcard q="RLM方法的一个主要局限性是什么？" >}}由于它允许模型进行迭代交互直到找到答案，导致处理轨迹的长度差异很大，从而引起推理成本和运行时间的高方差。{{< /flashcard >}}
{{< flashcard q="研究论文指出，未来可以通过什么方式进一步提升RLM的性能？" >}}通过专门训练模型来作为RLM使用（例如作为根模型或子模型），以提高其在上下文管理和决策上的效率。{{< /flashcard >}}
{{< flashcard q="RLM如何实现对答案的验证？" >}}它可以通过代码以编程方式验证答案，或使用上下文较小的子语言模型调用来隐式地避免上下文腐烂并进行验证。{{< /flashcard >}}
{{< flashcard q="在CodeAct基线方法中，提示是如何提供给语言模型的？" >}}与RLM不同，CodeAct将提示直接提供给语言模型，而不是将其卸载到代码环境中。{{< /flashcard >}}
{{< flashcard q="为什么说S-NIAH（单针捞针）任务对于现代前沿模型来说基本上已经解决了？" >}}因为这类任务的复杂度不随上下文长度增加而增加（“针”的大小是恒定的），所以前沿模型能够可靠地解决它们。{{< /flashcard >}}
{{< flashcard q="在GPT-5的成本图中，为什么95百分位的成本会出现一个巨大的尖峰？" >}}这反映了RLM处理复杂任务时可能产生非常长的推理轨迹，导致少数查询的成本异常高，体现了其高方差特性。{{< /flashcard >}}
{{< flashcard q="RLM在处理大型代码库时采用的一种常见策略是什么？" >}}将代码库分解成多个部分（块），对每个块进行子查询以寻找线索，然后聚合这些线索以得出最终答案。{{< /flashcard >}}
{{< flashcard q="研究人员在为不同模型设计RLM系统提示时发现了什么问题？" >}}完全相同的系统提示在不同模型上可能导致不同的、不期望的行为，需要进行微调（如为Qwen3-Coder添加警告以防止过多子调用）。{{< /flashcard >}}
{{< flashcard q="RLM如何处理那些需要组合来自上下文不同部分信息才能回答的问题？" >}}它通过对不同部分进行独立的查询，然后利用其工作环境（如REPL中的变量）来组合和整合从各处找到的信息。{{< /flashcard >}}
{{< flashcard q="在RLM框架下，语言模型的核心智能被视为什么？" >}}它被视为一个可以被外部“脚手架”（scaffolding）和工具增强的核心智能模块。{{< /flashcard >}}
{{< flashcard q="根据论文，信息密度（information density）如何影响任务的复杂性？" >}}信息密度指代理需要处理多少信息来完成任务，密度越高或随输入规模增长越快，任务就越复杂。{{< /flashcard >}}
{{< flashcard q="RLM中的REPL环境除了加载上下文，还提供了一个关键函数叫什么？" >}}一个名为 `llm_query` 的函数，允许在REPL环境中递归地查询一个子语言模型。{{< /flashcard >}}
{{< flashcard q="在RLM的GPT-5实验中，研究人员为根LM和递归LM分别选择了哪个模型以平衡能力和成本？" >}}根LM使用GPT-5，而递归（子）LM使用GPT-5-mini。{{< /flashcard >}}
{{< flashcard q="在研究中，不带子调用的RLM消融实验（ablation）证明了什么？" >}}仅仅将上下文卸载到REPL环境中，即使没有递归调用，也能够扩展模型处理能力超出其上下文限制，并优于基线。{{< /flashcard >}}
{{< flashcard q="在信息密集的OOLONG-Pairs任务上，RLM相比基础模型取得了多大的性能提升？" >}}基础模型几乎无法解决该任务（F1分数<0.1%），而RLM使用这些模型后能达到显著更高的F1分数（例如GPT-5达到58%）。{{< /flashcard >}}
{{< flashcard q="RLM的运行时间较慢的一个主要实现原因是什么，以及如何解决？" >}}原因是所有子语言模型调用都是阻塞/顺序执行的；通过实现异步调用可以显著减少运行时间。{{< /flashcard >}}
{{< flashcard q="研究人员观察到，在解决OOLONG任务时，Qwen3-Coder模型在子调用使用上与GPT-5有何不同？" >}}Qwen3-Coder倾向于对每一行都进行一次单独的子LM调用来进行语义转换，而GPT-5在子查询方面则更为保守。{{< /flashcard >}}
{{< flashcard q="RLM如何利用模型的先验知识来提高效率？" >}}模型可以利用其先验知识生成相关的搜索关键词（如地名“La Union”），从而缩小搜索范围，减少需要处理的词元数量。{{< /flashcard >}}
{{< flashcard q="在RLM的轨迹中，模型通常采用什么策略来分解信息密集的上下文？" >}}常见的策略是按行或关键词进行分块，然后对每个块进行递归的子LM调用。{{< /flashcard >}}
{{< flashcard q="在区分“下一步思考”和“最终答案”时，RLM实现面临的一个脆弱点是什么？" >}}它依赖于模型将答案包装在 `FINAL()` 或 `FINAL_VAR()` 标签中，但模型有时会做出奇怪的决定，比如将计划输出为最终答案。{{< /flashcard >}}
{{< flashcard q="实验表明，哪类基础模型作为RLM时会表现不佳？" >}}编码能力不足的模型和最大输出词元数不足的模型（因为思考过程可能会耗尽输出限制）。{{< /flashcard >}}
{{< flashcard q="在BrowseComp-Plus任务中，随着文档数量从10增加到1000，RLM(GPT-5)的性能表现如何？" >}}它几乎没有性能下降，是唯一一个在1000个文档规模下仍能保持近乎完美性能的方法。{{< /flashcard >}}
{{< flashcard q="RLM的研究方向属于长上下文管理的哪一个主要分支？" >}}它属于在不改变模型架构的情况下，围绕语言模型构建“脚手架”（scaffold）来隐式处理上下文的分支。{{< /flashcard >}}
{{< flashcard q="RLM与其他任务分解方法（如THREAD, AgentFold）的关键区别在于什么？" >}}关键区别在于RLM通过将提示置于外部环境，使其能够处理远超基础模型上下文窗口的输入，而其他方法不能。{{< /flashcard >}}
{{< flashcard q="根据Matthew Berman的观点，围绕模型核心智能构建_____是提升模型能力的一个巨大增长点。" >}}更多的工具和脚手架（tooling and scaffolding）{{< /flashcard >}}
{{< flashcard q="在YouTube视频的图表中，GPT-5在S-NIAH任务上表现稳定，但在OOLONG任务上性能迅速下降，这说明了什么？" >}}这说明了模型处理长上下文的能力不仅取决于长度，还高度依赖于任务的复杂性。{{< /flashcard >}}
{{< flashcard q="当RLM处理一个需要它从头构建一个复杂答案（例如一个包含数千个数据对的列表）的任务时，它是如何管理这个过程的？" >}}它会在REPL环境中以编程方式逐步构建答案，例如创建一个列表变量，并通过循环和子调用来填充它，最后返回该变量。{{< /flashcard >}}
{{< flashcard q="在RLM的实现中，为了让模型能够处理长达1000万词元的上下文，这个上下文被存放在哪里？" >}}它被存放在一个Python环境中，如同一个文本文件或一个巨大的字符串变量。{{< /flashcard >}}
{{< flashcard q="当一个基础模型因输入超出其上下文窗口而失败时，RLM在同样任务上的表现通常是怎样的？" >}}RLM通常能够有效处理这些超出基础模型限制的任务，并保持强大的性能。{{< /flashcard >}}
{{< flashcard q="为什么说RLM的成本效益很高，尤其是在处理大型文档语料库时？" >}}因为它不需要每次都将全部数百万词元的上下文加载到模型中，而是通过代码有选择地、迭代地检查上下文的相关部分。{{< /flashcard >}}
{{< flashcard q="根据论文，RLM轨迹可以被看作是一种形式的_____，这为未来的模型训练提供了思路。" >}}推理（reasoning）{{< /flashcard >}}
{{< flashcard q="在研究中，Summary agent（摘要代理）是如何处理超出其上下文窗口的输入的？" >}}它会将输入分块以适应模型上下文窗口，并对这些块应用迭代的摘要策略。{{< /flashcard >}}
{{< flashcard q="CodeAct (+ BM25)基线中的BM25检索器在什么任务中被使用？" >}}它在输入可以被索引的任务中被使用，例如在BrowseComp+任务中检索相关文档。{{< /flashcard >}}
{{< flashcard q="在OOLONG任务上，RLM(Qwen3-Coder)相比其基础模型性能提升了多少？" >}}性能提升了33.3%。{{< /flashcard >}}
{{< flashcard q="在RLM的BrowseComp-Plus任务轨迹示例中，GPT-5的第一步是什么？" >}}它首先使用正则表达式查询来探测包含某些关键词（基于先验知识和提示）的文档片段。{{< /flashcard >}}
{{< flashcard q="在RLM的OOLONG-Pairs轨迹示例中，Qwen3-Coder是如何对数据进行分类的？" >}}它将输入数据分批处理，并对每一批次使用递归的LM调用来进行语义分类。{{< /flashcard >}}
{{< flashcard q="RLM的哪个特性使其能够处理输入和输出都非常长的任务？" >}}将输入视为外部环境中的变量，并通过编程和子调用逐步构建存储在变量中的输出。{{< /flashcard >}}
{{< flashcard q="根据论文，为什么简单地增加Transformer的上下文长度可能不是扩展到千万级词元规模的最佳方案？" >}}因为即使在现有上下文长度内，模型也会表现出“上下文腐烂”，表明直接扩展可能无法解决根本的性能下降问题。{{< /flashcard >}}
{{< flashcard q="RLM框架是如何避免像摘要方法那样的信息丢失问题的？" >}}它不进行任何形式的摘要或压缩，而是保留完整的原始上下文，并允许模型根据需要访问任何部分。{{< /flashcard >}}
{{< flashcard q="研究论文对未来的工作提出了什么建议，以优化RLM的运行效率？" >}}建议探索替代策略，如使用异步子调用和沙盒化的REPL环境，以可能显著减少运行时间和推理成本。{{< /flashcard >}}
{{< flashcard q="在RLM框架下，模型从被动的信息处理器转变为一个怎样的角色？" >}}它转变为一个主动的代理，能够以编程方式检查、分解和操作其环境中的信息。{{< /flashcard >}}
{{< flashcard q="在RLM轨迹中观察到的“答案验证”行为是如何进行的？" >}}模型在找到一个可能的答案后，会再次发起子LM调用，用更简洁的提示来确认或提取关键信息，以验证其正确性。{{< /flashcard >}}
{{< flashcard q="RLM在什么情况下可能不是最佳选择？" >}}在处理输入上下文相对较短且任务不复杂的情况下，直接调用基础模型可能更高效且性能相当或更好。{{< /flashcard >}}
{{< flashcard q="在YouTube视频的图表中，即使应用了RLM策略，为什么在OOLONG-Pairs任务上的得分仍会随着上下文长度的增加而下降？" >}}尽管下降速度远慢于基础模型，但任务固有的二次方级复杂度意味着随着输入规模的增长，解决难度仍然会增加，导致性能有所下降。{{< /flashcard >}}
{{< flashcard q="RLM方法如何为解决“长时程任务”（long-horizon tasks）提供支持？" >}}通过提供处理数千万词元输入的能力，它使语言模型能够处理需要跨越巨大信息范围进行推理和规划的复杂任务。{{< /flashcard >}}
{{< /flashcards >}}

## 参考文献
* 论文：https://arxiv.org/pdf/2512.24601
* 解读：https://www.youtube.com/watch?v=huszaaJPjU8