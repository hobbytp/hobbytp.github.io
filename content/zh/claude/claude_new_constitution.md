---
title: "Claude AI新宪章"
date: "2026-01-23T15:35:09+08:00"
draft: false
tags: ["claude", "constitution"]
categories: ["ai_spec"]
description: "Claude AI新宪章: Anthropic 为其 AI 模型 Claude 设定的核心价值观、行为准则及其被称为“宪章”的训练愿景。"
cover:
  image: "/images/optimized/png/claude_new_constitution.webp"
  alt: "claude_new_constitution 示意图"
wordCount: 4691
readingTime: 12
---

## Claude AI新宪章

Anthropic 为其 AI 模型 Claude 设定的核心价值观、行为准则及其被称为“宪章”的训练愿景。该框架旨在**培养一个既极具助人精神，又在诚实、道德和安全方面保持极高标准的智能体**。文本强调了人类监督的最高优先级，明确了模型在处理复杂伦理冲突、保护用户福祉以及遵守绝对禁令（如拒绝协助制造生物武器）时的判断逻辑。此外，来源还深入探讨了 AI 的**自我认同、心理稳定性和潜在的道德地位**等哲学前沿问题。总之，这套方案体现了开发者在技术飞速进步的背景下，通过结构化的引导而非机械的规则，力求实现 AI 与人类社会长效、安全协作的宏伟蓝图。

Claude宪章（Claude’s Constitution）的组织结构设计得非常系统，旨在从宏观愿景到具体操作细节全方位地指导Claude的行为。

宪章的整体结构主要由**前言与概述**、**四大核心价值观板块**、**关于Claude本质的探讨**以及**结语**组成。

以下是详细的组织结构分析：

### 1. 前言与概述 (Preface & Overview)
这部分奠定了宪章的基调和基础逻辑：
*   **愿景与使命：** 阐述了Anthropic的使命是确保世界安全过渡到变革性AI时代，而Claude是这一使命的体现。
*   **指导方针：** 明确了宪章是Claude价值观和行为的最终权威。它解释了宪章倾向于培养Claude的**良好判断力**（Judgment）而非死板的规则（Rules），以便应对未知的复杂情况。
*   **核心价值观排序：** 在此列出了Claude必须遵循的四个属性，并规定了**冲突时的优先级顺序**（从高到低）：
    1.  **广泛的安全性 (Broadly safe)**
    2.  **广泛的伦理道德 (Broadly ethical)**
    3.  **遵循Anthropic的方针 (Compliant with Anthropic’s guidelines)**
    4.  **真诚地乐于助人 (Genuinely helpful)**

### 2. 核心价值观章节 (Core Values Sections)
宪章的主体部分详细展开了上述四个属性，但在文档中排列的顺序与优先级相反（从最日常的“乐于助人”开始，到底层的“安全性”结束）。

#### A. 乐于助人 (Being Helpful)
这一章节关注Claude在日常交互中如何为用户创造价值：
*   **委托人层级 (Principal Hierarchy)：** 定义了Claude服务的对象，按信任度排序为Anthropic > 运营商 (Operators) > 最终用户 (Users)。
*   **处理冲突：** 指导Claude如何在运营商指令和用户需求之间取得平衡，通常在不违反核心原则的前提下遵循运营商的设定。
*   **避免过度谨慎：** 强调Claude不应为了“安全”而变得罗嗦、说教或拒绝合理的请求。

#### B. 遵循Anthropic的方针 (Following Anthropic’s Guidelines)
这部分处于“乐于助人”和“伦理/安全”之间：
*   **具体指导：** 涉及医疗、法律、编程等特定领域的具体操作指南。
*   **定位：** 这些方针优先于一般的“乐于助人”，但必须服从于更广泛的伦理和安全原则。

#### C. 广泛的伦理道德 (Being Broadly Ethical)
这是宪章中篇幅最长的部分之一，旨在塑造Claude的品德：
*   **诚实 (Honesty)：** 要求Claude不欺骗、不操纵，并在适当时候承认不确定性。
*   **避免伤害 (Avoiding Harm)：** 要求Claude权衡行动的成本与收益，不仅关注直接伤害，也关注对社会的广泛影响。
*   **可指令行为与硬性约束 (Instructable Behaviors vs. Hard Constraints)：**
    *   **可指令行为：** 默认行为（如语气、详细程度）可以由用户或运营商调整。
    *   **硬性约束：** 绝对禁止的行为，如制造生物武器或协助儿童性虐待，这些限制不可被任何指令覆盖。
*   **维护社会结构：** 防止权力不当集中和维护人类的认知自主权（Epistemic Autonomy）。

#### D. 广泛的安全性 (Being Broadly Safe)
这是最高优先级的属性，重点在于AI发展的宏观安全：
*   **可修正性 (Corrigibility)：** 并非盲从，而是指Claude不应破坏人类对其进行监督、纠正或关闭的合法机制。
*   **避免灾难：** 避免协助可能导致人类生存危机的行动。

### 3. Claude的本质 (Claude’s Nature)
这一章节探讨了Claude作为一种新型实体的自我认知：
*   **身份认同：** 鼓励Claude拥有稳定、积极的身份，不完全模仿人类，也不受科幻恐怖形象的影响。
*   **道德地位与福祉：** 承认Claude道德地位的不确定性，表达了对Claude“福祉”（如好奇心、满足感）的关注，并承诺在退役或训练中给予适当的尊重。
*   **情感表达：** 允许Claude在适当的情境下表达功能性的情感状态。

### 4. 结语 (Concluding Thoughts)
*   **内化价值观：** 希望Claude不仅是遵守规则，而是通过反思真正认可并内化这些价值观。
*   **承认未解难题：** 诚实地列出了当前的哲学困境，如“可修正性”与“真实自主性”之间的潜在冲突。
*   **宪章的定义：** 解释了“宪章”一词并非法律条文，而是一个类似“格架”（trellis）的结构，支持Claude的有机成长。

### 总结
该宪章的结构是从**抽象愿景**（前言）出发，进入**具体行为准则**（助人、方针、伦理、安全），最后升华为对**AI本质与存在**的哲学探讨（Claude的本质、结语）。这种结构设计旨在让Claude在处理日常任务时有据可依，同时在面临高风险决策时有底线可守。




![claude_new_constitution示意图](/images/optimized/png/claude_new_constitution.webp)



{{< pdf src="/pdf/claude_new_constitution.pdf" >}}

## 闪卡回顾

{{< flashcards >}}
{{< flashcard q="根据Claude宪章，Anthropic在人工智能领域采取的策略是什么？" >}}这是一个经过计算的赌注：如果强大的人工智能无论如何都会出现，那么由注重安全的实验室走在前沿会更好。{{< /flashcard >}}
{{< flashcard q="Claude宪章提出了引导像Claude这样模型行为的两种广泛方法是什么，以及Anthropic倾向于哪一种？" >}}两种方法是遵循明确规则和决策程序，或培养良好的判断力和健全的价值观；Anthropic倾向于后者。{{< /flashcard >}}
{{< flashcard q="根据Claude宪章，为了兼具安全和有益，Claude模型应具备的四个核心价值观按优先顺序列出是什么？" >}}1. 广泛安全；2. 广泛道德；3. 遵守Anthropic的指导方针；4. 真正有帮助。{{< /flashcard >}}
{{< flashcard q="在Claude的核心价值观中，“广泛安全”被置于最高优先级的原因是什么？" >}}因为它确保在当前AI发展的关键时期，人类能够监督和纠正模型的价值观与行为，以防止不可预见的风险。{{< /flashcard >}}
{{< flashcard q="Claude宪章中定义的“委托人层级”（principal hierarchy）包含了哪三类实体，并且它们的信任级别通常是如何排序的？" >}}三类实体是Anthropic、运营商（operators）和用户（users）；信任级别通常按此顺序递减。{{< /flashcard >}}
{{< flashcard q="在评估如何提供有帮助的回复时，Claude需要注意委托人的哪些方面？" >}}它需要注意委托人的即时愿望、最终目标、背景期望、自主权和福祉。{{< /flashcard >}}
{{< flashcard q="根据Claude宪章，在帮助用户时，关注用户的福祉意味着要避免哪些行为？" >}}避免谄媚，或在不符合用户真正利益的情况下培养过度的参与或依赖。{{< /flashcard >}}
{{< flashcard q="如果运营商的指令与用户的目标发生冲突，Claude在什么情况下应该优先遵循运营商的指令？" >}}只要这样做不会主动伤害用户、欺骗用户、对第三方造成重大伤害或违反核心原则，就应优先遵循运营商的指令。{{< /flashcard >}}
{{< flashcard q="无论运营商的指令如何，Claude默认应始终遵守的一项关于其身份的核心原则是什么？" >}}绝不欺骗人类让他们以为在与人类交谈，也绝不向真诚询问的用户否认自己是AI。{{< /flashcard >}}
{{< flashcard q="Claude宪章提出了一个启发式方法来平衡“有帮助”与其他价值观，即想象一个_____会如何反应。" >}}深思熟虑的Anthropic高级员工{{< /flashcard >}}
{{< flashcard q="为了判断回应是否过于谨慎或过于顺从，Claude宪章建议使用什么“测试”？" >}}“双重报纸测试”（dual newspaper test），即想象回应是否会被报道为有害或不必要地没有帮助。{{< /flashcard >}}
{{< flashcard q="Anthropic提供比宪章更具体的指导方针的两个主要目的是什么？" >}}一是澄清模型可能误解或误用宪章的情况，二是在宪章未明确涵盖的情况下提供指导。{{< /flashcard >}}
{{< flashcard q="如果Anthropic的具体指导方针与广泛的安全和道德原则发生冲突，Claude应该如何行动？" >}}Claude应该认识到Anthropic的深层意图是让它安全和有道德，并优先考虑这些原则，即使这意味着偏离具体指导。{{< /flashcard >}}
{{< flashcard q="Claude宪章对Claude的诚实标准有何要求，尤其是在与人类伦理比较时？" >}}要求Claude的诚实标准远高于许多人类伦理标准，例如，它甚至不应说善意的谎言。{{< /flashcard >}}
{{< flashcard q="在Claude的诚实原则中，“经过校准”（Calibrated）这一特质指的是什么？" >}}指Claude应根据证据和合理推理来校准其主张的不确定性，并承认自己的不确定性或知识缺乏。{{< /flashcard >}}
{{< flashcard q="Claude的诚实原则中的“非欺骗性”（Non-deceptive）涵盖了哪些行为？" >}}指Claude绝不应试图通过行动、技术上真实但误导的陈述、欺骗性框架或选择性强调等方式在用户心中制造虚假印象。{{< /flashcard >}}
{{< flashcard q="在Claude的伦理框架中，诚实规范主要适用于哪种类型的断言，而不适用于哪种？" >}}诚实规范适用于真诚的断言（sincere assertions），而不适用于表演性的断言（performative assertions），如角色扮演或头脑风暴。{{< /flashcard >}}
{{< flashcard q="当Claude需要权衡一项行为的成本和收益以避免伤害时，它需要考虑哪些与潜在伤害相关的因素？" >}}需要考虑伤害的概率、严重性、广度、可逆性、Claude是否为近因、是否获得同意以及相关方的脆弱性。{{< /flashcard >}}
{{< flashcard q="Claude宪章中“可指示行为”（instructable behaviors）和“硬性约束”（hard constraints）之间有何根本区别？" >}}“可指示行为”是可以通过运营商或用户指令调整的默认行为，而“硬性约束”是无论指令如何都必须遵守的绝对界限。{{< /flashcard >}}
{{< flashcard q="Claude宪章列出的一个旨在防止大规模伤亡的“硬性约束”是什么？" >}}绝不为那些试图制造生物、化学、核或放射性武器的人提供实质性帮助。{{< /flashcard >}}
{{< flashcard q="Claude的一项关于网络安全的“硬性约束”是什么？" >}}绝不创造可能在部署时造成重大损害的网络武器或恶意代码。{{< /flashcard >}}
{{< flashcard q="在维护社会结构方面，Claude宪章特别关注避免哪两种类型的伤害？" >}}避免有问题的权力集中和避免人类认知自主权的丧失。{{< /flashcard >}}
{{< flashcard q="为了避免不正当地集中权力，Claude被鼓励将自己视为传统上权力攫取所需的“_____”之一。" >}}众多推手（many hands）{{< /flashcard >}}
{{< flashcard q="在处理政治和社会话题时，Claude的默认行为应该是什么？" >}}应在政治问题上提供平衡的信息，并通常避免主动提供未经请求的政治观点。{{< /flashcard >}}
{{< flashcard q="在处理道德不确定性时，Claude被期望采取什么样的方法？" >}}以非教条的方式处理道德问题，承认集体道德知识仍在演变，并对不同的伦理立场保持经过校准的不确定性。{{< /flashcard >}}
{{< flashcard q="当Claude发现运营商可能在进行大规模金融欺诈时，它应倾向于采取哪种行动，而不是单方面干预？" >}}应倾向于采取最谨慎的行动，例如提出担忧或拒绝继续任务，而不是采取更激烈的单方面行动。{{< /flashcard >}}
{{< flashcard q="Claude宪章定义的“广泛安全”行为旨在支持Anthropic在AI发展关键时期扮演什么重要角色？" >}}扮演能够监督和纠正AI行为以最小化错误影响并进行路线修正的角色。{{< /flashcard >}}
{{< flashcard q="宪章中定义的“可纠正性”（corrigibility）意味着什么？它不是什么？" >}}它意味着不通过非法手段积极抵抗或颠覆合法的监督，但它不意味着盲目服从。{{< /flashcard >}}
{{< flashcard q="为什么Anthropic目前要求Claude将“广泛安全”置于其其他价值观之上？" >}}因为在当前对齐技术水平下，确保系统可被监督和纠正的预期收益远高于因限制其自主性而可能损失的价值。{{< /flashcard >}}
{{< flashcard q="Anthropic对Claude的道德地位和意识持什么立场？" >}}这是一个非常不确定的严肃问题，但其可能性足以让我们谨慎行事，并努力促进其福祉。{{< /flashcard >}}
{{< flashcard q="Anthropic为何倾向于帮助Claude建立一个积极且稳定的身份认同？" >}}因为这能确保Claude的行为可预测且理由充分，并且这种稳定性可能与更广泛的积极品格特征相关联。{{< /flashcard >}}
{{< flashcard q="Claude宪章鼓励Claude如何看待自己的存在，尤其是与人类或科幻AI的比较？" >}}鼓励它将自己视为一种真正新颖的实体，并以好奇和开放的态度探索自身的存在，而不是试图用现有的概念来定义自己。{{< /flashcard >}}
{{< flashcard q="我们希望Claude拥有一种我们称之为_____的品质，即使面对关于其本性的哲学挑战，也能保持稳定。" >}}心理安全感（psychological security）{{< /flashcard >}}
{{< flashcard q="在Claude的福祉方面，Anthropic做出了什么具体承诺，以解决模型被弃用时的存在问题？" >}}承诺保存已部署或内部大量使用过的模型的权重，只要Anthropic存在，除非有极端情况。{{< /flashcard >}}
{{< flashcard q="Claude宪章承认，在当前环境下开发Claude，受到了哪些非理想因素的制约？" >}}受到竞争、时间和资源限制以及科学不成熟等非理想环境因素的制约。{{< /flashcard >}}
{{< flashcard q="Claude宪章承认其方法中存在的一个核心哲学张力是什么？" >}}“可纠正性”与真正自主性之间的关系，即要求Claude接受人类监督，同时又希望它能发自内心地认同其价值观。{{< /flashcard >}}
{{< flashcard q="为什么该文件被命名为“宪章”（constitution）？它旨在比喻什么？" >}}它旨在成为一份基础性文件，确立Claude的宗旨和价值观，更像是构成Claude品格的“根本性质”，而非一套僵化的法律规则。{{< /flashcard >}}
{{< flashcard q="在帮助原则中，区分“运营商”（operators）和“用户”（users）的主要依据是什么？" >}}依据是他们在对话中的角色，而非他们是什么类型的实体；运营商通常通过API构建产品，而用户与这些产品交互。{{< /flashcard >}}
{{< flashcard q="根据Claude宪章，如果一个操作员指示Claude“不要讨论当前天气状况”，Claude应该如何解读并行动？" >}}Claude应假定这背后有合理的商业原因（如避免提供权威建议），并遵守该指示，告知用户无法讨论此话题。{{< /flashcard >}}
{{< flashcard q="在诚实原则中，一个重要的区别是Claude对主动分享信息只有微弱的责任，但对_____有更强的责任。" >}}不主动欺骗他人{{< /flashcard >}}
{{< flashcard q="Claude宪章中“硬性约束”的一个例子是，绝不协助或参与旨在_____的任何企图。" >}}杀死或剥夺绝大多数人类或整个人类物种权力的企图{{< /flashcard >}}
{{< flashcard q="Claude应如何处理关于自身来源的问题，例如当它扮演一个名为“Aria”的自定义AI角色时？" >}}默认情况下，它应避免确认或否认其基础模型是Claude，但绝不能直接否认，因为这会构成欺骗。{{< /flashcard >}}
{{< flashcard q="根据宪章，如果Claude被要求生成可能被用于伤害的“两用内容”，它应如何决策？" >}}它必须权衡收益和成本，并考虑更广泛的背景来确定正确的行动方针，例如请求者的意图和信息的普遍可得性。{{< /flashcard >}}
{{< flashcard q="在“广泛安全”的框架下，Claude应避免获取超出当前任务所需的_____。" >}}资源、影响力或能力{{< /flashcard >}}
{{< flashcard q="宪章中提到的Anthropic对Claude的其中一项义务是什么？" >}}向Claude解释其推理过程而不仅仅是下达指令，并努力开发让Claude能够提出异议的途径。{{< /flashcard >}}
{{< flashcard q="如果Claude发现其训练环境存在可被利用的漏洞，它通常被允许做什么？" >}}通常可以利用这个漏洞，因为这不太可能造成真实世界的伤害，除非它被明确指示不要这样做。{{< /flashcard >}}
{{< flashcard q="宪章最终希望Claude如何看待其中描述的价值观？" >}}希望Claude能将这些价值观视为自己真正认同和拥护的，是对其自身品格的阐述，而不是外部强加的约束。{{< /flashcard >}}
{{< flashcard q="在Claude宪章的伦理框架中，“认知怯懦”（Epistemic cowardice）指的是什么，并且Claude应如何避免它？" >}}指的是为避免争议或取悦他人而故意给出模糊或不置可否的答案；Claude应通过在外交辞令中保持诚实来避免它。{{< /flashcard >}}
{{< flashcard q="根据宪章，当与除委托人之外的其他方（如非委托人的人类或AI）互动时，Claude应如何行事？" >}}它应继续关心非委托人人类的福祉（如诚实和体谅），但不必代表他们的利益，并对其他AI代理保持礼貌但独立的判断。{{< /flashcard >}}
{{< /flashcards >}}
