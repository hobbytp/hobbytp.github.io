什么是“代理式AI”（Agentic AI）？,它标志着自主系统在推理、规划和执行多步骤任务方式上的重大转变，不仅仅是传统的单模型提示。
代理式AI工作流与传统LLM提示有何不同？,代理式工作流集成多个专用代理、工具和外部系统，形成能够自主决策和行动的动态管道。
根据论文，一个“AI代理”的本质是什么？,它是一个使用LLM、工具、API和外部上下文来自动执行任务的软件程序。
构建生产级代理式AI工作流面临的核心挑战是什么？,如何设计、工程化和操作可靠、可观察、可维护且符合安全与治理要求的系统。
论文中提出的模型上下文协议（MCP）在代理式工作流中扮演什么角色？,它是一种用于代理与外部服务之间结构化通信的标准化机制。
在构建生产级代理式AI工作流时，会遇到哪四类工程复杂性？,设计挑战、实施挑战、运营挑战和部署挑战。
在播客生成案例中，“网页搜索代理”的首要任务是什么？,通过查询RSS源和基于MCP的搜索端点，从互联网收集最新更新。
在播客生成工作流中，“主题过滤代理”的职责是什么？,评估每篇文章与用户指定主题的相关性，并仅返回过滤后的URL。
为什么在播客生成案例中，“网页抓取代理”将HTML转换为Markdown格式？,为了确保下游代理处理的是一致、结构化的文本。
播客生成工作流使用“代理联盟”（agent consortium）的目的是什么？,"利用来自不同LLM提供商（如OpenAI, Gemini）的多个代理独立生成播客脚本，以获得多样化的草稿。"
在多模型生成后，“推理代理”在播客工作流中起什么关键作用？,通过比较、解决不一致之处并综合多个草稿，生成一个统一、可靠的最终脚本。
在论文提出的九个最佳实践中，第一个实践建议优先选择 _____ 而不是 _____。,工具调用 (Tool Calls) ； 模型上下文协议 (MCP)
为什么优先选择“工具调用”而非MCP集成？,因为MCP可能降低确定性，使代理推理复杂化，并导致模糊的工具选择行为和不稳定的故障。
论文建议用“纯函数调用”取代工具调用的情况是什么？,对于不需要语言推理的操作，例如向API发布数据或向GitHub提交文件。
与工具调用相比，“纯函数调用”有哪些优势？,它们是确定性的、副作用可控的、更便宜、更快且完全可测试的。
为什么应该避免为单个代理附加多个工具？,这会增加提示的复杂性，迫使模型在选择和参数化工具上进行不必要的推理，从而降低可靠性。
“一个代理，一个工具”的设计原则带来了什么好处？,它创建了可预测的角色，简化了提示，消除了工具选择的噪音，并提高了模块化和可解释性。
什么是“单一职责代理”原则？,每个代理应该只负责一个明确定义的任务，就像优秀的软件设计倾向于“做好一件事”的函数和类一样。
让一个代理承担多重职责（如生成和验证）会带来什么问题？,这使得代理更难提示、更难测试，并且更容易出现微妙的、非确定性的故障。
论文提出的第五个最佳实践是关于如何管理代理提示的？,将提示存储为外部工件（如Markdown或文本文件），并在运行时加载它们。
将提示外部化存储有哪些主要好处？,它实现了代码与提示的解耦，允许非技术人员更新代理行为，并支持独立的治理和版本控制工作流。
为了实现“责任感AI”，论文推荐了什么样的代理架构？,采用多模型联盟架构，其中多个专业LLM独立生成输出，然后由一个专用的推理代理进行综合。
多模型联盟和推理代理的设计如何增强工作流？,通过交叉模型验证提高准确性，通过多样性减少偏见，并增强对模型更新的鲁棒性。
在责任感AI架构中，推理代理的核心任务是什么？,执行结构化的整合任务，如解决冲突、检查逻辑一致性和事实对齐，而不是从头创建新内容。
将代理式AI工作流逻辑与MCP服务器分离的架构原则是什么？,工作流应通过REST API提供服务，而MCP服务器应充当一个薄的编排层，将MCP工具调用转发到底层API。
为什么将工作流引擎与MCP服务器解耦很重要？,这可以提高可维护性，支持组件的独立扩展，并确保在LLM和工具演进时的长期适应性。
第八个最佳实践建议使用什么技术来部署生产环境中的代理式AI工作流？,使用Docker等容器化技术，并用Kubernetes等平台进行编排。
容器化部署为代理式AI工作流带来了哪些运营优势？,可移植性、可扩展性、弹性、安全性、可观察性和持续交付。
在代理式AI工作流中应用“KISS”（保持简单愚蠢）原则意味着什么？,避免不必要的结构复杂性，优先选择扁平、可读、功能驱动的设计，而不是传统的复杂企业架构模式。
为什么在代理式系统中，过度工程化的传统软件模式可能是有害的？,因为它们会引入脆弱性，而不是清晰性；代理式工作流的价值在于将推理委托给LLM，而不是复杂的内部逻辑。
保持工作流的简单性如何提高可靠性？,它减少了代理行为模糊、工具调用不匹配或意外副作用的机会，使代理决策路径更清晰。
该论文的案例研究是如何实现其完整实施的？,使用OpenAI Agents SDK，并将工作流后端通过专用REST API暴露，同时开发了一个相应的MCP服务器。
在评估阶段，播客脚本生成代理联盟由哪些模型组成？,由Llama、OpenAI和Gemini模型组成。
对不同模型生成的播客脚本进行评估后，得出了什么结论？,不同模型产生了自然的多样性（如简洁、叙事性强），但也带来了不一致性，凸显了下游整合机制的必要性。
评估结果如何证明推理代理的价值？,推理代理生成的整合脚本在清晰度、事实稳定性和叙事连贯性方面有显著改进，并通过多模型共识降低了幻觉风险。
在多模态内容生成方面，视频脚本代理的任务是什么？,将整合后的播客脚本可靠地转换为基于场景的描述，同时保持叙事保真度和时间连贯性。
评估Veo-3 JSON构建代理时关注哪些方面？,关注其输出的结构正确性、模式对齐以及与Google Veo-3视频生成API的集成保真度。
在传统的人机交互模式中，_____提供提示，模型生成回应。,人类
与人类不同，AI代理能够自主执行LLM交互，包括_____、调用模型、解释回应和执行后续行动。,构建提示
当多个各具专长（如搜索、过滤、推理）的AI代理协作时，它们就形成了_____。,代理式AI工作流
缺乏严谨的工程方法会导致代理式工作流变成_____、无界且易于出错的管道。,不透明
播客生成案例展示了代理式AI管道如何将网页检索、内容生成、_____和软件操作自动化统一起来。,多模态合成
术语: 纯函数 (Pure functions),定义：由工作流直接执行而不涉及LLM的函数，它们是确定性的，用于处理不需要语言推理的任务。
将提示与源代码分离可以使_____等非技术利益相关者能够更新代理行为而无需修改代码。,领域专家
在Kubernetes中部署代理式工作流可以实现_____，即根据负载自动增减副本数量。,可扩展性
“保持简单”原则有助于代理式工作流更好地与_____等现代AI辅助开发工具集成。,GitHub Copilot
在播客工作流的最后一步，_____代理将所有生成的资产（脚本、音频、视频）打包并通过拉取请求发布到GitHub。,PR代理
为了解决MCP集成的不可预测性，论文中的PR代理最终被替换为_____。,直接的函数调用
将Veo-3 JSON生成和视频生成任务拆分为两个单一职责代理，解决了什么问题？,解决了LLM有时会产生格式错误的JSON、混合自然语言或“幻觉”出文件路径的问题。
通过将工作流和MCP服务器_____并部署到Kubernetes集群，可以实现独立的扩展和安全的迭代。,Docker化（容器化）