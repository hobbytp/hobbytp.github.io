<!doctype html><html lang=zh><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="一个关注 AI 各领域的专题博客"><meta property="og:title" content="EMOS: Embodiment-Aware Multi-Robot Operating System with LLM Agents - My AI Blog"><meta property="og:description" content="一个关注 AI 各领域的专题博客"><meta property="og:type" content="article"><meta name=twitter:card content="summary_large_image"><meta name=twitter:title content="EMOS: Embodiment-Aware Multi-Robot Operating System with LLM Agents - My AI Blog"><meta name=twitter:description content="一个关注 AI 各领域的专题博客"><script type=application/ld+json>{"@context":"https://schema.org","@type":"Article","headline":"EMOS: Embodiment-Aware Multi-Robot Operating System with LLM Agents","description":"一个关注 AI 各领域的专题博客","author":{"@type":"Person","name":"Your Name"},"datePublished":"2025-04-18T20:10:00\u002b08:00"}</script><title>EMOS: Embodiment-Aware Multi-Robot Operating System with LLM Agents - My AI Blog</title><link rel=stylesheet href=/css/main.css></head><body><div class=site-container><aside class=sidebar><div class=back-home><a href=/ class=back-link><span class=back-arrow>←</span>
<span>返回主页</span></a></div><div class=toc-container><h3 class=toc-title>目录</h3><div class=toc-content><nav id=TableOfContents><ul><li><a href=#emos-embodiment-aware-multi-robot-operating-system-with-llm-agents>EMOS: Embodiment-Aware Multi-Robot Operating System with LLM Agents</a></li></ul></nav></div></div></aside><main class=main-content><article class=article-container><header class=article-header><h1 class=article-title>EMOS: Embodiment-Aware Multi-Robot Operating System with LLM Agents</h1><div class=article-meta><time class=article-date>2025-04-18</time><div class=article-categories><span class=category>paper</span>
<span class=category>emos</span>
<span class=category>robotics</span></div><div class=article-tags><span class=tag>#AI</span>
<span class=tag>#robotics</span>
<span class=tag>#Agents</span>
<span class=tag>#论文</span>
<span class=tag>#技术</span></div></div></header><div class=article-content><h2 id=emos-embodiment-aware-multi-robot-operating-system-with-llm-agents>EMOS: Embodiment-Aware Multi-Robot Operating System with LLM Agents</h2><p><a href="https://openreview.net/pdf?id=Ey8KcabBpB">https://openreview.net/pdf?id=Ey8KcabBpB</a></p><p>这份文档介绍了 <strong>EMOS (Embodiment-Aware Heterogeneous Multi-Robot Operating System)</strong>，一个利用大型语言模型（LLM）代理来操作异构多机器人系统的创新框架。<strong>EMOS 的核心在于“机器人简历”</strong>，这是一种通过分析机器人 URDF 文件并结合运动学工具自主生成的机器人物理能力描述，取代了人为的角色分配。为了评估这种框架，作者提出了一个新的基准测试 <strong>Habitat-MAS</strong>，专注于考察多智能体系统在需要理解机器人具体形态的任务中的表现，涵盖了操作、感知、导航和复杂的多楼层物体重排列。<strong>实验结果表明，“机器人简历”和分层式的多智能体系统设计对于异构机器人在复杂环境中的有效协作至关重要。</strong></p><p>EMOS（Embodiment-Aware Heterogeneous Multi-Robot Operating System）利用 LLM（大型语言模型）代理来实现对具有不同物理形态的异构多机器人系统（HMRS）的控制与协同，其核心在于通过**“机器人履历”<strong>实现</strong>具身感知推理**。以下是 EMOS 如何实现这一点的详细说明：</p><ol><li><p><strong>机器人履历的生成与理解</strong>：EMOS 的关键设计是“机器人履历”。不同于人为设计的角色扮演，EMOS 采用一种<strong>自激式方法</strong>，让 LLM 代理理解机器人的 URDF（统一机器人描述格式）文件，并调用机器人运动学工具来生成描述其物理能力的“机器人履历”。这个履历包含了机器人<strong>移动能力</strong>、<strong>感知能力</strong>和<strong>操作能力</strong>的全面总结（自然语言描述和数值表示）。LLM 代理通过解析 URDF 文件构建机器人骨架树，并利用前向运动学 API 获取机器人的几何信息，例如传感器和末端执行器的位置以及机械臂的工作空间。</p></li><li><p><strong>分层任务规划、分配与执行框架</strong>：EMOS 采用一个三阶段的级联框架：</p><ul><li><strong>场景上下文构建（Scene Context Construction）</strong>：EMOS 通过一个自下而上的流程，利用理想的语义 SLAM（同步定位与地图构建）系统提供的几何表示，构建环境的文本描述作为场景上下文。这包括区域连接图、语义网格、代理和对象状态以及导航网格等几何表示，并将其中的区域连接图和代理/对象状态转化为文本描述。</li><li><strong>集中式群体讨论（Centralized Group Discussion）</strong>：在这个阶段，所有 LLM 代理进行同步讨论。<strong>一个中央规划者（CentralPlanner）<strong>利用任务描述、场景上下文和每个机器人的“机器人履历”来生成初始的任务计划，并将子任务分配给不同的机器人。每个机器人都有一个</strong>专属的 LLM 代理</strong>（Robot-dedicated agent），它会根据自身的“机器人履历”对分配到的子任务进行<strong>反思（Reflection）</strong>，判断其是否可行，并向中央规划者提供反馈。如果反馈表明子任务不可行，中央规划者会进行重新分配。这种集中式的讨论确保了任务分配是基于对每个机器人物理能力的理解。EMOS 的多代理系统设计遵循 HMAS-2 框架，采用星型拓扑的通信图，其中领导者 LLM 代理负责高层规划和任务分配，而机器人 LLM 代理提供反馈。</li><li><strong>分散式并行执行（Decentralized Action Parallel Execution）</strong>：在任务分配确定后，每个机器人专属的 LLM 代理开始并行地执行其分配到的子任务。每个代理通过**函数调用（Function Call）**其特定的技能库来选择当前要执行的动作，这些技能库连接到低级别的机器人控制接口（例如导航、移动机械臂、抓取、放置等）。执行历史会被记录下来，用于后续的决策。当一个机器人完成其所有任务后，会进入等待状态，直到所有机器人都完成任务。</li></ul></li><li><p><strong>具身感知推理</strong>：EMOS 的核心在于使 LLM 代理能够进行“具身感知推理”。这意味着代理能够理解自身的物理形态以及由此决定的硬件能力。通过“机器人履历”，LLM 代理不仅可以获得关于机器人能力（如移动类型、传感器类型、机械臂自由度等）的文本描述，还可以访问这些能力的数值表示（如机械臂工作空间的中心和半径，相机视野范围等）。在集中式群体讨论和分散式动作执行阶段，LLM 代理会利用这些信息进行以下操作：</p><ul><li><strong>判断机器人是否具备执行特定任务的能力</strong>：例如，判断轮式机器人是否能到达楼上的房间，或者某个机器人手臂的工作空间是否能触及目标物体。</li><li><strong>生成用于空间推理的代码</strong>：LLM 代理可以生成代码来利用机器人履历中的数值信息和场景描述中的物体位置等信息进行几何计算，例如判断物体是否在机器人的感知范围内或是否可以被机械臂抓取。</li><li><strong>做出符合机器人物理限制的决策</strong>：基于对自身能力的理解，机器人代理可以拒绝无法完成的任务，并为中央规划者提供有价值的反馈，从而实现更合理的任务分配。</li></ul></li><li><p><strong>Habitat-MAS 基准测试</strong>：为了评估 LLM 多代理系统在协同异构多机器人系统中的性能，特别是其对机器人物理能力的理解能力，EMOS 引入了一个新的基准测试 Habitat-MAS。这个基准包含多种类型的机器人（无人机、轮式机器人、腿式机器人）和多样化的室内环境（多层住宅、多房间公寓）。基准测试设计了四个任务，分别评估多代理系统在操作、感知、导航和综合多楼层物体重排任务中对机器人物理能力的理解。实验结果表明，机器人履历和 EMOS 的分层设计对于在这样的问题背景下有效操作异构多机器人系统至关重要。</p></li><li><p><strong>实验 (Experiments)</strong>：
论文在 Habitat-MAS 基准测试上对 EMOS 进行了实验，并进行了消融研究以分析不同组成部分的影响。消融实验包括：<strong>移除数值能力描述 (w/o. Numerical)</strong>、<strong>移除机器人履历并使用角色描述 (w/o. Robot resume)</strong> 以及 <strong>移除群体讨论 (w/o. Discussion)</strong>。实验使用了 GPT-4o API。评估指标包括成功率、子目标成功率、令牌使用量和仿真步数。</p></li><li><p><strong>结果与分析 (Results and Analysis)</strong>：
实验结果表明，<strong>EMOS 框架在 Habitat-MAS 基准测试中取得了明显的优势</strong>。消融研究强调了 <strong>机器人履历在具身感知推理中的重要性</strong>，尤其是在复杂的物体重排任务中，<strong>仅仅依靠文本描述是不够的，需要调用数学函数进行数值计算</strong>。移除数值能力描述或整个机器人履历都会导致任务成功率显著下降。与没有群体讨论的设置相比，EMOS 的性能也更好，表明群体讨论有助于任务分解和智能体间的协调。论文还分析了令牌使用效率，发现 EMOS 在成功率提升和令牌使用增加之间取得了较好的平衡。</p></li><li><p><strong>结论与未来工作 (Conclusion and Future Work)</strong>：
论文总结了 EMOS 作为一个基于 LLM 的多智能体系统，旨在解决复杂家庭环境中的多机器人操作问题，特别是具身感知推理和三维空间推理。实验结果证明了具身感知和空间推理在异构多机器人系统中的重要性，并强调了使用数值信息进行精确空间推理以及使用群体讨论模块分解复杂任务对于提高任务成功率的关键作用。未来的工作可以侧重于<strong>提高系统的可扩展性</strong>，以适应更多样化的机器人类型和更大数量的机器人，以及<strong>扩展框架在更动态的现实世界环境中的适应性</strong>.</p></li><li><p><strong>创新性与意义 (Novelty and Significance)</strong>：
这篇论文的主要创新在于提出了 <strong>EMOS 框架</strong>，该框架通过<strong>自生成的“机器人履历”</strong> 而不是人工分配的角色扮演，实现了异构多机器人系统的具身感知推理。<strong>Habitat-MAS 基准测试</strong> 是首个专门用于评估多智能体系统理解其物理形态的模拟基准测试，包含丰富的机器人类型和场景。<strong>“机器人履历” 的概念</strong> 是一种新颖的将机器人物理能力显式地表示给 LLM 智能体的方法，使其能够在任务规划和执行中考虑到机器人的具体限制和能力。这项工作对于推动基于 LLM 的多智能体系统在机器人领域的应用，特别是实现异构多机器人系统的完全自动化 (Level 4)，具有重要的理论和实践意义.</p></li></ol><p>总而言之，EMOS 通过让 LLM 代理理解和利用由 URDF 文件生成的“机器人履历”，在集中式群体讨论中进行具身感知的任务规划和分配，并在分散式执行阶段根据自身能力采取行动，从而实现了对具有不同物理形态的异构多机器人系统的有效控制与协同。实验结果也证明了“机器人履历”在提高异构多机器人系统任务成功率和效率方面的重要性.</p></div><footer class=article-footer><div class=article-navigation><a href=/zh/agi/google_safety_security_approach/ class="nav-link prev"><span class=nav-arrow>←</span>
<span class=nav-title>Google: 一种通往技术通用人工智能安全的方法</span></a>
<a href=/zh/validation/code_level/ class="nav-link next"><span class=nav-title>编程能力对比分析一撇</span>
<span class=nav-arrow>→</span></a></div></footer></article></main></div><style>:root{--primary:#2196f3;--primary-dark:#1a237e;--secondary:#64b5f6;--theme:#121212;--entry:#1e1e1e;--border:#2d2d2d;--code-bg:#2d2d2d;--text:#e0e0e0;--text-secondary:#b0b0b0}.article-content table{width:100%;border-collapse:collapse;margin:1.5rem 0;background:var(--entry);border-radius:8px;overflow:hidden;box-shadow:0 2px 4px rgba(0,0,0,.1)}.article-content th,.article-content td{padding:.8rem;text-align:left;border:1px solid var(--border)}.article-content th{background:var(--primary-dark);color:var(--text);font-weight:600}.article-content tr:nth-child(even){background:var(--code-bg)}.article-content tr:hover{background:rgba(33,150,243,.1)}.site-container{display:flex;gap:2rem;max-width:1200px;margin:0 auto;padding:1.5rem}.sidebar{flex:0 0 250px;position:sticky;top:1.5rem;height:fit-content}.back-home{margin-bottom:1.5rem}.back-link{display:flex;align-items:center;gap:.5rem;color:var(--text-secondary);text-decoration:none;padding:.5rem;border-radius:6px;transition:all .3s}.back-link:hover{background:var(--code-bg);color:var(--primary)}.back-arrow{font-size:1.2rem}.toc-container{background:var(--entry);border-radius:12px;padding:1.2rem;border:1px solid var(--border);box-shadow:0 4px 6px rgba(0,0,0,.3)}.toc-title{color:var(--primary);margin:0 0 1rem;font-size:1.1rem}.toc-content{font-size:.9rem;line-height:1.6}.toc-content ul{list-style:none;padding-left:.5rem;margin:0}.toc-content li{margin:.3rem 0}.toc-content a{color:var(--text-secondary);text-decoration:none;transition:all .3s;display:block;padding:.2rem .5rem;border-radius:4px}.toc-content a:hover{color:var(--primary);background:var(--code-bg)}.main-content{flex:1;min-width:0}.article-container{background:var(--entry);border-radius:12px;padding:2rem;box-shadow:0 2px 4px rgba(0,0,0,.1)}.article-header{margin-bottom:2rem;padding-bottom:1rem;border-bottom:1px solid var(--border)}.article-title{font-size:2rem;color:var(--primary);margin-bottom:1rem;line-height:1.3}.article-meta{display:flex;flex-wrap:wrap;gap:1rem;align-items:center;color:var(--text-secondary);font-size:.9rem}.article-date{background:var(--code-bg);padding:.3rem .6rem;border-radius:4px;font-family:monospace}.article-categories{display:flex;gap:.5rem}.article-tags{display:flex;gap:.5rem;flex-wrap:wrap}.article-content{color:var(--text);line-height:1.8;font-size:1.1rem}.article-content h2{color:var(--primary);margin:2rem 0 1rem;font-size:1.5rem}.article-content h3{color:var(--primary);margin:1.5rem 0 1rem;font-size:1.3rem}.article-content p{margin:1rem 0}.article-content code{background:var(--code-bg);padding:.2rem .4rem;border-radius:4px;font-family:monospace;font-size:.9em}.article-content pre{background:var(--code-bg);padding:1rem;border-radius:8px;overflow-x:auto;margin:1.5rem 0}.article-content blockquote{border-left:4px solid var(--primary);padding-left:1rem;margin:1.5rem 0;color:var(--text-secondary)}.article-content img{max-width:100%;height:auto;border-radius:8px;margin:1.5rem 0}.article-footer{margin-top:3rem;padding-top:2rem;border-top:1px solid var(--border)}.article-navigation{display:flex;justify-content:space-between;gap:1rem}.nav-link{display:flex;align-items:center;gap:.5rem;color:var(--text-secondary);text-decoration:none;padding:.5rem 1rem;border-radius:6px;transition:all .3s;max-width:45%}.nav-link:hover{background:var(--code-bg);color:var(--primary)}.nav-arrow{font-size:1.2rem}.nav-title{overflow:hidden;text-overflow:ellipsis;white-space:nowrap}@media(max-width:768px){.site-container{flex-direction:column;padding:1rem}.sidebar{position:static;width:100%;max-height:none}.article-container{padding:1.5rem}.article-title{font-size:1.5rem}.article-meta{flex-direction:column;align-items:flex-start}.article-navigation{flex-direction:column}.nav-link{max-width:100%}}</style><footer><p>&copy; 2025 My AI Blog. All rights reserved.</p><p><a href=https://github.com/hobbytp/hobbytp.github.io target=_blank>GitHub</a> |
<a href=https://github.com/hobbytp/hobbytp.github.io/issues target=_blank>反馈</a></p></footer><script>document.addEventListener("DOMContentLoaded",function(){document.querySelectorAll("pre > code.language-mermaid").forEach(function(e){const n=e.parentElement,t=document.createElement("div");t.className="mermaid",t.textContent=e.textContent,n.parentElement.replaceChild(t,n)}),window.mermaid&&mermaid.init()})</script></body></html>