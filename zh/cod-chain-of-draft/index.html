<!doctype html><html lang=zh><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="一个关注 AI 各领域的专题博客"><meta property="og:title" content="Chain of Draft 论文解读 - My AI Blog"><meta property="og:description" content="一个关注 AI 各领域的专题博客"><meta property="og:type" content="article"><meta name=twitter:card content="summary_large_image"><meta name=twitter:title content="Chain of Draft 论文解读 - My AI Blog"><meta name=twitter:description content="一个关注 AI 各领域的专题博客"><script type=application/ld+json>{"@context":"https://schema.org","@type":"Article","headline":"Chain of Draft 论文解读","description":"一个关注 AI 各领域的专题博客","author":{"@type":"Person","name":"Your Name"},"datePublished":"2025-03-01T20:00:00\u002b08:00"}</script><title>Chain of Draft 论文解读 - My AI Blog</title><link rel=stylesheet href=/css/main.css></head><body><div class=site-container><aside class=sidebar><div class=back-home><a href=/ class=back-link><span class=back-arrow>←</span>
<span>返回主页</span></a></div><div class=toc-container><h3 class=toc-title>目录</h3><div class=toc-content><nav id=TableOfContents><ul><li><a href=#论文十问解读>论文十问解读</a></li><li><a href=#论文cod的实例>论文CoD的实例</a></li><li><a href=#cod-与-cot-的对比>CoD 与 CoT 的对比</a></li><li><a href=#论文中提到的其他方法>论文中提到的其他方法</a></li><li><a href=#论文不足之处>论文不足之处</a></li></ul></nav></div></div></aside><main class=main-content><article class=article-container><header class=article-header><h1 class=article-title>Chain of Draft 论文解读</h1><div class=article-meta><time class=article-date>2025-03-01</time><div class=article-categories><span class=category>papers</span>
<span class=category>chain-of-draft</span></div><div class=article-tags><span class=tag>#AI</span>
<span class=tag>#Chain of Draft</span>
<span class=tag>#论文</span>
<span class=tag>#技术</span></div></div></header><div class=article-content><p><a href=https://arxiv.org/abs/2502.18600>https://arxiv.org/abs/2502.18600</a></p><p>本研究提出了一种名为“草稿链”（Chain of Draft, CoD）的新型提示策略，旨在提高大型语言模型（LLM）在复杂推理任务中的效率。 CoD 模仿人类的认知过程，鼓励 LLM 生成简洁但信息丰富的中间推理结果，而不是像“思维链”（Chain-of-Thought, CoT）那样产生冗长的步骤。实验结果表明，CoD 在保持或提高准确性的同时，显著减少了 token 使用量和延迟，从而降低了计算成本。 这种方法特别适用于对成本和延迟敏感的实际应用场景。CoD 通过减少不必要的文字，专注于关键见解，优化了 LLM 的推理过程。 研究还探讨了 CoD 对 LLM 设计、部署和实际可用性的影响，并展望了未来结合 CoD 与其他优化技术的可能性。</p><p>论文由Zoom Communications机构提供。论文的作者包括Silei Xu、Wenhao Xie、Lingxiao Zhao和Pengcheng He。</p><h2 id=论文十问解读>论文十问解读</h2><ol><li><p><strong>论文试图解决什么问题？</strong>
这篇论文旨在解决大型语言模型（LLMs）在复杂推理任务中使用<strong>Chain-of-Thought (CoT)</strong> 方法时产生的<strong>冗长和高计算成本</strong>问题。CoT 虽然提高了准确性，但其详细的逐步推理过程导致大量的 token 使用和更高的延迟，这在实际应用中是不利的。因此，论文提出了 <strong>Chain of Draft (CoD)</strong>，一种更高效、简洁的推理提示策略。</p></li><li><p><strong>研究现状如何？</strong>
目前，LLMs 通过 <strong>CoT</strong> 等结构化推理方法在复杂任务中表现出色。一些研究通过 <strong>self-consistency CoT, ReAct</strong> 等方法来增强推理的可靠性。然而，这些方法增加了 token 的使用量，使其难以应用于对成本和延迟敏感的场景。诸如 <strong>Skeleton-of-Thought (SoT)</strong> 和其他降低延迟的技术，要么不能减少计算成本，要么在复杂任务中效果不佳。<strong>Concise Thoughts (CCoT)</strong> 和 <strong>token-budget-aware LLM reasoning (TALE)</strong> 等方法试图通过限制 token 数量来提高效率，但它们在token预算的动态调整和复杂任务的适应性方面存在局限性。</p></li><li><p><strong>论文提出了什么方法？</strong></p><ul><li>论文提出了 <strong>Chain of Draft (CoD)</strong> 提示策略，灵感来源于人类在解决复杂问题时采用的简洁记录关键信息的习惯。</li><li><strong>CoD</strong> 鼓励 LLMs 在每个推理步骤中生成简洁、信息密集的输出，从而减少冗余，降低延迟和计算成本。</li><li><strong>CoD</strong> 的核心思想是限制每个推理步骤的字数，使其只关注必要的计算或转换。例如，在解决数学问题时，CoD 会将推理过程简化为简洁的方程式。</li></ul></li><li><p><strong>论文如何验证所提出的方法？</strong></p><ul><li>论文在多个需要多步骤推理的 benchmark 上进行了实验，包括算术推理 (<strong>GSM8k</strong>)、常识推理 (来自 <strong>BIG-bench</strong> 的日期理解和体育理解) 和符号推理 (coin flip tasks)。<ul><li><strong>算术推理</strong>：使用了 GSM8k 数据集。该数据集包含 8500 个小学水平的数学问题，涵盖算术、几何、代数和逻辑推理。</li><li><strong>常识推理</strong>：使用了 BIG-bench 中的日期理解和体育理解任务。</li><li><strong>符号推理</strong>：使用了论文中介绍的 coin flip tasks（抛硬币任务）。由于确切的数据集未公开发布，研究人员合成了包含 250 个示例的测试集。</li></ul></li><li>实验使用了 <strong>GPT-4o</strong> 和 <strong>Claude 3.5 Sonnet</strong> 这两个流行的模型。</li><li>对比了 <strong>CoD</strong>、<strong>CoT</strong> 和标准提示 (Standard prompting) 三种策略。</li><li>评估指标包括准确率、token 使用量和延迟。</li></ul></li><li><p><strong>实验结果如何？</strong></p><ul><li>在所有测试中，<strong>CoD</strong> 在保持或提高准确性的同时，显著减少了 token 的使用量和延迟。</li><li>例如，在 <strong>GSM8k</strong> 算术推理任务中，<strong>CoD</strong> 在 token 使用量减少 80% 的情况下，准确率与 <strong>CoT</strong> 相当。</li><li>在体育理解任务中，<strong>CoD</strong> 将 <strong>Claude 3.5 Sonnet</strong> 的平均输出 token 从 189.4 减少到 14.3，减少了 92.4%。</li><li>在 coin flip 任务中，<strong>CoD</strong> 和 <strong>CoT</strong> 都达到了 100% 的准确率，但 <strong>CoD</strong> 显著减少了 token 的使用。</li></ul></li><li><p><strong>论文有哪些重要的结论？</strong></p><ul><li><strong>CoD</strong> 是一种有效的推理方法，可以在不牺牲准确性的前提下显著降低延迟和计算成本。</li><li>简洁的推理步骤可以提高 LLMs 的效率，使其更适用于实际应用。</li><li><strong>CoD</strong> 表明，有效的推理不一定需要冗长的输出，可以在保持推理深度的同时最小化冗余。</li></ul></li><li><p><strong>论文有哪些局限性？</strong>
论文中提到，<strong>CoD</strong> 策略在提示中指导模型限制每个推理步骤的字数，但并没有强制执行这种限制。这可能导致在某些情况下，模型仍然会生成较长的推理步骤。</p></li><li><p><strong>未来的研究方向有哪些？</strong></p><ul><li>将 <strong>CoD</strong> 与其他降低延迟的方法 (如自适应并行推理或多通道验证) 相结合，进一步优化性能。</li><li>通过使用简洁的推理数据进行训练，改进推理模型，同时保持 LLMs 的可解释性和效率。</li></ul></li><li><p><strong>这篇论文对领域有什么贡献？</strong>
这篇论文的主要贡献在于提出了 <strong>Chain of Draft (CoD)</strong> 这一新的提示策略，它通过模仿人类简洁的思考方式，显著降低了 LLMs 在推理过程中的计算成本和延迟，同时保持了较高的准确性。这为 LLMs 的实际应用提供了一种更高效的解决方案。</p></li><li><p><strong>是否有其他相关的研究？</strong>
论文中提到了很多相关研究，包括 <strong>self-consistency CoT</strong>、<strong>ReAct</strong>、<strong>Skeleton-of-Thought (SoT)</strong>、<strong>Concise Thoughts (CCoT)</strong> 和 <strong>token-budget-aware LLM reasoning (TALE)</strong>。这些研究都致力于提高 LLMs 的推理能力和效率，但各有侧重。</p></li></ol><h2 id=论文cod的实例>论文CoD的实例</h2><p>这篇论文中提供了一个**Chain-of-Draft (CoD)<strong>的实际例子，展示了它与标准提示 (Standard prompting) 和</strong>Chain-of-Thought (CoT)**提示策略的不同。</p><p>以下是这个例子：</p><ul><li><strong>问题 (Q)</strong>：Jason 有 20 根棒棒糖。他给了 Denny 一些棒棒糖。现在 Jason 有 12 根棒棒糖。Jason 给了 Denny 多少根棒棒糖？</li><li><strong>标准提示 (Standard)</strong>：直接输出答案，没有任何推理过程。<ul><li><strong>回答 (A)</strong>: 8</li></ul></li><li><strong>Chain-of-Thought (CoT)</strong>：提供详细的推理过程，但包含不必要的细节。<ul><li><strong>回答 (A)</strong>：让我们逐步思考：1. 最初，Jason 有 20 根棒棒糖。2. 给 Denny 一些后，Jason 现在有 12 根棒棒糖。3. 为了找出 Jason 给了 Denny 多少根棒棒糖，我们需要计算初始棒棒糖数量和剩余数量之间的差额。4. 我们可以建立一个简单的减法问题：初始棒棒糖数量 - 剩余棒棒糖数量 = 给 Denny 的棒棒糖数量 5. 放入数字：20 - 12 = 给 Denny 的棒棒糖数量 6. 解减法：20 - 12 = 8 因此，Jason 给了 Denny 8 根棒棒糖。#### 8 根棒棒糖</li></ul></li><li><strong>Chain-of-Draft (CoD)</strong>：将推理过程浓缩为最小的、抽象的表示，只关注必要的数学运算。<ul><li><strong>回答 (A)</strong>：20 - x = 12; x = 20 - 12 = 8. #### 8</li></ul></li></ul><p>在这个例子中，<strong>CoD</strong> 通过使用简洁的方程式，<strong>直接表达了解题所需的关键信息，避免了冗长的解释和不必要的细节</strong>。这使得 <strong>CoD</strong> 既保持了透明度和正确性，又显著减少了 token 数量。</p><h2 id=cod-与-cot-的对比>CoD 与 CoT 的对比</h2><p>Prompt 示例：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#75715e># Standard</span>
</span></span><span style=display:flex><span>Answer the question directly. Do not <span style=color:#66d9ef>return</span> any preamble, explanation, or reasoning.
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Chain-of-Thought</span>
</span></span><span style=display:flex><span>Think step by step to answer the following question. Return the answer at the end of the response after a separator <span style=color:#75715e>####.</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Chain-of-Draft</span>
</span></span><span style=display:flex><span>Think step by step, but only keep a minimum draft <span style=color:#66d9ef>for</span> each thinking step, with <span style=color:#ae81ff>5</span> words at most. Return the answer at the end of the response after a separator <span style=color:#75715e>####.</span>
</span></span></code></pre></div><p>CoD 和 CoT 在不同模型上的效果对比表格：</p><table><thead><tr><th style=text-align:left>任务</th><th style=text-align:left>模型</th><th style=text-align:left>Prompt</th><th style=text-align:left>准确率</th><th style=text-align:left>Token 使用量</th><th style=text-align:left>延迟 (秒)</th></tr></thead><tbody><tr><td style=text-align:left>算术推理</td><td style=text-align:left>GPT-4o</td><td style=text-align:left>CoT</td><td style=text-align:left>95.4%</td><td style=text-align:left>205.1</td><td style=text-align:left>4.2</td></tr><tr><td style=text-align:left></td><td style=text-align:left></td><td style=text-align:left>CoD</td><td style=text-align:left>91.1%</td><td style=text-align:left>43.9</td><td style=text-align:left>1.0</td></tr><tr><td style=text-align:left></td><td style=text-align:left>Claude 3.5 Sonnet</td><td style=text-align:left>CoT</td><td style=text-align:left>95.8%</td><td style=text-align:left>190.0</td><td style=text-align:left>3.1</td></tr><tr><td style=text-align:left></td><td style=text-align:left></td><td style=text-align:left>CoD</td><td style=text-align:left>91.4%</td><td style=text-align:left>39.8</td><td style=text-align:left>1.6</td></tr><tr><td style=text-align:left>常识推理 (日期理解)</td><td style=text-align:left>GPT-4o</td><td style=text-align:left>CoT</td><td style=text-align:left>90.2%</td><td style=text-align:left>75.7</td><td style=text-align:left>1.7</td></tr><tr><td style=text-align:left></td><td style=text-align:left></td><td style=text-align:left>CoD</td><td style=text-align:left>88.1%</td><td style=text-align:left>30.2</td><td style=text-align:left>1.3</td></tr><tr><td style=text-align:left></td><td style=text-align:left>Claude 3.5 Sonnet</td><td style=text-align:left>CoT</td><td style=text-align:left>87.0%</td><td style=text-align:left>172.5</td><td style=text-align:left>3.2</td></tr><tr><td style=text-align:left></td><td style=text-align:left></td><td style=text-align:left>CoD</td><td style=text-align:left>89.7%</td><td style=text-align:left>31.3</td><td style=text-align:left>1.4</td></tr><tr><td style=text-align:left>常识推理 (体育理解)</td><td style=text-align:left>GPT-4o</td><td style=text-align:left>CoT</td><td style=text-align:left>95.9%</td><td style=text-align:left>28.7</td><td style=text-align:left>0.9</td></tr><tr><td style=text-align:left></td><td style=text-align:left></td><td style=text-align:left>CoD</td><td style=text-align:left>98.3%</td><td style=text-align:left>15.0</td><td style=text-align:left>0.7</td></tr><tr><td style=text-align:left></td><td style=text-align:left>Claude 3.5 Sonnet</td><td style=text-align:left>CoT</td><td style=text-align:left>93.2%</td><td style=text-align:left>189.4</td><td style=text-align:left>3.6</td></tr><tr><td style=text-align:left></td><td style=text-align:left></td><td style=text-align:left>CoD</td><td style=text-align:left>97.3%</td><td style=text-align:left>14.3</td><td style=text-align:left>1.0</td></tr><tr><td style=text-align:left>符号推理 (抛硬币)</td><td style=text-align:left>GPT-4o</td><td style=text-align:left>CoT</td><td style=text-align:left>100.0%</td><td style=text-align:left>52.4</td><td style=text-align:left>1.4</td></tr><tr><td style=text-align:left></td><td style=text-align:left></td><td style=text-align:left>CoD</td><td style=text-align:left>100.0%</td><td style=text-align:left>16.8</td><td style=text-align:left>0.8</td></tr><tr><td style=text-align:left></td><td style=text-align:left>Claude 3.5 Sonnet</td><td style=text-align:left>CoT</td><td style=text-align:left>100.0%</td><td style=text-align:left>135.3</td><td style=text-align:left>3.1</td></tr><tr><td style=text-align:left></td><td style=text-align:left></td><td style=text-align:left>CoD</td><td style=text-align:left>100.0%</td><td style=text-align:left>18.9</td><td style=text-align:left>1.6</td></tr></tbody></table><p>从以上数据可以看出，<strong>CoD 在保持或提高准确性的同时，显著减少了 token 的使用量和延迟</strong>。这表明 CoD 是一种更高效的推理策略。</p><p>相对于 Chain of Draft (CoD)，Chain-of-Thought (CoT) 存在以下缺陷：</p><ul><li><strong>冗长性</strong>：CoT 提供详细的推理过程，但通常包含不必要的细节。例如，在解决数学问题时，CoT 可能会包含与解题无关的背景信息。这种冗长性导致 token 数量增加，计算成本上升.</li><li><strong>高延迟</strong>：由于 CoT 需要生成较长的推理步骤，因此响应时间较长。这在对延迟敏感的实际应用中是一个显著的缺点.</li><li><strong>资源消耗</strong>：CoT 方法需要更多的计算资源，因为它需要处理大量的 token. 即使是简单的任务，CoT 也可能导致过度思考，从而浪费资源。</li><li><strong>缺乏效率</strong>：CoT 在生成推理步骤时，没有优先考虑效率和简洁性。这与人类解决问题的方式不同，人类通常会使用简洁的草稿或笔记来捕捉关键信息.</li><li><strong>成本较高</strong>：由于 CoT 需要生成大量的 token，因此在使用 LLM 时会产生更高的成本。这在需要大规模部署 LLM 或预算有限的情况下是一个重要考虑因素.</li></ul><p>总而言之，CoT 的主要缺陷在于其<strong>冗长性、高延迟、资源消耗和成本较高</strong>。CoD 通过鼓励 LLM 生成简洁、信息密集的推理步骤，<strong>在保持或提高准确性的同时，显著减少了 token 的使用量和延迟</strong>。</p><h2 id=论文中提到的其他方法>论文中提到的其他方法</h2><p>论文中提到了几个CoT类似的理论，并在论文中分析了它们的优缺点。这些理论包括：</p><ul><li><p><strong>Concise Thoughts (CCoT)</strong>：CCoT 建议为推理步骤使用固定的全局 token 预算。</p><ul><li><strong>优点</strong>：旨在通过限制 token 数量来提高效率。</li><li><strong>缺点</strong>：不同的任务可能需要不同的预算才能在性能和成本之间取得最佳平衡。LLM 可能无法遵守不切实际的预算，经常生成比预期更多的 token。</li></ul></li><li><p><strong>Token-budget-aware LLM reasoning (TALE)</strong>：TALE 通过动态估计不同问题的全局 token 预算来扩展 CCoT 的思想，预算基于推理的复杂性。</p><ul><li><strong>优点</strong>：试图根据任务的复杂性动态调整 token 预算，从而更有效地利用资源。</li><li><strong>缺点</strong>：需要额外的 LLM 调用来估计预算，这会增加延迟。它假设模型可以准确预测请求的复杂性，这限制了其在更复杂任务中的适用性，在这些任务中，可能需要在推理过程中进行反思、自我纠正或外部知识检索。</li></ul></li><li><p><strong>Skeleton-of-Thought (SoT)</strong>：SoT 首先引导 LLM 生成答案的骨架大纲，然后进行并行解码以减少延迟。</p><ul><li><strong>优点</strong>：有助于降低延迟，通过并行解码加速生成过程。</li><li><strong>缺点</strong>：不降低计算成本，仅限于可以有效并行化的任务。</li></ul></li><li><p><strong>Continuous Latent Space Reasoning (Coconut)</strong>：Coconut 训练 LLM 在连续潜在空间中执行推理，而不是在传统的自然语言空间中使用 LLM 的最终隐藏状态来表示推理过程.</p><ul><li><strong>优点</strong>：减少延迟和计算成本。</li><li><strong>缺点</strong>：在复杂任务（如 GSM8k）中，准确性降低。此外，它失去了自然语言推理的可解释性，并且不能应用于像 GPT 和 Claude 这样的黑盒模型。</li></ul></li></ul><p>总的来说，这些方法都试图在推理的准确性和效率之间找到平衡，但各有优缺点。<strong>CoD 的优势在于它采用了一种更灵活的策略，允许每个步骤有不同的 token 预算，从而更好地适应各种结构化推理技术</strong>。</p><h2 id=论文不足之处>论文不足之处</h2><p>不足之处主要体现在以下几个方面：</p><ul><li><strong>CoD</strong> 策略的约束力不足：论文提到，<strong>CoD 策略在提示中指导模型限制每个推理步骤的字数，但并没有强制执行这种限制</strong>。这可能导致在某些情况下，模型仍然会生成较长的推理步骤，从而影响 <strong>CoD</strong> 的效率。虽然设定了每个推理步骤最多五个单词的指导方针，但实际上并没有强制执行这一限制。</li><li><strong>实验数据集的局限性</strong>：虽然论文在算术推理 (<strong>GSM8k</strong>)、常识推理 (来自 <strong>BIG-bench</strong> 的日期理解和体育理解) 和符号推理 (coin flip tasks) 等多个任务上进行了评估，但这些数据集可能无法完全代表所有类型的复杂推理任务。例如，<strong>coin flip tasks</strong> 使用的是研究人员自己合成的数据集，可能存在一定的偏差。</li><li><strong>缺乏与其他延迟降低方法的结合</strong>：论文提出，未来的工作可以将 <strong>CoD</strong> 与其他降低延迟的方法 (如自适应并行推理或多通道验证) 相结合，以进一步优化性能。然而，论文本身并没有进行这方面的实验，因此无法证明 <strong>CoD</strong> 与其他方法结合的实际效果。</li><li><strong>没有提供相应的 GitHub 项目</strong>：论文主要介绍了 <strong>CoD</strong> 的概念、实验结果和潜在应用，但没有提及任何相关的代码库或项目发布。这使得其他研究人员难以复现论文的结果或将 <strong>CoD</strong> 应用于自己的项目中。</li></ul></div><footer class=article-footer><div class=article-navigation><a href=/zh/finetuning/ class="nav-link prev"><span class=nav-arrow>←</span>
<span class=nav-title>微调</span></a>
<a href=/zh/qwen/qwq32b/ class="nav-link next"><span class=nav-title>QwQ-32B Qwen推理大模型解读</span>
<span class=nav-arrow>→</span></a></div></footer></article></main></div><style>:root{--primary:#2196f3;--primary-dark:#1a237e;--secondary:#64b5f6;--theme:#121212;--entry:#1e1e1e;--border:#2d2d2d;--code-bg:#2d2d2d;--text:#e0e0e0;--text-secondary:#b0b0b0}.article-content table{width:100%;border-collapse:collapse;margin:1.5rem 0;background:var(--entry);border-radius:8px;overflow:hidden;box-shadow:0 2px 4px rgba(0,0,0,.1)}.article-content th,.article-content td{padding:.8rem;text-align:left;border:1px solid var(--border)}.article-content th{background:var(--primary-dark);color:var(--text);font-weight:600}.article-content tr:nth-child(even){background:var(--code-bg)}.article-content tr:hover{background:rgba(33,150,243,.1)}.site-container{display:flex;gap:2rem;max-width:1200px;margin:0 auto;padding:1.5rem}.sidebar{flex:0 0 250px;position:sticky;top:1.5rem;height:fit-content}.back-home{margin-bottom:1.5rem}.back-link{display:flex;align-items:center;gap:.5rem;color:var(--text-secondary);text-decoration:none;padding:.5rem;border-radius:6px;transition:all .3s}.back-link:hover{background:var(--code-bg);color:var(--primary)}.back-arrow{font-size:1.2rem}.toc-container{background:var(--entry);border-radius:12px;padding:1.2rem;border:1px solid var(--border);box-shadow:0 4px 6px rgba(0,0,0,.3)}.toc-title{color:var(--primary);margin:0 0 1rem;font-size:1.1rem}.toc-content{font-size:.9rem;line-height:1.6}.toc-content ul{list-style:none;padding-left:.5rem;margin:0}.toc-content li{margin:.3rem 0}.toc-content a{color:var(--text-secondary);text-decoration:none;transition:all .3s;display:block;padding:.2rem .5rem;border-radius:4px}.toc-content a:hover{color:var(--primary);background:var(--code-bg)}.main-content{flex:1;min-width:0}.article-container{background:var(--entry);border-radius:12px;padding:2rem;box-shadow:0 2px 4px rgba(0,0,0,.1)}.article-header{margin-bottom:2rem;padding-bottom:1rem;border-bottom:1px solid var(--border)}.article-title{font-size:2rem;color:var(--primary);margin-bottom:1rem;line-height:1.3}.article-meta{display:flex;flex-wrap:wrap;gap:1rem;align-items:center;color:var(--text-secondary);font-size:.9rem}.article-date{background:var(--code-bg);padding:.3rem .6rem;border-radius:4px;font-family:monospace}.article-categories{display:flex;gap:.5rem}.article-tags{display:flex;gap:.5rem;flex-wrap:wrap}.article-content{color:var(--text);line-height:1.8;font-size:1.1rem}.article-content h2{color:var(--primary);margin:2rem 0 1rem;font-size:1.5rem}.article-content h3{color:var(--primary);margin:1.5rem 0 1rem;font-size:1.3rem}.article-content p{margin:1rem 0}.article-content code{background:var(--code-bg);padding:.2rem .4rem;border-radius:4px;font-family:monospace;font-size:.9em}.article-content pre{background:var(--code-bg);padding:1rem;border-radius:8px;overflow-x:auto;margin:1.5rem 0}.article-content blockquote{border-left:4px solid var(--primary);padding-left:1rem;margin:1.5rem 0;color:var(--text-secondary)}.article-content img{max-width:100%;height:auto;border-radius:8px;margin:1.5rem 0}.article-footer{margin-top:3rem;padding-top:2rem;border-top:1px solid var(--border)}.article-navigation{display:flex;justify-content:space-between;gap:1rem}.nav-link{display:flex;align-items:center;gap:.5rem;color:var(--text-secondary);text-decoration:none;padding:.5rem 1rem;border-radius:6px;transition:all .3s;max-width:45%}.nav-link:hover{background:var(--code-bg);color:var(--primary)}.nav-arrow{font-size:1.2rem}.nav-title{overflow:hidden;text-overflow:ellipsis;white-space:nowrap}@media(max-width:768px){.site-container{flex-direction:column;padding:1rem}.sidebar{position:static;width:100%;max-height:none}.article-container{padding:1.5rem}.article-title{font-size:1.5rem}.article-meta{flex-direction:column;align-items:flex-start}.article-navigation{flex-direction:column}.nav-link{max-width:100%}}</style><footer><p>&copy; 2025 My AI Blog. All rights reserved.</p><p><a href=https://github.com/hobbytp/hobbytp.github.io target=_blank>GitHub</a> |
<a href=https://github.com/hobbytp/hobbytp.github.io/issues target=_blank>反馈</a></p></footer><script>document.addEventListener("DOMContentLoaded",function(){document.querySelectorAll("pre > code.language-mermaid").forEach(function(e){const n=e.parentElement,t=document.createElement("div");t.className="mermaid",t.textContent=e.textContent,n.parentElement.replaceChild(t,n)}),window.mermaid&&mermaid.init()})</script></body></html>