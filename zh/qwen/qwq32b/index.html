<!doctype html><html lang=zh><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="一个关注 AI 各领域的专题博客"><meta property="og:title" content="QwQ-32B Qwen推理大模型解读 - My AI Blog"><meta property="og:description" content="一个关注 AI 各领域的专题博客"><meta property="og:type" content="article"><meta name=twitter:card content="summary_large_image"><meta name=twitter:title content="QwQ-32B Qwen推理大模型解读 - My AI Blog"><meta name=twitter:description content="一个关注 AI 各领域的专题博客"><script type=application/ld+json>{"@context":"https://schema.org","@type":"Article","headline":"QwQ-32B Qwen推理大模型解读","description":"一个关注 AI 各领域的专题博客","author":{"@type":"Person","name":"Your Name"},"datePublished":"2025-03-06T20:21:00\u002b08:00"}</script><title>QwQ-32B Qwen推理大模型解读 - My AI Blog</title><link rel=stylesheet href=/css/main.css></head><body><div class=site-container><aside class=sidebar><div class=back-home><a href=/ class=back-link><span class=back-arrow>←</span>
<span>返回主页</span></a></div><div class=toc-container><h3 class=toc-title>目录</h3><div class=toc-content><nav id=TableOfContents><ul><li><a href=#模型介绍>模型介绍</a><ul><li><a href=#技术指标>技术指标</a></li><li><a href=#评测结果>评测结果</a></li><li><a href=#训练细节>训练细节</a></li><li><a href=#代码调用>代码调用</a></li><li><a href=#未来工作>未来工作</a></li><li><a href=#参考链接>参考链接</a></li></ul></li></ul></nav></div></div></aside><main class=main-content><article class=article-container><header class=article-header><h1 class=article-title>QwQ-32B Qwen推理大模型解读</h1><div class=article-meta><time class=article-date>2025-03-06</time><div class=article-categories><span class=category>llm</span>
<span class=category>qwen</span>
<span class=category>large_models</span></div><div class=article-tags><span class=tag>#AI</span>
<span class=tag>#深度思考</span>
<span class=tag>#QwQ-32B</span>
<span class=tag>#大模型</span>
<span class=tag>#Qwen</span></div></div></header><div class=article-content><h2 id=模型介绍>模型介绍</h2><p>这是一款拥有 320 亿参数的模型，其性能可与具备 6710 亿参数（其中 370 亿被激活）的 DeepSeek-R1 媲美。这一成果突显了将强化学习应用于经过大规模预训练的强大基础模型的有效性。
QwQ32B在推理模型中集成了与 Agent 相关的能力，使其能够在使用工具的同时进行批判性思考，并根据环境反馈调整推理过程。
通过这种方式，QwQ32B 能够执行复杂的推理任务，如数学问题解决和编程挑战。
这个模型于2025年3月6日发布。</p><h3 id=技术指标>技术指标</h3><ul><li>类型：因果语言模型</li><li>训练阶段：训练前和训练后（监督微调和强化学习）</li><li>架构：具有RoPE、SwiGLU、RMSNorm和注意力QKV偏置的变压器</li><li>参数数：32.5B</li><li>参数数量（非嵌入）：31.0B</li><li>层数：64</li><li>注意头数（GQA）：Q 40个，KV 8个</li><li>上下文长度：完整的131,072个令牌</li></ul><h3 id=评测结果>评测结果</h3><p>QwQ-32B 在一系列基准测试中进行了评估，测试了数学推理、编程能力和通用能力。以下结果展示了 QwQ-32B 与其他领先模型的性能对比，包括 DeepSeek-R1-Distilled-Qwen-32B、DeepSeek-R1-Distilled-Llama-70B、o1-mini 以及原始的 DeepSeek-R1。</p><h3 id=训练细节>训练细节</h3><p>QwQ基于Qwen2.5，在冷启动的基础上开展了大规模强化学习, 这个和DeepSeek R1的训练方式类似。
在初始阶段，我们特别针对数学和编程任务进行了 RL 训练。
与依赖传统的奖励模型（reward model）不同：</p><ol><li>通过校验生成答案的正确性来为数学问题提供反馈，</li><li>通过代码执行服务器评估生成的代码是否成功通过测试用例来提供代码的反馈。</li><li>随着训练轮次的推进，这两个领域中的性能均表现出持续的提升。<ol><li>在第一阶段的 RL 过后，我们增加了另一个针对通用能力的 RL。</li><li>此阶段使用通用奖励模型和一些基于规则的验证器进行训练。</li><li>通过少量步骤的通用 RL，可以提升其他通用能力，同时在数学和编程任务上的性能没有显著下降。</li></ol></li></ol><h3 id=代码调用>代码调用</h3><p>其代码已在最新的HuggingFace transformers中（v4.37）。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> openai <span style=color:#f92672>import</span> OpenAI
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> os
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Initialize OpenAI client</span>
</span></span><span style=display:flex><span>client <span style=color:#f92672>=</span> OpenAI(
</span></span><span style=display:flex><span>    <span style=color:#75715e># If the environment variable is not configured, replace with your API Key: api_key=&#34;sk-xxx&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># How to get an API Key：https://help.aliyun.com/zh/model-studio/developer-reference/get-api-key</span>
</span></span><span style=display:flex><span>    api_key<span style=color:#f92672>=</span>os<span style=color:#f92672>.</span>getenv(<span style=color:#e6db74>&#34;DASHSCOPE_API_KEY&#34;</span>),
</span></span><span style=display:flex><span>    base_url<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;https://dashscope.aliyuncs.com/compatible-mode/v1&#34;</span>
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>reasoning_content <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;&#34;</span>
</span></span><span style=display:flex><span>content <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>is_answering <span style=color:#f92672>=</span> <span style=color:#66d9ef>False</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>completion <span style=color:#f92672>=</span> client<span style=color:#f92672>.</span>chat<span style=color:#f92672>.</span>completions<span style=color:#f92672>.</span>create(
</span></span><span style=display:flex><span>    model<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;qwq-32b&#34;</span>,
</span></span><span style=display:flex><span>    messages<span style=color:#f92672>=</span>[
</span></span><span style=display:flex><span>        {<span style=color:#e6db74>&#34;role&#34;</span>: <span style=color:#e6db74>&#34;user&#34;</span>, <span style=color:#e6db74>&#34;content&#34;</span>: <span style=color:#e6db74>&#34;Which is larger, 9.9 or 9.11?&#34;</span>}
</span></span><span style=display:flex><span>    ],
</span></span><span style=display:flex><span>    stream<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>,
</span></span><span style=display:flex><span>    <span style=color:#75715e># Uncomment the following line to return token usage in the last chunk</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># stream_options={</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>#     &#34;include_usage&#34;: True</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># }</span>
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&#34;</span> <span style=color:#f92672>+</span> <span style=color:#e6db74>&#34;=&#34;</span> <span style=color:#f92672>*</span> <span style=color:#ae81ff>20</span> <span style=color:#f92672>+</span> <span style=color:#e6db74>&#34;reasoning content&#34;</span> <span style=color:#f92672>+</span> <span style=color:#e6db74>&#34;=&#34;</span> <span style=color:#f92672>*</span> <span style=color:#ae81ff>20</span> <span style=color:#f92672>+</span> <span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> chunk <span style=color:#f92672>in</span> completion:
</span></span><span style=display:flex><span>    <span style=color:#75715e># If chunk.choices is empty, print usage</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> <span style=color:#f92672>not</span> chunk<span style=color:#f92672>.</span>choices:
</span></span><span style=display:flex><span>        print(<span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>Usage:&#34;</span>)
</span></span><span style=display:flex><span>        print(chunk<span style=color:#f92672>.</span>usage)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>        delta <span style=color:#f92672>=</span> chunk<span style=color:#f92672>.</span>choices[<span style=color:#ae81ff>0</span>]<span style=color:#f92672>.</span>delta
</span></span><span style=display:flex><span>        <span style=color:#75715e># Print reasoning content</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> hasattr(delta, <span style=color:#e6db74>&#39;reasoning_content&#39;</span>) <span style=color:#f92672>and</span> delta<span style=color:#f92672>.</span>reasoning_content <span style=color:#f92672>is</span> <span style=color:#f92672>not</span> <span style=color:#66d9ef>None</span>:
</span></span><span style=display:flex><span>            print(delta<span style=color:#f92672>.</span>reasoning_content, end<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;&#39;</span>, flush<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>            reasoning_content <span style=color:#f92672>+=</span> delta<span style=color:#f92672>.</span>reasoning_content
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> delta<span style=color:#f92672>.</span>content <span style=color:#f92672>!=</span> <span style=color:#e6db74>&#34;&#34;</span> <span style=color:#f92672>and</span> is_answering <span style=color:#f92672>is</span> <span style=color:#66d9ef>False</span>:
</span></span><span style=display:flex><span>                print(<span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&#34;</span> <span style=color:#f92672>+</span> <span style=color:#e6db74>&#34;=&#34;</span> <span style=color:#f92672>*</span> <span style=color:#ae81ff>20</span> <span style=color:#f92672>+</span> <span style=color:#e6db74>&#34;content&#34;</span> <span style=color:#f92672>+</span> <span style=color:#e6db74>&#34;=&#34;</span> <span style=color:#f92672>*</span> <span style=color:#ae81ff>20</span> <span style=color:#f92672>+</span> <span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>                is_answering <span style=color:#f92672>=</span> <span style=color:#66d9ef>True</span>
</span></span><span style=display:flex><span>            <span style=color:#75715e># Print content</span>
</span></span><span style=display:flex><span>            print(delta<span style=color:#f92672>.</span>content, end<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;&#39;</span>, flush<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>            content <span style=color:#f92672>+=</span> delta<span style=color:#f92672>.</span>content
</span></span></code></pre></div><h3 id=未来工作>未来工作</h3><p>未来，我们将继续探索将智能体与RL集成，以实现长时推理，目标是通过推理时间扩展来释放更高的智能。
这是Qwen在大规模强化学习（RL）以增强推理能力方面的第一步。通过这一旅程，Qwen认识到预训练语言模型中尚未开发的可能性。更强大的基础模型与依托规模化计算资源的RL相结合，将会使Qwen更接近实现人工通用智能（AGI）。</p><h3 id=参考链接>参考链接</h3><ul><li>Demo: <a href=https://huggingface.co/spaces/Qwen/QwQ-32B-Demo>https://huggingface.co/spaces/Qwen/QwQ-32B-Demo</a></li><li>Qwen 官方文档:<ul><li><a href=https://qwen.readthedocs.io/en/latest/>https://qwen.readthedocs.io/en/latest/</a></li></ul></li><li>Qwen 博客<ul><li><a href=https://qwenlm.github.io/zh/blog/qwq-32b/>https://qwenlm.github.io/zh/blog/qwq-32b/</a></li></ul></li><li>大模型访问和下载:<ul><li><a href=https://modelscope.cn/models/Qwen/QwQ-32B>https://modelscope.cn/models/Qwen/QwQ-32B</a></li><li><a href=https://huggingface.co/Qwen/QwQ-32B>https://huggingface.co/Qwen/QwQ-32B</a></li></ul></li></ul></div><footer class=article-footer><div class=article-navigation><a href=/zh/cod-chain-of-draft/ class="nav-link prev"><span class=nav-arrow>←</span>
<span class=nav-title>Chain of Draft 论文解读</span></a>
<a href=/zh/agi/ai_usa_impact/ class="nav-link next"><span class=nav-title>雷·达里奥：美国隐藏的内战，以及在技术，经济和学术界击败中国的竞赛</span>
<span class=nav-arrow>→</span></a></div></footer></article></main></div><style>:root{--primary:#2196f3;--primary-dark:#1a237e;--secondary:#64b5f6;--theme:#121212;--entry:#1e1e1e;--border:#2d2d2d;--code-bg:#2d2d2d;--text:#e0e0e0;--text-secondary:#b0b0b0}.article-content table{width:100%;border-collapse:collapse;margin:1.5rem 0;background:var(--entry);border-radius:8px;overflow:hidden;box-shadow:0 2px 4px rgba(0,0,0,.1)}.article-content th,.article-content td{padding:.8rem;text-align:left;border:1px solid var(--border)}.article-content th{background:var(--primary-dark);color:var(--text);font-weight:600}.article-content tr:nth-child(even){background:var(--code-bg)}.article-content tr:hover{background:rgba(33,150,243,.1)}.site-container{display:flex;gap:2rem;max-width:1200px;margin:0 auto;padding:1.5rem}.sidebar{flex:0 0 250px;position:sticky;top:1.5rem;height:fit-content}.back-home{margin-bottom:1.5rem}.back-link{display:flex;align-items:center;gap:.5rem;color:var(--text-secondary);text-decoration:none;padding:.5rem;border-radius:6px;transition:all .3s}.back-link:hover{background:var(--code-bg);color:var(--primary)}.back-arrow{font-size:1.2rem}.toc-container{background:var(--entry);border-radius:12px;padding:1.2rem;border:1px solid var(--border);box-shadow:0 4px 6px rgba(0,0,0,.3)}.toc-title{color:var(--primary);margin:0 0 1rem;font-size:1.1rem}.toc-content{font-size:.9rem;line-height:1.6}.toc-content ul{list-style:none;padding-left:.5rem;margin:0}.toc-content li{margin:.3rem 0}.toc-content a{color:var(--text-secondary);text-decoration:none;transition:all .3s;display:block;padding:.2rem .5rem;border-radius:4px}.toc-content a:hover{color:var(--primary);background:var(--code-bg)}.main-content{flex:1;min-width:0}.article-container{background:var(--entry);border-radius:12px;padding:2rem;box-shadow:0 2px 4px rgba(0,0,0,.1)}.article-header{margin-bottom:2rem;padding-bottom:1rem;border-bottom:1px solid var(--border)}.article-title{font-size:2rem;color:var(--primary);margin-bottom:1rem;line-height:1.3}.article-meta{display:flex;flex-wrap:wrap;gap:1rem;align-items:center;color:var(--text-secondary);font-size:.9rem}.article-date{background:var(--code-bg);padding:.3rem .6rem;border-radius:4px;font-family:monospace}.article-categories{display:flex;gap:.5rem}.article-tags{display:flex;gap:.5rem;flex-wrap:wrap}.article-content{color:var(--text);line-height:1.8;font-size:1.1rem}.article-content h2{color:var(--primary);margin:2rem 0 1rem;font-size:1.5rem}.article-content h3{color:var(--primary);margin:1.5rem 0 1rem;font-size:1.3rem}.article-content p{margin:1rem 0}.article-content code{background:var(--code-bg);padding:.2rem .4rem;border-radius:4px;font-family:monospace;font-size:.9em}.article-content pre{background:var(--code-bg);padding:1rem;border-radius:8px;overflow-x:auto;margin:1.5rem 0}.article-content blockquote{border-left:4px solid var(--primary);padding-left:1rem;margin:1.5rem 0;color:var(--text-secondary)}.article-content img{max-width:100%;height:auto;border-radius:8px;margin:1.5rem 0}.article-footer{margin-top:3rem;padding-top:2rem;border-top:1px solid var(--border)}.article-navigation{display:flex;justify-content:space-between;gap:1rem}.nav-link{display:flex;align-items:center;gap:.5rem;color:var(--text-secondary);text-decoration:none;padding:.5rem 1rem;border-radius:6px;transition:all .3s;max-width:45%}.nav-link:hover{background:var(--code-bg);color:var(--primary)}.nav-arrow{font-size:1.2rem}.nav-title{overflow:hidden;text-overflow:ellipsis;white-space:nowrap}@media(max-width:768px){.site-container{flex-direction:column;padding:1rem}.sidebar{position:static;width:100%;max-height:none}.article-container{padding:1.5rem}.article-title{font-size:1.5rem}.article-meta{flex-direction:column;align-items:flex-start}.article-navigation{flex-direction:column}.nav-link{max-width:100%}}</style><footer><p>&copy; 2025 My AI Blog. All rights reserved.</p><p><a href=https://github.com/hobbytp/hobbytp.github.io target=_blank>GitHub</a> |
<a href=https://github.com/hobbytp/hobbytp.github.io/issues target=_blank>反馈</a></p></footer><script>document.addEventListener("DOMContentLoaded",function(){document.querySelectorAll("pre > code.language-mermaid").forEach(function(e){const n=e.parentElement,t=document.createElement("div");t.className="mermaid",t.textContent=e.textContent,n.parentElement.replaceChild(t,n)}),window.mermaid&&mermaid.init()})</script></body></html>