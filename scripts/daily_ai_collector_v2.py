#!/usr/bin/env python3
"""
æ¯æ—¥AIåŠ¨æ€æ”¶é›†è„šæœ¬ V2.0 - ä¸“ä¸šç‰ˆ
è‡ªåŠ¨æ”¶é›†AIé¢†åŸŸçš„æœ€æ–°åŠ¨æ€ï¼ŒåŒ…æ‹¬æ–°æ¨¡å‹ã€æ–°æ¡†æ¶ã€æ–°åº”ç”¨ç­‰
æ–°å¢åŠŸèƒ½ï¼š
- Perplexity API é›†æˆï¼ˆè·å– AI æ–°é—»å’Œè¶‹åŠ¿ï¼‰
- ç¼©çŸ­æ—¶é—´çª—å£è‡³ 24 å°æ—¶ï¼ˆçœŸæ­£çš„æ¯æ—¥åŠ¨æ€ï¼‰
- æ•°æ®å»é‡åŠŸèƒ½
- å†…å®¹è´¨é‡è¯„åˆ†å’Œæ’åº
- ç®€åŒ–çš„åˆ†ç±»ä½“ç³»
"""

import os
import json
import requests
import datetime
from typing import List, Dict, Any, Set
from pathlib import Path
import yaml
import hashlib
import re
import sys

# æ·»åŠ è„šæœ¬ç›®å½•åˆ°è·¯å¾„ä»¥ä¾¿å¯¼å…¥
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
try:
    from update_word_count import calculate_reading_stats
except ImportError:
    # Fallback implementation if import fails
    def calculate_reading_stats(text, reading_speed=400):
        # Fallback: count Chinese characters using regex (same as update_word_count.py)
        # \u4e00-\u9fa5: Chinese chars
        # \u3000-\u303f: CJK symbols and punctuation
        # \uff00-\uffef: Fullwidth symbols
        word_count = len(re.findall(r'[\u4e00-\u9fa5\u3000-\u303f\uff00-\uffef]', text))
        # If no Chinese chars found (e.g. English text), fall back to simple length or word count
        if word_count == 0 and len(text) > 0:
             word_count = len(text.split())
             
        reading_time = max(1, (word_count + reading_speed - 1) // reading_speed)
        return word_count, reading_time

# å°è¯•å¯¼å…¥ Google Gemini SDK
try:
    import google.generativeai as genai
    USE_GOOGLE_SDK = True
    print("[OK] google.generativeai åº“å¯¼å…¥æˆåŠŸ")
except ImportError as e:
    USE_GOOGLE_SDK = False
    print(f"[WARNING] google.generativeai åº“å¯¼å…¥å¤±è´¥: {e}")
    try:
        import openai
        print("[OK] openai åº“å¯¼å…¥æˆåŠŸï¼ˆå›é€€æ¨¡å¼ï¼‰")
    except ImportError:
        print("ERROR: æ—¢æ²¡æœ‰ google-generativeai ä¹Ÿæ²¡æœ‰ openai åº“")
        openai = None

# å°è¯•å¯¼å…¥ Perplexity SDK
try:
    from perplexity import Perplexity
    USE_PERPLEXITY = True
    print("[OK] perplexity åº“å¯¼å…¥æˆåŠŸ")
except ImportError as e:
    USE_PERPLEXITY = False
    print(f"[WARNING] perplexity åº“å¯¼å…¥å¤±è´¥: {e}")
    print("   å®‰è£…: pip install perplexityai")

# å°è¯•å¯¼å…¥ ai_news_collector_lib
try:
    from ai_news_collector_lib import (
        AdvancedAINewsCollector,
        AdvancedSearchConfig,
        ReportGenerator,
    )
    USE_AI_NEWS_LIB = True
    print("[OK] ai_news_collector_lib åº“å¯¼å…¥æˆåŠŸ")
except ImportError as e:
    try:
        from ai_news_collector import (
            AdvancedAINewsCollector, 
            AdvancedSearchConfig,
            ReportGenerator,
        )
        USE_AI_NEWS_LIB = True
        print("[OK] ai_news_collector åº“å¯¼å…¥æˆåŠŸï¼ˆå›é€€æ¨¡å¼ï¼‰")
    except ImportError as e2:
        USE_AI_NEWS_LIB = False
        print(f"[WARNING] ai_news_collector_lib åº“å¯¼å…¥å¤±è´¥: {e}")
        print("   å®‰è£…: pip install ai-news-collector-lib[advanced]")

class DailyAICollectorV2:
    def __init__(self):
        # åˆå§‹åŒ– Gemini API
        gemini_key = os.getenv('GEMINI_API_KEY')
        if not gemini_key:
            print("WARNING: GEMINI_API_KEY æœªè®¾ç½®ï¼")
            self.ai_client = None
            self.use_google_sdk = False
        else:
            print(f"GEMINI_API_KEY å·²è®¾ç½® (é•¿åº¦: {len(gemini_key)})")
            
            if USE_GOOGLE_SDK:
                try:
                    genai.configure(api_key=gemini_key)
                    self.ai_client = genai.GenerativeModel('gemini-2.5-flash')
                    self.use_google_sdk = True
                    print("[OK] Google Gemini SDK åˆå§‹åŒ–æˆåŠŸ (æ¨¡å‹: gemini-2.5-flash)")
                except Exception as e:
                    print(f"ERROR: Google SDK åˆå§‹åŒ–å¤±è´¥: {e}")
                    self.ai_client = None
                    self.use_google_sdk = False
            elif openai:
                try:
                    self.ai_client = openai.OpenAI(
                        api_key=gemini_key,
                        base_url="https://generativelanguage.googleapis.com/v1beta/openai/"
                    )
                    self.use_google_sdk = False
                    print("[OK] OpenAIå…¼å®¹å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
                except Exception as e:
                    print(f"ERROR: OpenAIå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
                    self.ai_client = None
                    self.use_google_sdk = False
            else:
                print("ERROR: æ²¡æœ‰å¯ç”¨çš„ AI å®¢æˆ·ç«¯åº“")
                self.ai_client = None
                self.use_google_sdk = False

        # åˆå§‹åŒ– Perplexity API
        self.perplexity_key = os.getenv('PERPLEXITY_API_KEY')
        if USE_PERPLEXITY and self.perplexity_key:
            try:
                self.perplexity_client = Perplexity(api_key=self.perplexity_key)
                print(f"[OK] Perplexity API åˆå§‹åŒ–æˆåŠŸ (keyé•¿åº¦: {len(self.perplexity_key)})")
            except Exception as e:
                print(f"ERROR: Perplexity åˆå§‹åŒ–å¤±è´¥: {e}")
                self.perplexity_client = None
        else:
            self.perplexity_client = None
            if not USE_PERPLEXITY:
                print("[WARNING] Perplexity SDK æœªå®‰è£…")
            elif not self.perplexity_key:
                print("[WARNING] PERPLEXITY_API_KEY æœªè®¾ç½®")

        self.github_token = os.getenv('GITHUB_TOKEN')
        self.hf_token = os.getenv('HUGGINGFACE_API_KEY')
        
        # æ£€æŸ¥ ai_news_collector_lib éœ€è¦çš„ç¯å¢ƒå˜é‡
        self.news_api_key = os.getenv('NEWS_API_KEY')
        self.tavily_api_key = os.getenv('TAVILY_API_KEY')
        self.google_search_api_key = os.getenv('GOOGLE_SEARCH_API_KEY')
        self.google_search_engine_id = os.getenv('GOOGLE_SEARCH_ENGINE_ID')
        self.serper_api_key = os.getenv('SERPER_API_KEY')
        self.brave_search_api_key = os.getenv('BRAVE_SEARCH_API_KEY')
        self.metasosearch_api_key = os.getenv('METASOSEARCH_API_KEY')
        
        print(f"GitHub Token: {'å·²è®¾ç½®' if self.github_token else 'æœªè®¾ç½®'}")
        print(f"HuggingFace Token: {'å·²è®¾ç½®' if self.hf_token else 'æœªè®¾ç½®'}")
        print(f"News API Key: {'å·²è®¾ç½®' if self.news_api_key else 'æœªè®¾ç½®'}")
        print(f"Tavily API Key: {'å·²è®¾ç½®' if self.tavily_api_key else 'æœªè®¾ç½®'}")
        print(f"Google Search API Key: {'å·²è®¾ç½®' if self.google_search_api_key else 'æœªè®¾ç½®'}")
        print(f"Google Search Engine ID: {'å·²è®¾ç½®' if self.google_search_engine_id else 'æœªè®¾ç½®'}")
        print(f"Serper API Key: {'å·²è®¾ç½®' if self.serper_api_key else 'æœªè®¾ç½®'}")
        print(f"Brave Search API Key: {'å·²è®¾ç½®' if self.brave_search_api_key else 'æœªè®¾ç½®'}")
        print(f"Metasosearch API Key: {'å·²è®¾ç½®' if self.metasosearch_api_key else 'æœªè®¾ç½®'}")
        
        self.content_dir = Path("content/zh/daily_ai")
        self.content_dir.mkdir(exist_ok=True)
        
        # ç”¨äºå»é‡çš„é›†åˆ
        self.seen_urls: Set[str] = set()
        self.seen_titles: Set[str] = set()
        
    def get_date_range(self, hours_back: int = 24) -> tuple:
        """è·å–æ—¶é—´èŒƒå›´ï¼ˆé»˜è®¤è¿‡å»24å°æ—¶ï¼‰"""
        now = datetime.datetime.now(datetime.timezone.utc)
        today_8am = now.replace(hour=8, minute=0, second=0, microsecond=0)
        start_time = today_8am - datetime.timedelta(hours=hours_back)
        return start_time, today_8am
    
    def load_history_items(self, days_back: int = 7) -> Set[str]:
        """åŠ è½½æœ€è¿‘Nå¤©çš„å†å²é¡¹ç›®ï¼Œç”¨äºå»é‡"""
        history_urls = set()
        history_titles = set()
        
        now = datetime.datetime.now(datetime.timezone.utc)
        for i in range(days_back):
            date = now - datetime.timedelta(days=i)
            file_path = self.content_dir / f"{date.strftime('%Y-%m-%d')}.md"
            
            if file_path.exists():
                try:
                    content = file_path.read_text(encoding='utf-8')
                    # æå– URL
                    urls = re.findall(r'https?://[^\s\)]+', content)
                    history_urls.update(urls)
                    # æå–æ ‡é¢˜ï¼ˆMarkdown headingï¼‰
                    titles = re.findall(r'###\s+(.+)', content)
                    history_titles.update(titles)
                except Exception as e:
                    print(f"è¯»å–å†å²æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
        
        print(f"åŠ è½½äº† {len(history_urls)} ä¸ªå†å²URLï¼Œ{len(history_titles)} ä¸ªå†å²æ ‡é¢˜")
        return history_urls | history_titles
    
    def is_duplicate(self, item: Dict) -> bool:
        """æ£€æŸ¥é¡¹ç›®æ˜¯å¦é‡å¤"""
        # æ£€æŸ¥ URL
        url = item.get('html_url') or item.get('link') or item.get('url', '')
        if url and url in self.seen_urls:
            return True
        if url:
            self.seen_urls.add(url)
        
        # æ£€æŸ¥æ ‡é¢˜
        title = item.get('name') or item.get('title') or item.get('modelId', '')
        if title and title in self.seen_titles:
            return True
        if title:
            self.seen_titles.add(title)
        
        return False
    
    def is_within_time_range(self, item: Dict, source: str) -> bool:
        """æ£€æŸ¥é¡¹ç›®æ˜¯å¦åœ¨æ—¶é—´èŒƒå›´å†…"""
        yesterday, today = self.get_date_range(hours_back=24)
        
        if source == 'github':
            created_at = item.get('created_at', '')
            if created_at:
                try:
                    create_time = datetime.datetime.strptime(created_at, '%Y-%m-%dT%H:%M:%SZ')
                    return create_time >= yesterday
                except:
                    return False
        
        elif source == 'huggingface':
            created_at = item.get('createdAt', '')
            if created_at:
                try:
                    create_time = datetime.datetime.fromisoformat(created_at.replace('Z', '+00:00'))
                    return create_time >= yesterday
                except:
                    return False
        
        elif source == 'arxiv':
            # ArXiv çš„æ—¶é—´æ£€æŸ¥åœ¨æœç´¢å‚æ•°ä¸­å·²ç»å¤„ç†
            return True
        
        elif source == 'ai_news_lib':
            published_date = item.get('published_date', '')
            if published_date:
                try:
                    if 'T' in published_date:
                        pub_time = datetime.datetime.fromisoformat(published_date.replace('Z', '+00:00'))
                    else:
                        pub_time = datetime.datetime.strptime(published_date, '%Y-%m-%d')
                    return pub_time >= yesterday
                except:
                    return False
        
        return True  # å¦‚æœæ— æ³•ç¡®å®šæ—¶é—´ï¼Œé»˜è®¤é€šè¿‡
    
    def calculate_quality_score(self, item: Dict, source: str) -> float:
        """è®¡ç®—å†…å®¹è´¨é‡åˆ†æ•°ï¼ˆ0-10åˆ†ï¼‰"""
        score = 5.0  # åŸºç¡€åˆ†
        
        if source == 'github':
            stars = item.get('stargazers_count', 0)
            score += min(stars / 150, 4.0)  # æé«˜ stars æƒé‡ï¼ˆæœ€å¤šåŠ 4åˆ†ï¼‰
            
            # æ˜¯å¦æœ‰è¯¦ç»†æè¿°
            desc_len = len(item.get('description', ''))
            if desc_len > 100:
                score += 1.5  # æé«˜æè¿°æƒé‡
            elif desc_len > 50:
                score += 1.0
            
            # æœ€è¿‘æ›´æ–°æ—¶é—´
            updated_at = item.get('updated_at', '')
            if updated_at:
                try:
                    update_time = datetime.datetime.strptime(updated_at, '%Y-%m-%dT%H:%M:%SZ')
                    days_ago = (datetime.datetime.now() - update_time).days
                    if days_ago <= 1:
                        score += 1.5  # æé«˜æ—¶æ•ˆæ€§æƒé‡
                    elif days_ago <= 7:
                        score += 1.0
                except:
                    pass
        
        elif source == 'huggingface':
            downloads = item.get('downloads', 0)
            score += min(downloads / 500, 3.0)  # æé«˜ downloads æƒé‡ï¼ˆæœ€å¤šåŠ 3åˆ†ï¼‰
            
            # æ˜¯å¦æœ‰ pipeline_tag
            if item.get('pipeline_tag'):
                score += 1.0  # æé«˜æ ‡ç­¾æƒé‡
        
        elif source == 'arxiv':
            # ä½œè€…æ•°é‡
            authors = len(item.get('authors', []))
            score += min(authors / 3, 1.5)  # æé«˜ä½œè€…æƒé‡
            
            # æ‘˜è¦é•¿åº¦
            summary_len = len(item.get('summary', ''))
            if 200 < summary_len < 2000:
                score += 1.5  # æé«˜æ‘˜è¦è´¨é‡æƒé‡
            elif 100 < summary_len < 3000:
                score += 1.0
        
        elif source == 'perplexity':
            # Perplexity ç»“æœé€šå¸¸è´¨é‡è¾ƒé«˜
            score += 2.0
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å‘å¸ƒæ—¥æœŸ
            if item.get('date'):
                score += 0.5
        
        elif source == 'ai_news_lib':
            # ai_news_collector_lib ç»“æœè´¨é‡è¾ƒé«˜ï¼ˆå¤šæºèšåˆï¼‰
            score += 2.5
            
            # æ£€æŸ¥å…³é”®è¯æ•°é‡
            keywords = item.get('keywords', [])
            if len(keywords) > 3:
                score += 1.0
            elif len(keywords) > 0:
                score += 0.5
                
            # æ£€æŸ¥å‘å¸ƒæ—¥æœŸ
            if item.get('published_date'):
                score += 0.5
        
        elif source == 'google_focus':
            # Google Search ä»Šæ—¥ç„¦ç‚¹ï¼Œè´¨é‡é€šå¸¸è¾ƒé«˜
            score += 2.5
            
            # æ£€æŸ¥æ ‡é¢˜é•¿åº¦å’Œå†…å®¹ä¸°å¯Œåº¦
            title_len = len(item.get('title', ''))
            snippet_len = len(item.get('snippet', ''))
            
            if title_len > 20 and snippet_len > 100:
                score += 1.5
            elif title_len > 10 and snippet_len > 50:
                score += 1.0
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å‘å¸ƒæ—¶é—´
            if item.get('published_date'):
                score += 1.0
        
        elif source == 'applications':
            # åº”ç”¨ä¸äº§å“ï¼Œå¤šæºå¹¶è¡Œæœç´¢
            score += 2.0
            
            # æ£€æŸ¥å…³é”®è¯
            keywords = item.get('keywords', [])
            if len(keywords) > 3:
                score += 1.5
            elif len(keywords) > 0:
                score += 0.5
            
            # æ£€æŸ¥å†…å®¹ä¸°å¯Œåº¦
            snippet_len = len(item.get('snippet', ''))
            if snippet_len > 150:
                score += 1.0
            elif snippet_len > 80:
                score += 0.5
        
        return min(score, 10.0)  # æœ€é«˜10åˆ†
    
    def search_perplexity_ai_news(self) -> List[Dict]:
        """ä½¿ç”¨ Perplexity æœç´¢ AI æ–°é—»å’Œè¶‹åŠ¿"""
        if not self.perplexity_client:
            print("WARNING: Perplexity å®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œè·³è¿‡æœç´¢")
            return []
        
        try:
            yesterday, today = self.get_date_range(hours_back=24)
            date_str = yesterday.strftime('%Y-%m-%d')
            today_str = today.strftime('%Y-%m-%d')
            
            # å¤šæŸ¥è¯¢æœç´¢ï¼šæœ€è¿‘24å°æ—¶çš„ AI æ–°é—»ï¼ˆä½¿ç”¨æ›´ç²¾ç¡®çš„æ—¥æœŸè¿‡æ»¤ï¼‰
            queries = [
                f"AI news breakthroughs after {date_str} before {today_str}",
                f"new AI model releases after {date_str}",
                f"AI tools frameworks launched after {date_str}"
            ]
            
            print(f"ä½¿ç”¨ Perplexity æœç´¢: {queries}")
            
            search = self.perplexity_client.search.create(
                query=queries,
                max_results=5,
                max_tokens_per_page=1024
            )
            
            all_results = []
            query_types = ['news', 'models', 'tools']
            
            if hasattr(search, 'results'):
                # å¤„ç†å¤šæŸ¥è¯¢ç»“æœ
                for query_idx, query_results in enumerate(search.results):
                    # å®‰å…¨è·å– query_typeï¼Œè¶…å‡ºç´¢å¼•åˆ™ä½¿ç”¨ 'general'
                    query_type = query_types[query_idx] if query_idx < len(query_types) else 'general'
                    
                    if isinstance(query_results, list):
                        # query_results æ˜¯åˆ—è¡¨
                        for result in query_results:
                            item = {
                                'title': result.title if hasattr(result, 'title') else '',
                                'url': result.url if hasattr(result, 'url') else '',
                                'snippet': result.snippet if hasattr(result, 'snippet') else '',
                                'date': result.date if hasattr(result, 'date') else '',
                                'query_type': query_type
                            }
                            all_results.append(item)
                    else:
                        # query_results æ˜¯å•ä¸ªç»“æœå¯¹è±¡
                        item = {
                            'title': query_results.title if hasattr(query_results, 'title') else '',
                            'url': query_results.url if hasattr(query_results, 'url') else '',
                            'snippet': query_results.snippet if hasattr(query_results, 'snippet') else '',
                            'date': query_results.date if hasattr(query_results, 'date') else '',
                            'query_type': query_type
                        }
                        all_results.append(item)
            
            print(f"Perplexity æ‰¾åˆ° {len(all_results)} æ¡ç»“æœ")
            return all_results
            
        except Exception as e:
            print(f"Perplexity æœç´¢é”™è¯¯: {e}")
            print(f"é”™è¯¯ç±»å‹: {type(e).__name__}")
            import traceback
            print("å®Œæ•´é”™è¯¯å †æ ˆ:")
            traceback.print_exc()
            print("æç¤º: Perplexity API è°ƒç”¨å¤±è´¥ï¼Œå°†è·³è¿‡æ–°é—»æœç´¢")
            return []

    def search_ai_news_lib(self) -> List[Dict]:
        """ä½¿ç”¨ ai_news_collector_lib æœç´¢å¤šæº AI æ–°é—»"""
        if not USE_AI_NEWS_LIB:
            print("WARNING: ai_news_collector_lib æœªå¯¼å…¥ï¼Œè·³è¿‡æœç´¢")
            return []
        
        try:
            yesterday, today = self.get_date_range(hours_back=24)
            
            # æ„å»ºæœç´¢é…ç½® - å¯ç”¨æ‚¨é…ç½®çš„æ‰€æœ‰APIæº
            search_config = AdvancedSearchConfig(
                # åŸºç¡€æº
                enable_hackernews=True,
                enable_arxiv=True,
                enable_duckduckgo=True,
                enable_rss_feeds=True,
                
                # æ‚¨é…ç½®çš„APIæº
                enable_newsapi=True,        # NEWS_API_KEY
                enable_tavily=True,         # TAVILY_API_KEY  
                enable_google_search=True,  # GOOGLE_SEARCH_API_KEY
                enable_serper=True,         # SERPER_API_KEY
                enable_brave_search=True,   # BRAVE_SEARCH_API_KEY
                enable_metasota_search=True, # METASOSEARCH_API_KEY
                
                # æœç´¢å‚æ•° - ä¸¥æ ¼é™åˆ¶æ—¶é—´èŒƒå›´
                max_articles_per_source=3,
                days_back=0,  # æ”¹ä¸º0ï¼Œåªæœç´¢ä»Šå¤©çš„å†…å®¹
                similarity_threshold=0.85,
                
                # é«˜çº§åŠŸèƒ½
                enable_content_extraction=False,  # å‡å°‘å¤„ç†æ—¶é—´
                enable_keyword_extraction=True,
                cache_results=False,  # ç¦ç”¨ç¼“å­˜ï¼Œé¿å…æ—§ä¿¡æ¯é‡å¤ä½¿ç”¨
            )
            
            # åˆ›å»ºæ”¶é›†å™¨
            collector = AdvancedAINewsCollector(search_config)
            
            # å®šä¹‰æœç´¢ä¸»é¢˜ï¼ˆèšç„¦24å°æ—¶å†…çš„AIåŠ¨æ€ï¼‰
            topics = [
                "latest AI model releases today",
                "new AI tools and frameworks launched",
                "AI research breakthroughs and papers",
                "AI company news and product updates"
            ]
            
            print(f"ä½¿ç”¨ ai_news_collector_lib æœç´¢ AI æ–°é—»...")
            
            # å¼‚æ­¥æ”¶é›†ï¼ˆå¦‚æœæ”¯æŒæ‰¹é‡ä¸»é¢˜ï¼‰
            import asyncio
            
            async def collect_async():
                if hasattr(collector, 'collect_multiple_topics'):
                    return await collector.collect_multiple_topics(topics)
                else:
                    # å…¼å®¹å›é€€ï¼šé€ä¸»é¢˜æ”¶é›†
                    results = []
                    for topic in topics:
                        if hasattr(collector, 'collect_news_advanced'):
                            result = await collector.collect_news_advanced(topic)
                            results.append(result)
                    
                    # åˆå¹¶ç»“æœ
                    all_articles = []
                    for r in results:
                        all_articles.extend(r.get('articles', []))
                    return {"articles": all_articles, "unique_articles": len(all_articles)}
            
            # è¿è¡Œå¼‚æ­¥æ”¶é›†
            result = asyncio.run(collect_async())
            articles = result.get('articles', [])
            
            print(f"ai_news_collector_lib æ‰¾åˆ° {len(articles)} æ¡ç»“æœ")
            print(f"å¼€å§‹æ—¶é—´è¿‡æ»¤ï¼Œç›®æ ‡æ—¶é—´èŒƒå›´: {yesterday.strftime('%Y-%m-%d %H:%M')} åˆ° {today.strftime('%Y-%m-%d %H:%M')}")
            
            # è½¬æ¢ä¸ºç»Ÿä¸€æ ¼å¼å¹¶ä¸¥æ ¼è¿‡æ»¤æ—¶é—´
            formatted_results = []
            for article in articles:
                # æ£€æŸ¥å‘å¸ƒæ—¶é—´ - ai_news_collector_libä½¿ç”¨'published'å­—æ®µ
                published_date = article.get('published', '') or article.get('published_date', '')
                
                # å¦‚æœæ²¡æœ‰å‘å¸ƒæ—¶é—´ï¼Œç›´æ¥è·³è¿‡
                if not published_date or published_date == '':
                    print(f"è·³è¿‡æ— å‘å¸ƒæ—¶é—´æ–‡ç« : {article.get('title', '')[:50]}...")
                    continue
                
                try:
                    # è§£æå‘å¸ƒæ—¶é—´
                    if 'T' in published_date:
                        pub_time = datetime.datetime.fromisoformat(published_date.replace('Z', '+00:00'))
                    else:
                        pub_time = datetime.datetime.strptime(published_date, '%Y-%m-%d')
                    
                    # ç¡®ä¿æ—¶åŒºä¸€è‡´æ€§ - å¦‚æœpub_timeæ²¡æœ‰æ—¶åŒºä¿¡æ¯ï¼Œå‡è®¾ä¸ºUTC
                    if pub_time.tzinfo is None:
                        pub_time = pub_time.replace(tzinfo=datetime.timezone.utc)
                    
                    # æ£€æŸ¥å‘å¸ƒæ—¶é—´æ˜¯å¦åœ¨åˆç†èŒƒå›´å†…ï¼ˆä¸èƒ½æ˜¯æœªæ¥æ—¶é—´ï¼‰
                    now = datetime.datetime.now(datetime.timezone.utc)
                    if pub_time > now:
                        print(f"è·³è¿‡æœªæ¥æ—¶é—´æ–‡ç« : {article.get('title', '')[:50]}... (å‘å¸ƒæ—¶é—´: {published_date})")
                        continue
                    
                    # é¢å¤–æ£€æŸ¥ï¼šå¦‚æœå‘å¸ƒæ—¶é—´è¶…è¿‡7å¤©ï¼Œä¹Ÿè·³è¿‡ï¼ˆé˜²æ­¢æç«¯æƒ…å†µï¼‰
                    if pub_time < now - datetime.timedelta(days=7):
                        print(f"è¿‡æ»¤æ‰è¿‡æ—§æ–‡ç« : {article.get('title', '')[:50]}... (å‘å¸ƒæ—¶é—´: {published_date})")
                        continue
                    
                    # æ£€æŸ¥æ˜¯å¦åœ¨24å°æ—¶å†…
                    if pub_time < yesterday:
                        print(f"è¿‡æ»¤æ‰è¿‡æœŸæ–‡ç« : {article.get('title', '')[:50]}... (å‘å¸ƒæ—¶é—´: {published_date})")
                        continue
                        
                except Exception as e:
                    print(f"æ—¶é—´è§£æé”™è¯¯: {published_date}, é”™è¯¯: {e}")
                    # å¦‚æœæ—¶é—´è§£æå¤±è´¥ï¼Œè·³è¿‡è¯¥æ–‡ç« 
                    continue
                
                item = {
                    'title': article.get('title', ''),
                    'url': article.get('url', ''),
                    'snippet': article.get('description', '')[:300],
                    'source': article.get('source', 'ai_news_lib'),
                    'published_date': published_date,
                    'keywords': article.get('keywords', [])
                }
                formatted_results.append(item)
            
            print(f"ai_news_collector_lib æ—¶é—´è¿‡æ»¤åå‰©ä½™ {len(formatted_results)} æ¡ç»“æœ")
            return formatted_results
            
        except Exception as e:
            print(f"ai_news_collector_lib æœç´¢é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
            print("æç¤º: ai_news_collector_lib è°ƒç”¨å¤±è´¥ï¼Œå°†è·³è¿‡æ­¤æ•°æ®æº")
            return []
    
    def search_focus_news(self) -> List[Dict]:
        """ä½¿ç”¨ Google Search æœç´¢å¤§æ¨¡å‹å‚å•†ç›¸å…³æ–°é—»ï¼ˆä»Šæ—¥ç„¦ç‚¹ï¼‰"""
        if not self.google_search_api_key or not self.google_search_engine_id:
            print("WARNING: Google Search API æœªé…ç½®ï¼Œè·³è¿‡ä»Šæ—¥ç„¦ç‚¹æœç´¢")
            return []
        
        try:
            yesterday, today = self.get_date_range(hours_back=24)
            
            # å®šä¹‰å¤§æ¨¡å‹å‚å•†å…³é”®è¯
            companies = [
                "OpenAI", "Google Gemini", "Anthropic Claude", 
                "xAI Grok", "Meta Llama", "Mistral", "Microsoft","Apple","Amazon",
                "Qwen", "é€šä¹‰åƒé—®","DeepSeek", "seedream","å­—èŠ‚è·³åŠ¨","GLM", "æ™ºè°±", "Kimi", "æœˆä¹‹æš—é¢"
            ]
            
            # æ„å»ºæœç´¢æŸ¥è¯¢
            query = f"({' OR '.join(companies)}) AND (AI OR å¤§æ¨¡å‹ OR å‘å¸ƒ OR release OR announcement OR æ›´æ–°) after:{yesterday.strftime('%Y-%m-%d')}"
            
            print(f"ä½¿ç”¨ Google Search æœç´¢ä»Šæ—¥ç„¦ç‚¹: {query[:100]}...")
            
            url = "https://www.googleapis.com/customsearch/v1"
            params = {
                'key': self.google_search_api_key,
                'cx': self.google_search_engine_id,
                'q': query,
                'num': 10,  # æœ€å¤šè¿”å›10æ¡
                'dateRestrict': 'd1',  # è¿‡å»1å¤©
                'sort': 'date'  # æŒ‰æ—¥æœŸæ’åº
            }
            
            response = requests.get(url, params=params)
            print(f"Google Search APIå“åº”çŠ¶æ€: {response.status_code}")
            
            if response.status_code == 200:
                data = response.json()
                items = data.get('items', [])
                
                formatted_results = []
                for item in items:
                    news_item = {
                        'title': item.get('title', ''),
                        'url': item.get('link', ''),
                        'snippet': item.get('snippet', '')[:300],
                        'source': 'google_search',
                        'published_date': item.get('pagemap', {}).get('metatags', [{}])[0].get('article:published_time', ''),
                    }
                    
                    # å»é‡æ£€æŸ¥
                    if not self.is_duplicate(news_item):
                        news_item['quality_score'] = self.calculate_quality_score(news_item, 'google_focus')
                        formatted_results.append(news_item)
                
                # æŒ‰è´¨é‡è¯„åˆ†æ’åº
                formatted_results.sort(key=lambda x: x.get('quality_score', 0), reverse=True)
                
                print(f"Google Search ä»Šæ—¥ç„¦ç‚¹æ‰¾åˆ° {len(formatted_results)} æ¡ç»“æœ")
                return formatted_results[:5]  # è¿”å›å‰5æ¡é«˜è´¨é‡æ–°é—»
            else:
                print(f"Google Search APIé”™è¯¯: {response.status_code}")
                if response.status_code == 429:
                    print("æç¤º: APIé…é¢å·²ç”¨å®Œï¼Œè¯·ç¨åå†è¯•")
                    
        except Exception as e:
            print(f"Google Search æœç´¢é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
        
        return []
    
    def search_applications(self) -> List[Dict]:
        """ä½¿ç”¨å¤šæºå¹¶è¡Œæœç´¢AIåº”ç”¨ä¸äº§å“ï¼ˆåº”ç”¨ä¸äº§å“ç« èŠ‚ï¼‰"""
        if not USE_AI_NEWS_LIB:
            print("WARNING: ai_news_collector_lib æœªå¯¼å…¥ï¼Œè·³è¿‡åº”ç”¨æœç´¢")
            return []
        
        try:
            yesterday, today = self.get_date_range(hours_back=24)
            
            # æ„å»ºæœç´¢é…ç½® - ä»…å¯ç”¨æŒ‡å®šçš„APIæºï¼ˆé’ˆå¯¹åº”ç”¨ä¸äº§å“ï¼‰
            search_config = AdvancedSearchConfig(
                # ç¦ç”¨åŸºç¡€æº
                enable_hackernews=False,
                enable_arxiv=False,
                enable_duckduckgo=False,
                enable_rss_feeds=False,
                
                # ä»…å¯ç”¨æŒ‡å®šçš„APIæº
                enable_newsapi=True,        # NEWS_API_KEY
                enable_tavily=True,         # TAVILY_API_KEY  
                enable_google_search=True,  # GOOGLE_SEARCH_API_KEY
                enable_serper=True,         # SERPER_API_KEY
                enable_brave_search=True,   # BRAVE_SEARCH_API_KEY
                enable_metasota_search=False, # ä¸ä½¿ç”¨
                
                # æœç´¢å‚æ•°
                max_articles_per_source=5,
                days_back=0,
                similarity_threshold=0.85,
                
                # é«˜çº§åŠŸèƒ½
                enable_content_extraction=False,
                enable_keyword_extraction=True,
                cache_results=False,
            )
            
            # åˆ›å»ºæ”¶é›†å™¨
            collector = AdvancedAINewsCollector(search_config)
            
            # å®šä¹‰åº”ç”¨ä¸äº§å“ç›¸å…³çš„æœç´¢ä¸»é¢˜
            topics = [
                "new AI applications launched today",
                "AI product releases and updates",
                "AI tools for consumers and businesses",
                "AI-powered apps and services"
            ]
            
            print(f"ä½¿ç”¨å¤šæºå¹¶è¡Œæœç´¢ AI åº”ç”¨ä¸äº§å“...")
            
            # å¼‚æ­¥æ”¶é›†
            import asyncio
            
            async def collect_async():
                if hasattr(collector, 'collect_multiple_topics'):
                    return await collector.collect_multiple_topics(topics)
                else:
                    results = []
                    for topic in topics:
                        if hasattr(collector, 'collect_news_advanced'):
                            result = await collector.collect_news_advanced(topic)
                            results.append(result)
                    
                    all_articles = []
                    for r in results:
                        all_articles.extend(r.get('articles', []))
                    return {"articles": all_articles, "unique_articles": len(all_articles)}
            
            result = asyncio.run(collect_async())
            articles = result.get('articles', [])
            
            print(f"åº”ç”¨ä¸äº§å“æœç´¢æ‰¾åˆ° {len(articles)} æ¡ç»“æœ")
            
            # è½¬æ¢æ ¼å¼å¹¶è¿‡æ»¤
            formatted_results = []
            for article in articles:
                published_date = article.get('published', '') or article.get('published_date', '')
                
                if not published_date:
                    continue
                
                try:
                    if 'T' in published_date:
                        pub_time = datetime.datetime.fromisoformat(published_date.replace('Z', '+00:00'))
                    else:
                        pub_time = datetime.datetime.strptime(published_date, '%Y-%m-%d')
                    
                    if pub_time.tzinfo is None:
                        pub_time = pub_time.replace(tzinfo=datetime.timezone.utc)
                    
                    now = datetime.datetime.now(datetime.timezone.utc)
                    if pub_time > now or pub_time < yesterday:
                        continue
                        
                except Exception:
                    continue
                
                item = {
                    'title': article.get('title', ''),
                    'url': article.get('url', ''),
                    'snippet': article.get('description', '')[:300],
                    'source': article.get('source', 'multi_source'),
                    'published_date': published_date,
                    'keywords': article.get('keywords', [])
                }
                
                if not self.is_duplicate(item):
                    item['quality_score'] = self.calculate_quality_score(item, 'applications')
                    formatted_results.append(item)
            
            # æŒ‰è´¨é‡è¯„åˆ†æ’åº
            formatted_results.sort(key=lambda x: x.get('quality_score', 0), reverse=True)
            
            print(f"åº”ç”¨ä¸äº§å“æœç´¢è¿‡æ»¤åå‰©ä½™ {len(formatted_results)} æ¡ç»“æœ")
            return formatted_results[:10]  # è¿”å›å‰10æ¡
            
        except Exception as e:
            print(f"åº”ç”¨ä¸äº§å“æœç´¢é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
            return []
    
    def search_github_trending(self) -> List[Dict]:
        """æœç´¢GitHub Starå¿«é€Ÿå¢é•¿çš„AIé¡¹ç›®ï¼ˆå·¥å…·ä¸æ¡†æ¶ï¼‰"""
        if not self.github_token:
            print("WARNING: GitHub tokenæœªè®¾ç½®ï¼Œè·³è¿‡GitHubæœç´¢")
            return []
            
        headers = {'Authorization': f'Bearer {self.github_token}'}
        url = 'https://api.github.com/search/repositories'
        
        yesterday, today = self.get_date_range(hours_back=24)
        
        # ç­–ç•¥ï¼šæœç´¢æœ€è¿‘7å¤©åˆ›å»ºçš„AIé¡¹ç›®ï¼ŒæŒ‰starsæ’åº
        # æ–°é¡¹ç›®starå¢é•¿å¿«ï¼Œèƒ½åæ˜ å‡ºå¿«é€Ÿè·å¾—å…³æ³¨çš„ä¼˜è´¨é¡¹ç›®
        week_ago = (yesterday - datetime.timedelta(days=6)).strftime('%Y-%m-%d')
        today_str = today.strftime('%Y-%m-%d')
        
        # æ‰©å±•æœç´¢å…³é”®è¯ï¼Œè¦†ç›–æ›´å¤šAIç›¸å…³é¡¹ç›®
        queries = [
            f'AI agent created:{week_ago}..{today_str}',
            f'AI coding created:{week_ago}..{today_str}',
            f'AI Tool created:{week_ago}..{today_str}',
            f'AI Algorithm created:{week_ago}..{today_str}',
            f'AI MCP created:{week_ago}..{today_str}',
            f'Claude Code related created:{week_ago}..{today_str}'
        ]
        
        all_projects = []
        
        for query in queries:
            params = {
                'q': query,
                'sort': 'stars',
                'order': 'desc',
                'per_page': 20
            }
            
            try:
                print(f"æœç´¢GitHubé¡¹ç›®: {query[:60]}...")
                response = requests.get(url, headers=headers, params=params)
                print(f"GitHub APIå“åº”çŠ¶æ€: {response.status_code}")
                
                if response.status_code == 200:
                    data = response.json()
                    items = data.get('items', [])
                    
                    # è®¡ç®—starå¢é•¿ç‡ï¼ˆstars/å¤©æ•°ï¼‰
                    for item in items:
                        created_at = item.get('created_at', '')
                        stars = item.get('stargazers_count', 0)
                        
                        if created_at:
                            try:
                                create_time = datetime.datetime.strptime(created_at, '%Y-%m-%dT%H:%M:%SZ')
                                days_since_creation = max(1, (datetime.datetime.now() - create_time).days)
                                item['stars_per_day'] = stars / days_since_creation
                                item['days_old'] = days_since_creation
                            except:
                                item['stars_per_day'] = 0
                                item['days_old'] = 999
                        else:
                            item['stars_per_day'] = 0
                            item['days_old'] = 999
                        
                        # å»é‡æ£€æŸ¥
                        if not self.is_duplicate(item):
                            item['quality_score'] = self.calculate_quality_score(item, 'github')
                            all_projects.append(item)
                    
                    print(f"æ‰¾åˆ° {len(items)} ä¸ªé¡¹ç›®")
                    
                elif response.status_code == 403:
                    print("GitHub API é™æµï¼Œè·³è¿‡åç»­æŸ¥è¯¢")
                    break
                else:
                    print(f"GitHub APIé”™è¯¯: {response.status_code}")
                    
            except Exception as e:
                print(f"GitHubæœç´¢é”™è¯¯: {e}")
        
        if not all_projects:
            return []
        
        # æŒ‰ star å¢é•¿ç‡æ’åºï¼ˆä¼˜å…ˆï¼‰æˆ–æŒ‰ quality_score æ’åº
        all_projects.sort(key=lambda x: (x.get('stars_per_day', 0), x.get('quality_score', 0)), reverse=True)
        
        print(f"GitHub: å…±æ‰¾åˆ° {len(all_projects)} ä¸ªé¡¹ç›®ï¼ŒæŒ‰starå¢é•¿ç‡æ’åº")
        
        # è¿”å›å‰10ä¸ªå¿«é€Ÿå¢é•¿çš„é¡¹ç›®
        top_projects = all_projects[:10]
        for p in top_projects:
            print(f"  - {p.get('name', '')}: {p.get('stargazers_count', 0)} stars ({p.get('stars_per_day', 0):.1f} stars/day)")
        
        return top_projects
    
    def search_huggingface_models(self) -> List[Dict]:
        """æœç´¢Hugging Faceæ–°æ¨¡å‹ï¼ˆç¼©çŸ­æ—¶é—´çª—å£è‡³24å°æ—¶ï¼‰"""
        if not self.hf_token:
            print("WARNING: Hugging Face tokenæœªè®¾ç½®ï¼Œè·³è¿‡HFæœç´¢")
            return []
            
        headers = {'Authorization': f'Bearer {self.hf_token}'}
        url = 'https://huggingface.co/api/models'
        
        yesterday, today = self.get_date_range(hours_back=24)
        date_str = yesterday.strftime('%Y-%m-%d')
        today_str = today.strftime('%Y-%m-%d')
        
        params = {
            'filter': 'pytorch',
            'sort': 'createdAt',
            'direction': -1,
            'limit': 50,
            'createdAt': f'{date_str}T00:00:00.000Z..{today_str}T23:59:59.999Z'  # ä¸¥æ ¼é™åˆ¶æ—¶é—´èŒƒå›´
        }
        
        try:
            print(f"æœç´¢Hugging Faceæ¨¡å‹")
            response = requests.get(url, headers=headers, params=params)
            print(f"HF APIå“åº”çŠ¶æ€: {response.status_code}")
            
            if response.status_code == 200:
                models = response.json()
                
                # è¿‡æ»¤æœ€è¿‘24å°æ—¶çš„æ¨¡å‹
                filtered_models = []
                for model in models:
                    if not self.is_duplicate(model) and self.is_within_time_range(model, 'huggingface'):
                        model['quality_score'] = self.calculate_quality_score(model, 'huggingface')
                        filtered_models.append(model)
                    elif not self.is_within_time_range(model, 'huggingface'):
                        print(f"è¿‡æ»¤æ‰è¿‡æœŸHFæ¨¡å‹: {model.get('modelId', '')[:50]}... (åˆ›å»ºæ—¶é—´: {model.get('createdAt', '')})")
                
                filtered_models.sort(key=lambda x: x['quality_score'], reverse=True)
                
                print(f"HF: åŸå§‹ {len(models)} ä¸ªï¼Œç­›é€‰å {len(filtered_models)} ä¸ª")
                return filtered_models[:5]
            else:
                print(f"HF APIé”™è¯¯: {response.status_code}")
        except Exception as e:
            print(f"Hugging Faceæœç´¢é”™è¯¯: {e}")
            
        return []
    
    def search_arxiv_papers(self) -> List[Dict]:
        """æœç´¢ArXivæœ€æ–°AIè®ºæ–‡ï¼ˆç¼©çŸ­æ—¶é—´çª—å£è‡³24å°æ—¶ï¼‰"""
        url = 'http://export.arxiv.org/api/query'
        
        yesterday, today = self.get_date_range(hours_back=24)
        start_date = yesterday.strftime('%Y%m%d')
        end_date = today.strftime('%Y%m%d')
        
        # ä½¿ç”¨æ›´ä¸¥æ ¼çš„æ—¶é—´èŒƒå›´ï¼Œåªæœç´¢ä»Šå¤©æäº¤çš„è®ºæ–‡
        params = {
            'search_query': f'cat:cs.AI OR cat:cs.LG OR cat:cs.CL AND submittedDate:[{start_date}0000 TO {end_date}2359]',
            'start': 0,
            'max_results': 30,
            'sortBy': 'submittedDate',
            'sortOrder': 'descending'
        }
        
        try:
            print(f"æœç´¢ArXivè®ºæ–‡: {start_date} åˆ° {end_date}")
            response = requests.get(url, params=params)
            print(f"ArXiv APIå“åº”çŠ¶æ€: {response.status_code}")
            
            if response.status_code == 200:
                import xml.etree.ElementTree as ET
                root = ET.fromstring(response.content)
                
                papers = []
                for entry in root.findall('{http://www.w3.org/2005/Atom}entry'):
                    title_elem = entry.find('{http://www.w3.org/2005/Atom}title')
                    summary_elem = entry.find('{http://www.w3.org/2005/Atom}summary')
                    link_elem = entry.find('{http://www.w3.org/2005/Atom}id')
                    
                    if title_elem is not None and summary_elem is not None and link_elem is not None:
                        paper = {
                            'title': title_elem.text.strip() if title_elem.text else 'æ— æ ‡é¢˜',
                            'authors': [author.find('{http://www.w3.org/2005/Atom}name').text 
                                      for author in entry.findall('{http://www.w3.org/2005/Atom}author')
                                      if author.find('{http://www.w3.org/2005/Atom}name') is not None],
                            'summary': summary_elem.text.strip() if summary_elem.text else 'æ— æ‘˜è¦',
                            'link': link_elem.text.strip() if link_elem.text else ''
                        }
                        
                        if not self.is_duplicate(paper) and self.is_within_time_range(paper, 'arxiv'):
                            paper['quality_score'] = self.calculate_quality_score(paper, 'arxiv')
                            papers.append(paper)
                
                papers.sort(key=lambda x: x['quality_score'], reverse=True)
                
                print(f"ArXiv: æ‰¾åˆ° {len(papers)} ç¯‡è®ºæ–‡ï¼ˆå·²å»é‡å’Œæ’åºï¼‰")
                return papers[:10]
            else:
                print(f"ArXiv APIé”™è¯¯: {response.status_code}")
        except Exception as e:
            print(f"ArXivæœç´¢é”™è¯¯: {e}")
            
        return []
    
    def generate_ai_summary(self, collected_data: Dict) -> str:
        """ä½¿ç”¨AIç”Ÿæˆæ¯æ—¥åŠ¨æ€æ‘˜è¦ï¼ˆæ–°åˆ†ç« èŠ‚ä¸“ç”¨æ•°æ®æºï¼‰"""
        focus_count = len(collected_data.get('focus_news', []))
        hf_count = len(collected_data.get('hf_models', []))
        arxiv_count = len(collected_data.get('arxiv_papers', []))
        github_count = len(collected_data.get('github_projects', []))
        applications_count = len(collected_data.get('applications', []))
        
        print(f"æ•°æ®æ”¶é›†ç»Ÿè®¡ï¼ˆæŒ‰ç« èŠ‚ï¼‰:")
        print(f"   ä»Šæ—¥ç„¦ç‚¹ï¼ˆGoogleï¼‰: {focus_count}")
        print(f"   æ¨¡å‹ä¸ç®—æ³•ï¼ˆHuggingFaceï¼‰: {hf_count}")
        print(f"   å­¦æœ¯å‰æ²¿ï¼ˆarXivï¼‰: {arxiv_count}")
        print(f"   å·¥å…·ä¸æ¡†æ¶ï¼ˆGitHubï¼‰: {github_count}")
        print(f"   åº”ç”¨ä¸äº§å“ï¼ˆå¤šæºï¼‰: {applications_count}")
        
        total_items = focus_count + hf_count + arxiv_count + github_count + applications_count
        
        if total_items == 0:
            print("WARNING: æ²¡æœ‰æ”¶é›†åˆ°ä»»ä½•æ•°æ®")
            return self.generate_fallback_summary(collected_data)
        
        if not self.ai_client:
            print("WARNING: AIå®¢æˆ·ç«¯æœªåˆå§‹åŒ–")
            return self.generate_fallback_summary(collected_data)
        
        prompt = f"""åŸºäºä»¥ä¸‹æ”¶é›†çš„AIæŠ€æœ¯åŠ¨æ€æ•°æ®ï¼Œç”Ÿæˆä¸€ä»½ä¸“ä¸šçš„æ¯æ—¥AIåŠ¨æ€æŠ¥å‘Šã€‚

**æ•°æ®æ¥æºè¯´æ˜**ï¼š
- focus_news: Google Searchï¼ˆä¸“æ³¨å¤§æ¨¡å‹å‚å•†ï¼šOpenAI, Gemini, Anthropic, xAI, Meta, Qwen, DeepSeek, GLM, Kimiï¼‰
- hf_models: HuggingFaceï¼ˆæ–°å¼€æºæ¨¡å‹ï¼‰
- arxiv_papers: arXivï¼ˆæœ€æ–°AIè®ºæ–‡ï¼‰
- github_projects: GitHubï¼ˆStarå¿«é€Ÿå¢é•¿çš„AIé¡¹ç›®ï¼‰
- applications: NewsAPI, Tavily, Google, Serper, Braveï¼ˆå¤šæºå¹¶è¡Œæœç´¢ï¼‰

æ”¶é›†åˆ°çš„æ•°æ®ï¼š
{json.dumps(collected_data, ensure_ascii=False, indent=2)}

è¯·æŒ‰ç…§ä»¥ä¸‹**åˆ†ç« èŠ‚ä½“ç³»**ç”Ÿæˆå†…å®¹ï¼š

## ğŸ“° ä»Šæ—¥ç„¦ç‚¹
**æ•°æ®æ¥æºï¼šfocus_newsï¼ˆGoogle Search - å¤§æ¨¡å‹å‚å•†ï¼‰**

ä» focus_news ä¸­å±•ç¤ºæ‰€æœ‰æ”¶é›†åˆ°çš„æ–°é—»ï¼ˆé€šå¸¸5-8æ¡ï¼‰ï¼ŒæŒ‰è´¨é‡è¯„åˆ†æ’åºï¼Œæ¯æ¡åŒ…å«ï¼š
- ğŸ”¥ çƒ­åº¦æ ‡è¯†ï¼ˆæ ¹æ®é‡è¦æ€§ï¼šğŸ”¥ğŸ”¥ğŸ”¥ é«˜ / ğŸ”¥ğŸ”¥ ä¸­ / ğŸ”¥ æ™®é€šï¼‰
- æ ‡é¢˜å’Œä¸€å¥è¯æ€»ç»“
- ä¸ºä»€ä¹ˆé‡è¦
- é“¾æ¥

**é‡è¦**ï¼šè¯·å±•ç¤º focus_news ä¸­çš„æ‰€æœ‰æˆ–å¤§éƒ¨åˆ†æ¡ç›®ï¼Œä¸è¦åªé€‰æ‹©2-3æ¡ã€‚

## ğŸ§  æ¨¡å‹ä¸ç®—æ³•
**æ•°æ®æ¥æºï¼šhf_modelsï¼ˆHuggingFaceï¼‰**

ä» hf_models ä¸­å±•ç¤ºæ‰€æœ‰æ”¶é›†åˆ°çš„æ¨¡å‹ï¼ˆé€šå¸¸3-5ä¸ªï¼‰ï¼ŒæŒ‰è´¨é‡è¯„åˆ†æ’åºï¼Œæ¯é¡¹åŒ…å«ï¼š
- æ¨¡å‹åç§°å’Œé“¾æ¥
- æ ¸å¿ƒç‰¹æ€§
- ä¸‹è½½é‡/çƒ­åº¦
- é€‚ç”¨åœºæ™¯

**é‡è¦**ï¼šè¯·å±•ç¤º hf_models ä¸­çš„æ‰€æœ‰æ¡ç›®ï¼Œé™¤éè´¨é‡è¯„åˆ†è¿‡ä½ã€‚

## ğŸ› ï¸ å·¥å…·ä¸æ¡†æ¶
**æ•°æ®æ¥æºï¼šgithub_projectsï¼ˆGitHub Starå¿«é€Ÿå¢é•¿ï¼‰**

ä» github_projects ä¸­å±•ç¤ºå‰5-8ä¸ªæœ€é‡è¦çš„é¡¹ç›®ï¼ŒæŒ‰ star å¢é•¿ç‡å’Œè´¨é‡è¯„åˆ†æ’åºï¼Œæ¯é¡¹åŒ…å«ï¼š
- å·¥å…·åç§°å’Œé“¾æ¥
- ä¸»è¦åŠŸèƒ½
- Stars æ•°é‡å’Œå¢é•¿ç‡ï¼ˆstars/dayï¼‰
- æ¨èæŒ‡æ•°ï¼ˆâ­â­â­â­â­ï¼‰

**é‡è¦**ï¼šè¯·å±•ç¤º github_projects ä¸­çš„å‰5-8ä¸ªé«˜è´¨é‡é¡¹ç›®ï¼Œä¸è¦è¿‡åº¦ç­›é€‰ã€‚

## ğŸ“± åº”ç”¨ä¸äº§å“
**æ•°æ®æ¥æºï¼šapplicationsï¼ˆå¤šæºå¹¶è¡Œæœç´¢ï¼‰**

ä» applications ä¸­å±•ç¤ºå‰5-8ä¸ªæœ€é‡è¦çš„åº”ç”¨ï¼ŒæŒ‰è´¨é‡è¯„åˆ†æ’åºï¼Œæ¯é¡¹åŒ…å«ï¼š
- åº”ç”¨åç§°å’Œé“¾æ¥
- åŠŸèƒ½æè¿°
- å®ç”¨æ€§è¯„ä¼°

**é‡è¦**ï¼šè¯·å±•ç¤º applications ä¸­çš„å‰5-8ä¸ªé«˜è´¨é‡åº”ç”¨ï¼Œä¸è¦åªé€‰æ‹©2-3ä¸ªã€‚

## ğŸ“š å­¦æœ¯å‰æ²¿
**æ•°æ®æ¥æºï¼šarxiv_papersï¼ˆarXivï¼‰**

ä» arxiv_papers ä¸­å±•ç¤ºå‰5-8ç¯‡æœ€é‡è¦çš„è®ºæ–‡ï¼ŒæŒ‰è´¨é‡è¯„åˆ†æ’åºï¼Œæ¯é¡¹åŒ…å«ï¼š
- è®ºæ–‡æ ‡é¢˜å’Œé“¾æ¥
- ä½œè€…
- æ ¸å¿ƒè´¡çŒ®
- åˆ›æ–°ç‚¹

**é‡è¦**ï¼šè¯·å±•ç¤º arxiv_papers ä¸­çš„å‰5-8ç¯‡é«˜è´¨é‡è®ºæ–‡ï¼Œä¸è¦è¿‡åº¦ç­›é€‰ã€‚

## ğŸ’¡ ç¼–è¾‘ç‚¹è¯„
æ ¹æ®ä»Šæ—¥å†…å®¹ï¼Œæä¾›ï¼š
- æŠ€æœ¯è¶‹åŠ¿è§‚å¯Ÿï¼ˆ2-3æ¡ï¼‰
- å€¼å¾—å…³æ³¨çš„æ–¹å‘
- è¡Œä¸šå½±å“åˆ†æ

**è¦æ±‚**ï¼š
1. ä¸¥æ ¼æŒ‰ç…§å„ç« èŠ‚çš„ä¸“ç”¨æ•°æ®æºç”Ÿæˆå†…å®¹
2. focus_news ç”¨äºä»Šæ—¥ç„¦ç‚¹ï¼Œhf_models ç”¨äºæ¨¡å‹ä¸ç®—æ³•ï¼Œarxiv_papers ç”¨äºå­¦æœ¯å‰æ²¿ï¼Œgithub_projects ç”¨äºå·¥å…·ä¸æ¡†æ¶ï¼Œapplications ç”¨äºåº”ç”¨ä¸äº§å“
3. **æ•°é‡è¦æ±‚**ï¼šå¿…é¡»å±•ç¤ºå¤§éƒ¨åˆ†æˆ–å…¨éƒ¨æ”¶é›†åˆ°çš„æ•°æ®ï¼Œä¸è¦è¿‡åº¦ç­›é€‰ï¼š
   - ä»Šæ—¥ç„¦ç‚¹ï¼šå±•ç¤ºæ‰€æœ‰æ”¶é›†åˆ°çš„æ–°é—»ï¼ˆé€šå¸¸5-8æ¡ï¼‰
   - æ¨¡å‹ä¸ç®—æ³•ï¼šå±•ç¤ºæ‰€æœ‰æ”¶é›†åˆ°çš„æ¨¡å‹ï¼ˆé€šå¸¸3-5ä¸ªï¼‰
   - å·¥å…·ä¸æ¡†æ¶ï¼šå±•ç¤ºå‰5-8ä¸ªé«˜è´¨é‡é¡¹ç›®
   - åº”ç”¨ä¸äº§å“ï¼šå±•ç¤ºå‰5-8ä¸ªé«˜è´¨é‡åº”ç”¨
   - å­¦æœ¯å‰æ²¿ï¼šå±•ç¤ºå‰5-8ç¯‡é«˜è´¨é‡è®ºæ–‡
4. å†…å®¹è¦å‡†ç¡®ã€ä¸“ä¸šã€æœ‰ä»·å€¼ï¼Œçªå‡ºé‡ç‚¹
5. ä½¿ç”¨ Emoji å¢åŠ å¯è¯»æ€§
6. å¦‚æœæŸä¸ªåˆ†ç±»æ•°æ®ä¸è¶³ï¼Œç®€è¦è¯´æ˜"ä»Šæ—¥è¯¥ç±»åˆ«æš‚æ— é‡è¦æ›´æ–°"
7. ä½¿ç”¨ä¸­æ–‡ï¼Œè¯­è¨€ç®€æ´ä¸“ä¸š
8. ä¸ºé‡è¦é¡¹ç›®æ·»åŠ è´¨é‡è¯„ä»·ï¼ˆstarsã€ä¸‹è½½é‡ã€ä½œè€…ç­‰ï¼‰
"""
        
        try:
            print("å¼€å§‹AIç”Ÿæˆæ‘˜è¦ï¼ˆæ–°æ ¼å¼ï¼‰...")
            
            if self.use_google_sdk:
                response = self.ai_client.generate_content(prompt)
                content = response.text if hasattr(response, 'text') else None
            else:
                response = self.ai_client.chat.completions.create(
                    model="gemini-2.5-flash",
                    messages=[{"role": "user", "content": prompt}],
                    max_tokens=3000,
                    temperature=0.7
                )
                content = response.choices[0].message.content if response.choices else None
            
            if not content or content.strip() == "":
                print("WARNING: AIè¿”å›äº†ç©ºå†…å®¹")
                return self.generate_fallback_summary(collected_data)
            
            print(f"AIæ‘˜è¦ç”Ÿæˆå®Œæˆï¼ˆé•¿åº¦: {len(content)}ï¼‰")
            return content
            
        except Exception as e:
            print(f"AIç”Ÿæˆæ‘˜è¦é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
            return self.generate_fallback_summary(collected_data)
    
    def generate_fallback_summary(self, collected_data: Dict) -> str:
        """ç”Ÿæˆå¤‡ç”¨æ‘˜è¦ï¼ˆæ–°åˆ†ç« èŠ‚ä¸“ç”¨æ•°æ®æºï¼‰"""
        print("ä½¿ç”¨ fallback æ‘˜è¦ç”Ÿæˆå™¨ï¼ˆæ–°ç­–ç•¥ï¼šæŒ‰ç« èŠ‚åˆ†é…æ•°æ®æºï¼‰...")
        summary = ""
        
        # ä»Šæ—¥ç„¦ç‚¹ - ä½¿ç”¨ focus_news (Google Search - å¤§æ¨¡å‹å‚å•†)
        summary += "## ğŸ“° ä»Šæ—¥ç„¦ç‚¹\n\n"
        focus_news = collected_data.get('focus_news', [])
        
        if focus_news:
            print(f"DEBUG: ä» focus_news ä¸­ç”Ÿæˆä»Šæ—¥ç„¦ç‚¹ ({len(focus_news)} æ¡)")
            # æŒ‰è´¨é‡è¯„åˆ†æ’åº
            sorted_news = sorted(focus_news, key=lambda x: x.get('quality_score', 0), reverse=True)
            
            for item in sorted_news[:3]:
                title = item.get('title', 'æœªçŸ¥æ ‡é¢˜')
                url = item.get('url', '')
                snippet = item.get('snippet', '')[:150] 
                score = item.get('quality_score', 5.0)
                
                if title and url:
                    heat_icon = "ğŸ”¥ğŸ”¥ğŸ”¥" if score >= 8.0 else "ğŸ”¥ğŸ”¥" if score >= 7.0 else "ğŸ”¥"
                    summary += f"### {heat_icon} [{title}]({url})\n"
                    summary += f"- **ç®€ä»‹**: {snippet}...\n"
                    summary += f"- **æ¥æº**: Google Searchï¼ˆå¤§æ¨¡å‹å‚å•†ï¼‰\n"
                    summary += f"- **è´¨é‡è¯„åˆ†**: {score:.1f}/10\n\n"
        else:
            summary += "âš ï¸ ä»Šæ—¥æš‚æ— å¤§æ¨¡å‹å‚å•†é‡è¦æ–°é—»å‘å¸ƒã€‚\n\n"
        
        # æ¨¡å‹ä¸ç®—æ³• - ä½¿ç”¨ hf_models (HuggingFace)
        summary += "## ğŸ§  æ¨¡å‹ä¸ç®—æ³•\n\n"
        hf_models = collected_data.get('hf_models', [])
        if hf_models:
            for model in hf_models[:5]:
                model_id = model.get('modelId', 'æœªçŸ¥æ¨¡å‹')
                pipeline = model.get('pipeline_tag', 'æœªçŸ¥ç±»å‹')
                downloads = model.get('downloads', 0)
                score = model.get('quality_score', 5.0)
                stars = 'â­' * int(score / 2)
                summary += f"### {model_id}\n"
                summary += f"- **ç±»å‹**: {pipeline}\n"
                summary += f"- **ä¸‹è½½é‡**: {downloads:,}\n"
                summary += f"- **è¯„åˆ†**: {stars} ({score:.1f}/10)\n"
                summary += f"- **é“¾æ¥**: https://huggingface.co/{model_id}\n\n"
        else:
            summary += "âš ï¸ ä»Šæ—¥æš‚æ— æ–°æ¨¡å‹å‘å¸ƒã€‚\n\n"
        
        # å·¥å…·ä¸æ¡†æ¶ - ä½¿ç”¨ github_projects (GitHub Starå¿«é€Ÿå¢é•¿)
        summary += "## ğŸ› ï¸ å·¥å…·ä¸æ¡†æ¶\n\n"
        github_projects = collected_data.get('github_projects', [])
        if github_projects:
            for project in github_projects[:5]:
                name = project.get('name', 'æœªçŸ¥é¡¹ç›®')
                desc = project.get('description', 'æ— æè¿°')
                url = project.get('html_url', '')
                stars = project.get('stargazers_count', 0)
                stars_per_day = project.get('stars_per_day', 0)
                score = project.get('quality_score', 5.0)
                rating = 'â­' * min(5, int(score / 2))
                summary += f"### [{name}]({url})\n"
                summary += f"- **åŠŸèƒ½**: {desc}\n"
                summary += f"- **Stars**: {stars:,} ({stars_per_day:.1f} stars/day)\n"
                summary += f"- **æ¨èæŒ‡æ•°**: {rating}\n\n"
        else:
            summary += "âš ï¸ ä»Šæ—¥æš‚æ— æ–°å·¥å…·æ¡†æ¶å‘å¸ƒã€‚\n\n"
        
        # åº”ç”¨ä¸äº§å“ - ä½¿ç”¨ applications (å¤šæºå¹¶è¡Œæœç´¢)
        summary += "## ğŸ“± åº”ç”¨ä¸äº§å“\n\n"
        applications = collected_data.get('applications', [])
        if applications:
            for app in applications[:5]:
                title = app.get('title', 'æœªçŸ¥åº”ç”¨')
                url = app.get('url', '')
                snippet = app.get('snippet', 'æ— æè¿°')[:150]
                source = app.get('source', 'æœªçŸ¥æ¥æº')
                score = app.get('quality_score', 5.0)
                summary += f"### [{title}]({url})\n"
                summary += f"- **ç®€ä»‹**: {snippet}...\n"
                summary += f"- **æ¥æº**: {source}\n"
                summary += f"- **è´¨é‡è¯„åˆ†**: {score:.1f}/10\n\n"
        else:
            summary += "âš ï¸ ä»Šæ—¥æš‚æ— æ–°åº”ç”¨ä¸äº§å“å‘å¸ƒã€‚\n\n"
        
        # å­¦æœ¯å‰æ²¿ - ä½¿ç”¨ arxiv_papers (arXiv)
        summary += "## ğŸ“š å­¦æœ¯å‰æ²¿\n\n"
        arxiv_papers = collected_data.get('arxiv_papers', [])
        if arxiv_papers:
            for paper in arxiv_papers[:5]:
                title = paper.get('title', 'æœªçŸ¥æ ‡é¢˜')
                authors = ', '.join(paper.get('authors', [])[:3])
                if len(paper.get('authors', [])) > 3:
                    authors += ' ç­‰'
                link = paper.get('link', '')
                abstract = paper.get('summary', 'æ— æ‘˜è¦')[:200]
                score = paper.get('quality_score', 5.0)
                summary += f"### [{title}]({link})\n"
                summary += f"- **ä½œè€…**: {authors}\n"
                summary += f"- **æ‘˜è¦**: {abstract}...\n"
                summary += f"- **è´¨é‡è¯„åˆ†**: {score:.1f}/10\n\n"
        else:
            summary += "âš ï¸ ä»Šæ—¥æš‚æ— é‡è¦è®ºæ–‡å‘å¸ƒã€‚\n\n"
        
        # ç¼–è¾‘ç‚¹è¯„
        summary += "## ğŸ’¡ ç¼–è¾‘ç‚¹è¯„\n\n"
        total_items = len(focus_news) + len(hf_models) + len(arxiv_papers) + len(github_projects) + len(applications)
        if total_items > 0:
            summary += f"ä»Šæ—¥å…±æ”¶é›†åˆ° {total_items} æ¡AIåŠ¨æ€ï¼ˆæŒ‰ç« èŠ‚åˆ†é…æ•°æ®æºï¼‰ï¼Œå…¶ä¸­ï¼š\n"
            if focus_news:
                summary += f"- ğŸ“° ä»Šæ—¥ç„¦ç‚¹ï¼ˆGoogleï¼‰: {len(focus_news)} æ¡\n"
            if hf_models:
                summary += f"- ğŸ§  æ¨¡å‹ä¸ç®—æ³•ï¼ˆHuggingFaceï¼‰: {len(hf_models)} ä¸ª\n"
            if arxiv_papers:
                summary += f"- ğŸ“š å­¦æœ¯å‰æ²¿ï¼ˆarXivï¼‰: {len(arxiv_papers)} ç¯‡\n"
            if github_projects:
                summary += f"- ğŸ› ï¸ å·¥å…·ä¸æ¡†æ¶ï¼ˆGitHubï¼‰: {len(github_projects)} ä¸ª\n"
            if applications:
                summary += f"- ğŸ“± åº”ç”¨ä¸äº§å“ï¼ˆå¤šæºï¼‰: {len(applications)} æ¡\n"
            summary += "\nå†…å®¹è´¨é‡å‡ç»è¿‡è‡ªåŠ¨è¯„åˆ†å’Œæ’åºï¼Œä¼˜å…ˆå±•ç¤ºé«˜è´¨é‡é¡¹ç›®ã€‚\n"
            summary += "\n**æ–°ç­–ç•¥**ï¼šé‡‡ç”¨åˆ†ç« èŠ‚ä¸“ç”¨æ•°æ®æºï¼Œç¡®ä¿å†…å®¹æ›´ç²¾å‡†ã€æ›´èšç„¦ã€‚\n"
        else:
            summary += "ä»Šæ—¥æ•°æ®æ”¶é›†é‡åˆ°é—®é¢˜ï¼Œå»ºè®®ç¨åé‡è¯•æˆ–æ‰‹åŠ¨æ£€æŸ¥æ•°æ®æºã€‚\n\n"
            summary += "å¯èƒ½åŸå› ï¼šAPIé™æµã€ç½‘ç»œé—®é¢˜ã€æˆ–ç¡®å®æ— æ–°å†…å®¹å‘å¸ƒã€‚\n"
        
        return summary
    
    def create_daily_content(self) -> str:
        """åˆ›å»ºæ¯æ—¥å†…å®¹æ–‡ä»¶"""
        yesterday, today = self.get_date_range(hours_back=24)
        date_str = today.strftime('%Y-%m-%d')
        time_range = f"{yesterday.strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M')} - {today.strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M')}"
        
        print(f"æ—¶é—´èŒƒå›´: {time_range}")
        
        # åŠ è½½å†å²æ•°æ®ç”¨äºå»é‡
        self.seen_urls = self.load_history_items(days_back=7)
        self.seen_titles = set()  # é‡ç½®æ ‡é¢˜é›†åˆ
        
        # æ”¶é›†æ•°æ®ï¼ˆæŒ‰æ–°ç« èŠ‚åˆ†é…æ•°æ®æºï¼‰
        print("=" * 60)
        print("å¼€å§‹æ”¶é›†æ•°æ®ï¼ˆæ–°ç­–ç•¥ï¼šæŒ‰ç« èŠ‚åˆ†é…ä¸“ç”¨æ•°æ®æºï¼‰...")
        print("=" * 60)
        
        collected_data = {
            # ä»Šæ—¥ç„¦ç‚¹ - ä»…ä½¿ç”¨ Google Search æœç´¢å¤§æ¨¡å‹å‚å•†
            'focus_news': self.search_focus_news(),
            
            # æ¨¡å‹ä¸ç®—æ³• - ä»…ä½¿ç”¨ HuggingFace
            'hf_models': self.search_huggingface_models(),
            
            # å­¦æœ¯å‰æ²¿ - ä»…ä½¿ç”¨ arXiv
            'arxiv_papers': self.search_arxiv_papers(),
            
            # å·¥å…·ä¸æ¡†æ¶ - GitHub Star å¿«é€Ÿå¢é•¿
            'github_projects': self.search_github_trending(),
            
            # åº”ç”¨ä¸äº§å“ - NewsAPI, Tavily, Google, Serper, Brave å¹¶è¡Œæœç´¢
            'applications': self.search_applications(),
        }
        
        print("=" * 60)
        print("æ•°æ®æ”¶é›†å®Œæˆï¼Œå¼€å§‹ç”Ÿæˆæ‘˜è¦...")
        print("=" * 60)
        
        # ç”ŸæˆAIæ‘˜è¦
        ai_summary = self.generate_ai_summary(collected_data)
        
        # ç»Ÿè®¡ä¿¡æ¯
        total_items = sum(len(v) for v in collected_data.values())
        
        # æ„å»ºé¡µè„šï¼ˆç”¨äºç»Ÿè®¡å­—æ•°ï¼‰
        footer = """
## ğŸ“Š æ•°æ®æ¥æº

æœ¬æŠ¥å‘Šé‡‡ç”¨**åˆ†ç« èŠ‚ä¸“ç”¨æ•°æ®æº**ç­–ç•¥ï¼š

- ğŸ“° **ä»Šæ—¥ç„¦ç‚¹**: Google Searchï¼ˆä¸“æ³¨å¤§æ¨¡å‹å‚å•†ï¼šOpenAI, Gemini, Anthropic, xAI, Meta, Qwen, DeepSeek, GLM, Kimiç­‰ï¼‰
- ğŸ§  **æ¨¡å‹ä¸ç®—æ³•**: HuggingFaceï¼ˆæ–°å¼€æºæ¨¡å‹ï¼‰
- ğŸ“š **å­¦æœ¯å‰æ²¿**: arXivï¼ˆæœ€æ–°AIè®ºæ–‡ï¼‰
- ğŸ› ï¸ **å·¥å…·ä¸æ¡†æ¶**: GitHubï¼ˆStarå¿«é€Ÿå¢é•¿çš„AIé¡¹ç›®ï¼‰
- ğŸ“± **åº”ç”¨ä¸äº§å“**: NewsAPI, Tavily, Google, Serper, Braveï¼ˆå¤šæºå¹¶è¡Œæœç´¢ï¼‰

æ‰€æœ‰å†…å®¹ç»è¿‡**è´¨é‡è¯„åˆ†**ã€**å»é‡**å’Œ**æ™ºèƒ½æ’åº**ï¼Œç¡®ä¿ä¿¡æ¯çš„ä»·å€¼å’Œæ—¶æ•ˆæ€§ã€‚

---

> ğŸ’¡ **æç¤º**: æœ¬å†…å®¹ç”± AI è‡ªåŠ¨ç”Ÿæˆï¼Œæ¯æ—¥åŒ—äº¬æ—¶é—´ 08:00 æ›´æ–°ã€‚  
> å¦‚æœ‰é—æ¼æˆ–é”™è¯¯ï¼Œæ¬¢è¿é€šè¿‡ [Issues](https://github.com/hobbytp/hobbytp.github.io/issues) åé¦ˆã€‚
"""

        # é¢„è®¡ç®—æ­£æ–‡éƒ¨åˆ†çš„å­—æ•°å’Œé˜…è¯»æ—¶é—´ï¼ˆç”¨äºHeaderä¸­çš„æ˜¾ç¤ºï¼‰
        # æ­£æ–‡ = AIæ‘˜è¦ + é¡µè„š
        body_for_calc = ai_summary + "\n" + footer
        _, estimated_reading_time = calculate_reading_stats(body_for_calc, reading_speed=400)
        
        # æ„å»ºHeader
        header = f"""# æ¯æ—¥AIåŠ¨æ€ - {date_str}

> ğŸ“… **æ—¶é—´èŒƒå›´**: {time_range} (åŒ—äº¬æ—¶é—´)  
> ğŸ“Š **å†…å®¹ç»Ÿè®¡**: å…± {total_items} æ¡åŠ¨æ€  
> â±ï¸ **é¢„è®¡é˜…è¯»**: {estimated_reading_time} åˆ†é’Ÿ"""

        # ç»„è£…å®Œæ•´æ­£æ–‡
        full_body = f"{header}\n\n---\n\n{ai_summary}\n\n---\n{footer}"
        
        # æœ€ç»ˆç»Ÿè®¡å­—æ•°å’Œé˜…è¯»æ—¶é—´ï¼ˆåŒ…æ‹¬Headerï¼‰
        word_count, reading_time = calculate_reading_stats(full_body, reading_speed=400)

        # åˆ›å»ºMarkdownå†…å®¹
        content = f"""---
title: "æ¯æ—¥AIåŠ¨æ€ - {date_str}"
date: {today.strftime('%Y-%m-%dT%H:%M:%S+08:00')}
draft: false
categories: ["news"]
tags: ["AIåŠ¨æ€", "æŠ€æœ¯æ›´æ–°", "è¡Œä¸šè¶‹åŠ¿"]
description: "{date_str}çš„AIæŠ€æœ¯åŠ¨æ€æ±‡æ€»"
readingTime: {reading_time}
wordCount: {word_count}
totalItems: {total_items}
---

{full_body}
"""
        
        return content
    
    def save_daily_content(self):
        """ä¿å­˜æ¯æ—¥å†…å®¹"""
        content = self.create_daily_content()
        date_str = datetime.datetime.now().strftime('%Y-%m-%d')
        file_path = self.content_dir / f"{date_str}.md"
        
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(content)
        
        print("=" * 60)
        print(f"[OK] æ¯æ—¥AIåŠ¨æ€å·²ä¿å­˜åˆ°: {file_path}")
        print("=" * 60)
        return file_path

def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("æ¯æ—¥AIåŠ¨æ€æ”¶é›†å™¨ V2.0 - ä¸“ä¸šç‰ˆ")
    print("=" * 60)
    
    collector = DailyAICollectorV2()
    file_path = collector.save_daily_content()
    
    print(f"\n[OK] æ”¶é›†å®Œæˆ: {file_path}")
    print("\næ–°åŠŸèƒ½ï¼š")
    print("  [OK] Perplexity AI æ–°é—»æœç´¢")
    print("  [OK] 24å°æ—¶æ—¶é—´çª—å£ï¼ˆçœŸæ­£çš„æ¯æ—¥åŠ¨æ€ï¼‰")
    print("  [OK] æ™ºèƒ½å»é‡")
    print("  [OK] å†…å®¹è´¨é‡è¯„åˆ†")
    print("  [OK] æ–°çš„åˆ†ç±»ä½“ç³»")
    print("  [OK] æ”¹è¿›çš„å±•ç°æ ¼å¼")

if __name__ == "__main__":
    main()
