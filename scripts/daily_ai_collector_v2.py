#!/usr/bin/env python3
"""
æ¯æ—¥AIåŠ¨æ€æ”¶é›†è„šæœ¬ V2.0 - ä¸“ä¸šç‰ˆ
è‡ªåŠ¨æ”¶é›†AIé¢†åŸŸçš„æœ€æ–°åŠ¨æ€ï¼ŒåŒ…æ‹¬æ–°æ¨¡å‹ã€æ–°æ¡†æ¶ã€æ–°åº”ç”¨ç­‰
æ–°å¢åŠŸèƒ½ï¼š
- Perplexity API é›†æˆï¼ˆè·å– AI æ–°é—»å’Œè¶‹åŠ¿ï¼‰
- ç¼©çŸ­æ—¶é—´çª—å£è‡³ 24 å°æ—¶ï¼ˆçœŸæ­£çš„æ¯æ—¥åŠ¨æ€ï¼‰
- æ•°æ®å»é‡åŠŸèƒ½
- å†…å®¹è´¨é‡è¯„åˆ†å’Œæ’åº
- ç®€åŒ–çš„åˆ†ç±»ä½“ç³»
"""

import os
import json
import requests
import datetime
from typing import List, Dict, Any, Set
from pathlib import Path
import yaml
import hashlib
import re

# å°è¯•å¯¼å…¥ Google Gemini SDK
try:
    import google.generativeai as genai
    USE_GOOGLE_SDK = True
    print("âœ… google.generativeai åº“å¯¼å…¥æˆåŠŸ")
except ImportError as e:
    USE_GOOGLE_SDK = False
    print(f"âš ï¸ google.generativeai åº“å¯¼å…¥å¤±è´¥: {e}")
    try:
        import openai
        print("âœ… openai åº“å¯¼å…¥æˆåŠŸï¼ˆå›é€€æ¨¡å¼ï¼‰")
    except ImportError:
        print("ERROR: æ—¢æ²¡æœ‰ google-generativeai ä¹Ÿæ²¡æœ‰ openai åº“")
        openai = None

# å°è¯•å¯¼å…¥ Perplexity SDK
try:
    from perplexity import Perplexity
    USE_PERPLEXITY = True
    print("âœ… perplexity åº“å¯¼å…¥æˆåŠŸ")
except ImportError as e:
    USE_PERPLEXITY = False
    print(f"âš ï¸ perplexity åº“å¯¼å…¥å¤±è´¥: {e}")
    print("   å®‰è£…: pip install perplexityai")

class DailyAICollectorV2:
    def __init__(self):
        # åˆå§‹åŒ– Gemini API
        gemini_key = os.getenv('GEMINI_API_KEY')
        if not gemini_key:
            print("WARNING: GEMINI_API_KEY æœªè®¾ç½®ï¼")
            self.ai_client = None
            self.use_google_sdk = False
        else:
            print(f"GEMINI_API_KEY å·²è®¾ç½® (é•¿åº¦: {len(gemini_key)})")
            
            if USE_GOOGLE_SDK:
                try:
                    genai.configure(api_key=gemini_key)
                    self.ai_client = genai.GenerativeModel('gemini-2.5-flash')
                    self.use_google_sdk = True
                    print("âœ… Google Gemini SDK åˆå§‹åŒ–æˆåŠŸ (æ¨¡å‹: gemini-2.5-flash)")
                except Exception as e:
                    print(f"ERROR: Google SDK åˆå§‹åŒ–å¤±è´¥: {e}")
                    self.ai_client = None
                    self.use_google_sdk = False
            elif openai:
                try:
                    self.ai_client = openai.OpenAI(
                        api_key=gemini_key,
                        base_url="https://generativelanguage.googleapis.com/v1beta/openai/"
                    )
                    self.use_google_sdk = False
                    print("âœ… OpenAIå…¼å®¹å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
                except Exception as e:
                    print(f"ERROR: OpenAIå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
                    self.ai_client = None
                    self.use_google_sdk = False
            else:
                print("ERROR: æ²¡æœ‰å¯ç”¨çš„ AI å®¢æˆ·ç«¯åº“")
                self.ai_client = None
                self.use_google_sdk = False

        # åˆå§‹åŒ– Perplexity API
        self.perplexity_key = os.getenv('PERPLEXITY_API_KEY')
        if USE_PERPLEXITY and self.perplexity_key:
            try:
                self.perplexity_client = Perplexity(api_key=self.perplexity_key)
                print(f"âœ… Perplexity API åˆå§‹åŒ–æˆåŠŸ (keyé•¿åº¦: {len(self.perplexity_key)})")
            except Exception as e:
                print(f"ERROR: Perplexity åˆå§‹åŒ–å¤±è´¥: {e}")
                self.perplexity_client = None
        else:
            self.perplexity_client = None
            if not USE_PERPLEXITY:
                print("âš ï¸ Perplexity SDK æœªå®‰è£…")
            elif not self.perplexity_key:
                print("âš ï¸ PERPLEXITY_API_KEY æœªè®¾ç½®")

        self.github_token = os.getenv('GITHUB_TOKEN')
        self.hf_token = os.getenv('HUGGINGFACE_API_KEY')
        
        print(f"GitHub Token: {'å·²è®¾ç½®' if self.github_token else 'æœªè®¾ç½®'}")
        print(f"HuggingFace Token: {'å·²è®¾ç½®' if self.hf_token else 'æœªè®¾ç½®'}")
        
        self.content_dir = Path("content/zh/daily_ai")
        self.content_dir.mkdir(exist_ok=True)
        
        # ç”¨äºå»é‡çš„é›†åˆ
        self.seen_urls: Set[str] = set()
        self.seen_titles: Set[str] = set()
        
    def get_date_range(self, hours_back: int = 24) -> tuple:
        """è·å–æ—¶é—´èŒƒå›´ï¼ˆé»˜è®¤è¿‡å»24å°æ—¶ï¼‰"""
        now = datetime.datetime.now()
        today_8am = now.replace(hour=8, minute=0, second=0, microsecond=0)
        start_time = today_8am - datetime.timedelta(hours=hours_back)
        return start_time, today_8am
    
    def load_history_items(self, days_back: int = 7) -> Set[str]:
        """åŠ è½½æœ€è¿‘Nå¤©çš„å†å²é¡¹ç›®ï¼Œç”¨äºå»é‡"""
        history_urls = set()
        history_titles = set()
        
        now = datetime.datetime.now()
        for i in range(days_back):
            date = now - datetime.timedelta(days=i)
            file_path = self.content_dir / f"{date.strftime('%Y-%m-%d')}.md"
            
            if file_path.exists():
                try:
                    content = file_path.read_text(encoding='utf-8')
                    # æå– URL
                    urls = re.findall(r'https?://[^\s\)]+', content)
                    history_urls.update(urls)
                    # æå–æ ‡é¢˜ï¼ˆMarkdown headingï¼‰
                    titles = re.findall(r'###\s+(.+)', content)
                    history_titles.update(titles)
                except Exception as e:
                    print(f"è¯»å–å†å²æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
        
        print(f"åŠ è½½äº† {len(history_urls)} ä¸ªå†å²URLï¼Œ{len(history_titles)} ä¸ªå†å²æ ‡é¢˜")
        return history_urls | history_titles
    
    def is_duplicate(self, item: Dict) -> bool:
        """æ£€æŸ¥é¡¹ç›®æ˜¯å¦é‡å¤"""
        # æ£€æŸ¥ URL
        url = item.get('html_url') or item.get('link') or item.get('url', '')
        if url and url in self.seen_urls:
            return True
        if url:
            self.seen_urls.add(url)
        
        # æ£€æŸ¥æ ‡é¢˜
        title = item.get('name') or item.get('title') or item.get('modelId', '')
        if title and title in self.seen_titles:
            return True
        if title:
            self.seen_titles.add(title)
        
        return False
    
    def calculate_quality_score(self, item: Dict, source: str) -> float:
        """è®¡ç®—å†…å®¹è´¨é‡åˆ†æ•°ï¼ˆ0-10åˆ†ï¼‰"""
        score = 5.0  # åŸºç¡€åˆ†
        
        if source == 'github':
            stars = item.get('stargazers_count', 0)
            score += min(stars / 200, 3.0)  # stars æƒé‡ï¼ˆæœ€å¤šåŠ 3åˆ†ï¼‰
            
            # æ˜¯å¦æœ‰è¯¦ç»†æè¿°
            desc_len = len(item.get('description', ''))
            if desc_len > 100:
                score += 1.0
            elif desc_len > 50:
                score += 0.5
            
            # æœ€è¿‘æ›´æ–°æ—¶é—´
            updated_at = item.get('updated_at', '')
            if updated_at:
                try:
                    update_time = datetime.datetime.strptime(updated_at, '%Y-%m-%dT%H:%M:%SZ')
                    days_ago = (datetime.datetime.now() - update_time).days
                    if days_ago <= 1:
                        score += 1.0
                    elif days_ago <= 7:
                        score += 0.5
                except:
                    pass
        
        elif source == 'huggingface':
            downloads = item.get('downloads', 0)
            score += min(downloads / 1000, 2.0)  # downloads æƒé‡
            
            # æ˜¯å¦æœ‰ pipeline_tag
            if item.get('pipeline_tag'):
                score += 0.5
        
        elif source == 'arxiv':
            # ä½œè€…æ•°é‡
            authors = len(item.get('authors', []))
            score += min(authors / 5, 1.0)
            
            # æ‘˜è¦é•¿åº¦
            summary_len = len(item.get('summary', ''))
            if 200 < summary_len < 2000:
                score += 1.0
            elif 100 < summary_len < 3000:
                score += 0.5
        
        elif source == 'perplexity':
            # Perplexity ç»“æœé€šå¸¸è´¨é‡è¾ƒé«˜
            score += 2.0
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å‘å¸ƒæ—¥æœŸ
            if item.get('date'):
                score += 0.5
        
        return min(score, 10.0)  # æœ€é«˜10åˆ†
    
    def search_perplexity_ai_news(self) -> List[Dict]:
        """ä½¿ç”¨ Perplexity æœç´¢ AI æ–°é—»å’Œè¶‹åŠ¿"""
        if not self.perplexity_client:
            print("WARNING: Perplexity å®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œè·³è¿‡æœç´¢")
            return []
        
        try:
            yesterday, today = self.get_date_range(hours_back=24)
            date_str = yesterday.strftime('%Y-%m-%d')
            
            # å¤šæŸ¥è¯¢æœç´¢ï¼šAIæ–°é—»ã€æ¨¡å‹å‘å¸ƒã€å·¥å…·å‘å¸ƒ
            queries = [
                f"latest AI news and breakthroughs since {date_str}",
                f"new AI models released since {date_str}",
                f"new AI tools and frameworks since {date_str}"
            ]
            
            print(f"ä½¿ç”¨ Perplexity æœç´¢: {queries}")
            
            search = self.perplexity_client.search.create(
                query=queries,
                max_results=5,
                max_tokens_per_page=1024
            )
            
            all_results = []
            query_types = ['news', 'models', 'tools']
            
            if hasattr(search, 'results'):
                # å¤„ç†å¤šæŸ¥è¯¢ç»“æœ
                for query_idx, query_results in enumerate(search.results):
                    # å®‰å…¨è·å– query_typeï¼Œè¶…å‡ºç´¢å¼•åˆ™ä½¿ç”¨ 'general'
                    query_type = query_types[query_idx] if query_idx < len(query_types) else 'general'
                    
                    if isinstance(query_results, list):
                        # query_results æ˜¯åˆ—è¡¨
                        for result in query_results:
                            item = {
                                'title': result.title if hasattr(result, 'title') else '',
                                'url': result.url if hasattr(result, 'url') else '',
                                'snippet': result.snippet if hasattr(result, 'snippet') else '',
                                'date': result.date if hasattr(result, 'date') else '',
                                'query_type': query_type
                            }
                            all_results.append(item)
                    else:
                        # query_results æ˜¯å•ä¸ªç»“æœå¯¹è±¡
                        item = {
                            'title': query_results.title if hasattr(query_results, 'title') else '',
                            'url': query_results.url if hasattr(query_results, 'url') else '',
                            'snippet': query_results.snippet if hasattr(query_results, 'snippet') else '',
                            'date': query_results.date if hasattr(query_results, 'date') else '',
                            'query_type': query_type
                        }
                        all_results.append(item)
            
            print(f"Perplexity æ‰¾åˆ° {len(all_results)} æ¡ç»“æœ")
            return all_results
            
        except Exception as e:
            print(f"Perplexity æœç´¢é”™è¯¯: {e}")
            print(f"é”™è¯¯ç±»å‹: {type(e).__name__}")
            import traceback
            print("å®Œæ•´é”™è¯¯å †æ ˆ:")
            traceback.print_exc()
            print("æç¤º: Perplexity API è°ƒç”¨å¤±è´¥ï¼Œå°†è·³è¿‡æ–°é—»æœç´¢")
            return []
    
    def search_github_trending(self) -> List[Dict]:
        """æœç´¢GitHubçƒ­é—¨AIé¡¹ç›®ï¼ˆç¼©çŸ­æ—¶é—´çª—å£è‡³24å°æ—¶ï¼‰"""
        if not self.github_token:
            print("WARNING: GitHub tokenæœªè®¾ç½®ï¼Œè·³è¿‡GitHubæœç´¢")
            return []
            
        headers = {'Authorization': f'Bearer {self.github_token}'}
        url = 'https://api.github.com/search/repositories'
        
        # ç¼©çŸ­ä¸ºè¿‡å»24å°æ—¶
        yesterday, today = self.get_date_range(hours_back=24)
        date_str = yesterday.strftime('%Y-%m-%d')
        
        params = {
            'q': f'AI machine-learning deep-learning created:>{date_str} language:python',
            'sort': 'stars',
            'order': 'desc',
            'per_page': 30  # å¢åŠ è·å–æ•°é‡ï¼Œåç»­ä¼šè¿‡æ»¤
        }
        
        try:
            print(f"æœç´¢GitHubé¡¹ç›®: created:>{date_str}")
            response = requests.get(url, headers=headers, params=params)
            print(f"GitHub APIå“åº”çŠ¶æ€: {response.status_code}")
            
            if response.status_code == 200:
                data = response.json()
                items = data.get('items', [])
                
                # å»é‡å’Œè´¨é‡è¯„åˆ†
                filtered_items = []
                for item in items:
                    if not self.is_duplicate(item):
                        item['quality_score'] = self.calculate_quality_score(item, 'github')
                        filtered_items.append(item)
                
                # æŒ‰è´¨é‡åˆ†æ•°æ’åº
                filtered_items.sort(key=lambda x: x['quality_score'], reverse=True)
                
                print(f"GitHub: åŸå§‹ {len(items)} ä¸ªï¼Œå»é‡å {len(filtered_items)} ä¸ª")
                return filtered_items[:10]  # è¿”å›å‰10ä¸ªé«˜è´¨é‡é¡¹ç›®
            else:
                print(f"GitHub APIé”™è¯¯: {response.status_code}")
        except Exception as e:
            print(f"GitHubæœç´¢é”™è¯¯: {e}")
            
        return []
    
    def search_huggingface_models(self) -> List[Dict]:
        """æœç´¢Hugging Faceæ–°æ¨¡å‹ï¼ˆç¼©çŸ­æ—¶é—´çª—å£è‡³24å°æ—¶ï¼‰"""
        if not self.hf_token:
            print("WARNING: Hugging Face tokenæœªè®¾ç½®ï¼Œè·³è¿‡HFæœç´¢")
            return []
            
        headers = {'Authorization': f'Bearer {self.hf_token}'}
        url = 'https://huggingface.co/api/models'
        
        yesterday, today = self.get_date_range(hours_back=24)
        date_str = yesterday.strftime('%Y-%m-%d')
        
        params = {
            'filter': 'pytorch',
            'sort': 'createdAt',
            'direction': -1,
            'limit': 50
        }
        
        try:
            print(f"æœç´¢Hugging Faceæ¨¡å‹")
            response = requests.get(url, headers=headers, params=params)
            print(f"HF APIå“åº”çŠ¶æ€: {response.status_code}")
            
            if response.status_code == 200:
                models = response.json()
                
                # è¿‡æ»¤æœ€è¿‘24å°æ—¶çš„æ¨¡å‹
                filtered_models = []
                for model in models:
                    created_at = model.get('createdAt', '')
                    if created_at and created_at >= date_str:
                        if not self.is_duplicate(model):
                            model['quality_score'] = self.calculate_quality_score(model, 'huggingface')
                            filtered_models.append(model)
                
                filtered_models.sort(key=lambda x: x['quality_score'], reverse=True)
                
                print(f"HF: åŸå§‹ {len(models)} ä¸ªï¼Œç­›é€‰å {len(filtered_models)} ä¸ª")
                return filtered_models[:5]
            else:
                print(f"HF APIé”™è¯¯: {response.status_code}")
        except Exception as e:
            print(f"Hugging Faceæœç´¢é”™è¯¯: {e}")
            
        return []
    
    def search_arxiv_papers(self) -> List[Dict]:
        """æœç´¢ArXivæœ€æ–°AIè®ºæ–‡ï¼ˆç¼©çŸ­æ—¶é—´çª—å£è‡³24å°æ—¶ï¼‰"""
        url = 'http://export.arxiv.org/api/query'
        
        yesterday, today = self.get_date_range(hours_back=24)
        start_date = yesterday.strftime('%Y%m%d')
        end_date = today.strftime('%Y%m%d')
        
        params = {
            'search_query': f'cat:cs.AI OR cat:cs.LG OR cat:cs.CL AND submittedDate:[{start_date}0000 TO {end_date}2359]',
            'start': 0,
            'max_results': 30,
            'sortBy': 'submittedDate',
            'sortOrder': 'descending'
        }
        
        try:
            print(f"æœç´¢ArXivè®ºæ–‡: {start_date} åˆ° {end_date}")
            response = requests.get(url, params=params)
            print(f"ArXiv APIå“åº”çŠ¶æ€: {response.status_code}")
            
            if response.status_code == 200:
                import xml.etree.ElementTree as ET
                root = ET.fromstring(response.content)
                
                papers = []
                for entry in root.findall('{http://www.w3.org/2005/Atom}entry'):
                    title_elem = entry.find('{http://www.w3.org/2005/Atom}title')
                    summary_elem = entry.find('{http://www.w3.org/2005/Atom}summary')
                    link_elem = entry.find('{http://www.w3.org/2005/Atom}id')
                    
                    if title_elem is not None and summary_elem is not None and link_elem is not None:
                        paper = {
                            'title': title_elem.text.strip() if title_elem.text else 'æ— æ ‡é¢˜',
                            'authors': [author.find('{http://www.w3.org/2005/Atom}name').text 
                                      for author in entry.findall('{http://www.w3.org/2005/Atom}author')
                                      if author.find('{http://www.w3.org/2005/Atom}name') is not None],
                            'summary': summary_elem.text.strip() if summary_elem.text else 'æ— æ‘˜è¦',
                            'link': link_elem.text.strip() if link_elem.text else ''
                        }
                        
                        if not self.is_duplicate(paper):
                            paper['quality_score'] = self.calculate_quality_score(paper, 'arxiv')
                            papers.append(paper)
                
                papers.sort(key=lambda x: x['quality_score'], reverse=True)
                
                print(f"ArXiv: æ‰¾åˆ° {len(papers)} ç¯‡è®ºæ–‡ï¼ˆå·²å»é‡å’Œæ’åºï¼‰")
                return papers[:10]
            else:
                print(f"ArXiv APIé”™è¯¯: {response.status_code}")
        except Exception as e:
            print(f"ArXivæœç´¢é”™è¯¯: {e}")
            
        return []
    
    def generate_ai_summary(self, collected_data: Dict) -> str:
        """ä½¿ç”¨AIç”Ÿæˆæ¯æ—¥åŠ¨æ€æ‘˜è¦ï¼ˆæ–°åˆ†ç±»ä½“ç³»ï¼‰"""
        github_count = len(collected_data.get('github_projects', []))
        hf_count = len(collected_data.get('hf_models', []))
        arxiv_count = len(collected_data.get('arxiv_papers', []))
        perplexity_count = len(collected_data.get('perplexity_news', []))
        
        print(f"æ•°æ®æ”¶é›†ç»Ÿè®¡:")
        print(f"   Perplexityæ–°é—»: {perplexity_count}")
        print(f"   GitHubé¡¹ç›®: {github_count}")
        print(f"   HFæ¨¡å‹: {hf_count}")
        print(f"   ArXivè®ºæ–‡: {arxiv_count}")
        
        total_items = github_count + hf_count + arxiv_count + perplexity_count
        
        if total_items == 0:
            print("WARNING: æ²¡æœ‰æ”¶é›†åˆ°ä»»ä½•æ•°æ®")
            return self.generate_fallback_summary(collected_data)
        
        if not self.ai_client:
            print("WARNING: AIå®¢æˆ·ç«¯æœªåˆå§‹åŒ–")
            return self.generate_fallback_summary(collected_data)
        
        prompt = f"""åŸºäºä»¥ä¸‹æ”¶é›†çš„AIæŠ€æœ¯åŠ¨æ€æ•°æ®ï¼Œç”Ÿæˆä¸€ä»½ä¸“ä¸šçš„æ¯æ—¥AIåŠ¨æ€æŠ¥å‘Šã€‚

æ”¶é›†åˆ°çš„æ•°æ®ï¼š
{json.dumps(collected_data, ensure_ascii=False, indent=2)}

è¯·æŒ‰ç…§ä»¥ä¸‹**æ–°çš„åˆ†ç±»ä½“ç³»**ç”Ÿæˆå†…å®¹ï¼š

## ğŸ“° ä»Šæ—¥ç„¦ç‚¹
ç²¾é€‰2-3æ¡æœ€é‡è¦çš„AIæ–°é—»/å‘å¸ƒï¼Œæ¯æ¡åŒ…å«ï¼š
- ğŸ”¥ çƒ­åº¦æ ‡è¯†ï¼ˆæ ¹æ®é‡è¦æ€§ï¼šğŸ”¥ğŸ”¥ğŸ”¥ é«˜ / ğŸ”¥ğŸ”¥ ä¸­ / ğŸ”¥ æ™®é€šï¼‰
- æ ‡é¢˜å’Œä¸€å¥è¯æ€»ç»“
- ä¸ºä»€ä¹ˆé‡è¦
- é“¾æ¥

## ğŸ§  æ¨¡å‹ä¸ç®—æ³•
åŒ…å«æ–°å‘å¸ƒçš„æ¨¡å‹ï¼ˆå¤§è¯­è¨€æ¨¡å‹ã€å¤šæ¨¡æ€æ¨¡å‹ã€ä¸“ä¸šé¢†åŸŸæ¨¡å‹ç­‰ï¼‰ï¼Œæ¯é¡¹åŒ…å«ï¼š
- æ¨¡å‹åç§°å’Œé“¾æ¥
- æ ¸å¿ƒç‰¹æ€§
- æ€§èƒ½æ•°æ®ï¼ˆå¦‚æœ‰ï¼‰
- é€‚ç”¨åœºæ™¯

## ğŸ› ï¸ å·¥å…·ä¸æ¡†æ¶
åŒ…å«å¼€å‘æ¡†æ¶ã€è®­ç»ƒå·¥å…·ã€éƒ¨ç½²æ–¹æ¡ˆï¼Œæ¯é¡¹åŒ…å«ï¼š
- å·¥å…·åç§°å’Œé“¾æ¥
- ä¸»è¦åŠŸèƒ½
- Stars æ•°é‡ï¼ˆGitHubé¡¹ç›®ï¼‰
- æ¨èæŒ‡æ•°ï¼ˆâ­â­â­â­â­ï¼‰

## ğŸ“± åº”ç”¨ä¸äº§å“
åŒ…å«å•†ä¸šäº§å“ã€å¼€æºåº”ç”¨ã€DemoåŸå‹ï¼Œæ¯é¡¹åŒ…å«ï¼š
- åº”ç”¨åç§°å’Œé“¾æ¥
- åŠŸèƒ½æè¿°
- æŠ€æœ¯æ ˆ
- å®ç”¨æ€§è¯„ä¼°

## ğŸ“š å­¦æœ¯å‰æ²¿
åŒ…å«æœ€æ–°è®ºæ–‡ï¼Œæ¯é¡¹åŒ…å«ï¼š
- è®ºæ–‡æ ‡é¢˜å’Œé“¾æ¥
- ä½œè€…
- æ ¸å¿ƒè´¡çŒ®
- åˆ›æ–°ç‚¹

## ğŸ’¡ ç¼–è¾‘ç‚¹è¯„
æ ¹æ®ä»Šæ—¥å†…å®¹ï¼Œæä¾›ï¼š
- æŠ€æœ¯è¶‹åŠ¿è§‚å¯Ÿï¼ˆ2-3æ¡ï¼‰
- å€¼å¾—å…³æ³¨çš„æ–¹å‘
- è¡Œä¸šå½±å“åˆ†æ

**è¦æ±‚**ï¼š
1. å†…å®¹è¦å‡†ç¡®ã€ä¸“ä¸šã€æœ‰ä»·å€¼
2. çªå‡ºé‡ç‚¹ï¼Œé¿å…ä¿¡æ¯å †ç Œ
3. ä½¿ç”¨ Emoji å¢åŠ å¯è¯»æ€§
4. å¦‚æœæŸä¸ªåˆ†ç±»æ•°æ®ä¸è¶³ï¼Œå¯ä»¥åˆå¹¶åˆ°å…¶ä»–åˆ†ç±»æˆ–ç®€è¦è¯´æ˜
5. ä½¿ç”¨ä¸­æ–‡ï¼Œè¯­è¨€ç®€æ´ä¸“ä¸š
6. ä¸ºæ¯ä¸ªé‡è¦é¡¹ç›®æ·»åŠ è´¨é‡è¯„ä»·ï¼ˆåŸºäº starsã€ä¸‹è½½é‡ã€ä½œè€…ç­‰ï¼‰
"""
        
        try:
            print("å¼€å§‹AIç”Ÿæˆæ‘˜è¦ï¼ˆæ–°æ ¼å¼ï¼‰...")
            
            if self.use_google_sdk:
                response = self.ai_client.generate_content(prompt)
                content = response.text if hasattr(response, 'text') else None
            else:
                response = self.ai_client.chat.completions.create(
                    model="gemini-2.5-flash",
                    messages=[{"role": "user", "content": prompt}],
                    max_tokens=3000,
                    temperature=0.7
                )
                content = response.choices[0].message.content if response.choices else None
            
            if not content or content.strip() == "":
                print("WARNING: AIè¿”å›äº†ç©ºå†…å®¹")
                return self.generate_fallback_summary(collected_data)
            
            print(f"AIæ‘˜è¦ç”Ÿæˆå®Œæˆï¼ˆé•¿åº¦: {len(content)}ï¼‰")
            return content
            
        except Exception as e:
            print(f"AIç”Ÿæˆæ‘˜è¦é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
            return self.generate_fallback_summary(collected_data)
    
    def generate_fallback_summary(self, collected_data: Dict) -> str:
        """ç”Ÿæˆå¤‡ç”¨æ‘˜è¦ï¼ˆæ–°æ ¼å¼ï¼‰"""
        print("ä½¿ç”¨ fallback æ‘˜è¦ç”Ÿæˆå™¨ï¼ˆæ–°æ ¼å¼ï¼‰...")
        summary = ""
        
        # ä»Šæ—¥ç„¦ç‚¹
        summary += "## ğŸ“° ä»Šæ—¥ç„¦ç‚¹\n\n"
        focus_items = []
        
        # ä» Perplexity ç»“æœä¸­é€‰æ‹©ç„¦ç‚¹
        perplexity_news = collected_data.get('perplexity_news', [])
        if perplexity_news:
            for item in perplexity_news[:2]:
                title = item.get('title', 'æœªçŸ¥æ ‡é¢˜')
                url = item.get('url', '')
                snippet = item.get('snippet', '')[:150]
                summary += f"### ğŸ”¥ğŸ”¥ [{title}]({url})\n"
                summary += f"- **ç®€ä»‹**: {snippet}...\n"
                summary += f"- **æ¥æº**: Perplexity AI æ–°é—»æœç´¢\n\n"
                focus_items.append(title)
        
        # ä» GitHub é«˜è´¨é‡é¡¹ç›®ä¸­é€‰æ‹©
        github_projects = sorted(
            collected_data.get('github_projects', []),
            key=lambda x: x.get('quality_score', 0),
            reverse=True
        )
        if github_projects and len(focus_items) < 3:
            item = github_projects[0]
            name = item.get('name', 'æœªçŸ¥é¡¹ç›®')
            desc = item.get('description', 'æ— æè¿°')
            url = item.get('html_url', '')
            stars = item.get('stargazers_count', 0)
            summary += f"### ğŸ”¥ [{name}]({url})\n"
            summary += f"- **æè¿°**: {desc}\n"
            summary += f"- **çƒ­åº¦**: â­ {stars} stars\n"
            summary += f"- **æ¨èç†ç”±**: é«˜è´¨é‡å¼€æºé¡¹ç›®ï¼Œå€¼å¾—å…³æ³¨\n\n"
        
        if not focus_items and not github_projects:
            summary += "æš‚æ— ç‰¹åˆ«çªå‡ºçš„ç„¦ç‚¹æ–°é—»ã€‚\n\n"
        
        # æ¨¡å‹ä¸ç®—æ³•
        summary += "## ğŸ§  æ¨¡å‹ä¸ç®—æ³•\n\n"
        hf_models = collected_data.get('hf_models', [])
        if hf_models:
            for model in hf_models[:3]:
                model_id = model.get('modelId', 'æœªçŸ¥æ¨¡å‹')
                pipeline = model.get('pipeline_tag', 'æœªçŸ¥ç±»å‹')
                downloads = model.get('downloads', 0)
                score = model.get('quality_score', 5.0)
                stars = 'â­' * int(score / 2)
                summary += f"### {model_id}\n"
                summary += f"- **ç±»å‹**: {pipeline}\n"
                summary += f"- **ä¸‹è½½é‡**: {downloads:,}\n"
                summary += f"- **è¯„åˆ†**: {stars} ({score:.1f}/10)\n"
                summary += f"- **é“¾æ¥**: https://huggingface.co/{model_id}\n\n"
        else:
            summary += "ä»Šæ—¥æš‚æ— æ–°æ¨¡å‹å‘å¸ƒã€‚\n\n"
        
        # å·¥å…·ä¸æ¡†æ¶
        summary += "## ğŸ› ï¸ å·¥å…·ä¸æ¡†æ¶\n\n"
        if github_projects:
            for project in github_projects[:5]:
                name = project.get('name', 'æœªçŸ¥é¡¹ç›®')
                desc = project.get('description', 'æ— æè¿°')
                url = project.get('html_url', '')
                stars = project.get('stargazers_count', 0)
                score = project.get('quality_score', 5.0)
                rating = 'â­' * min(5, int(score / 2))
                summary += f"### [{name}]({url})\n"
                summary += f"- **åŠŸèƒ½**: {desc}\n"
                summary += f"- **Stars**: {stars:,}\n"
                summary += f"- **æ¨èæŒ‡æ•°**: {rating}\n\n"
        else:
            summary += "ä»Šæ—¥æš‚æ— æ–°å·¥å…·æ¡†æ¶å‘å¸ƒã€‚\n\n"
        
        # å­¦æœ¯å‰æ²¿
        summary += "## ğŸ“š å­¦æœ¯å‰æ²¿\n\n"
        arxiv_papers = collected_data.get('arxiv_papers', [])
        if arxiv_papers:
            for paper in arxiv_papers[:5]:
                title = paper.get('title', 'æœªçŸ¥æ ‡é¢˜')
                authors = ', '.join(paper.get('authors', [])[:3])
                if len(paper.get('authors', [])) > 3:
                    authors += ' ç­‰'
                link = paper.get('link', '')
                abstract = paper.get('summary', 'æ— æ‘˜è¦')[:200]
                score = paper.get('quality_score', 5.0)
                summary += f"### [{title}]({link})\n"
                summary += f"- **ä½œè€…**: {authors}\n"
                summary += f"- **æ‘˜è¦**: {abstract}...\n"
                summary += f"- **è´¨é‡è¯„åˆ†**: {score:.1f}/10\n\n"
        else:
            summary += "ä»Šæ—¥æš‚æ— é‡è¦è®ºæ–‡å‘å¸ƒã€‚\n\n"
        
        # ç¼–è¾‘ç‚¹è¯„
        summary += "## ğŸ’¡ ç¼–è¾‘ç‚¹è¯„\n\n"
        total_items = len(github_projects) + len(hf_models) + len(arxiv_papers) + len(perplexity_news)
        if total_items > 0:
            summary += f"ä»Šæ—¥å…±æ”¶é›†åˆ° {total_items} æ¡AIåŠ¨æ€ï¼Œå…¶ä¸­ï¼š\n"
            if perplexity_news:
                summary += f"- ğŸ“° AIæ–°é—»: {len(perplexity_news)} æ¡\n"
            if github_projects:
                summary += f"- ğŸ› ï¸ GitHubé¡¹ç›®: {len(github_projects)} ä¸ª\n"
            if hf_models:
                summary += f"- ğŸ§  æ–°æ¨¡å‹: {len(hf_models)} ä¸ª\n"
            if arxiv_papers:
                summary += f"- ğŸ“š å­¦æœ¯è®ºæ–‡: {len(arxiv_papers)} ç¯‡\n"
            summary += "\nå†…å®¹è´¨é‡å‡ç»è¿‡è‡ªåŠ¨è¯„åˆ†å’Œæ’åºï¼Œä¼˜å…ˆå±•ç¤ºé«˜è´¨é‡é¡¹ç›®ã€‚\n"
        else:
            summary += "ä»Šæ—¥æ•°æ®æ”¶é›†é‡åˆ°é—®é¢˜ï¼Œå»ºè®®ç¨åé‡è¯•æˆ–æ‰‹åŠ¨æ£€æŸ¥æ•°æ®æºã€‚\n\n"
            summary += "å¯èƒ½åŸå› ï¼šAPIé™æµã€ç½‘ç»œé—®é¢˜ã€æˆ–ç¡®å®æ— æ–°å†…å®¹å‘å¸ƒã€‚\n"
        
        return summary
    
    def create_daily_content(self) -> str:
        """åˆ›å»ºæ¯æ—¥å†…å®¹æ–‡ä»¶"""
        yesterday, today = self.get_date_range(hours_back=24)
        date_str = today.strftime('%Y-%m-%d')
        time_range = f"{yesterday.strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M')} - {today.strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M')}"
        
        print(f"æ—¶é—´èŒƒå›´: {time_range}")
        
        # åŠ è½½å†å²æ•°æ®ç”¨äºå»é‡
        self.seen_urls = self.load_history_items(days_back=7)
        self.seen_titles = set()  # é‡ç½®æ ‡é¢˜é›†åˆ
        
        # æ”¶é›†æ•°æ®
        print("=" * 60)
        print("å¼€å§‹æ”¶é›†æ•°æ®...")
        print("=" * 60)
        
        collected_data = {
            'perplexity_news': self.search_perplexity_ai_news(),
            'github_projects': self.search_github_trending(),
            'hf_models': self.search_huggingface_models(),
            'arxiv_papers': self.search_arxiv_papers()
        }
        
        print("=" * 60)
        print("æ•°æ®æ”¶é›†å®Œæˆï¼Œå¼€å§‹ç”Ÿæˆæ‘˜è¦...")
        print("=" * 60)
        
        # ç”ŸæˆAIæ‘˜è¦
        ai_summary = self.generate_ai_summary(collected_data)
        
        # ç»Ÿè®¡ä¿¡æ¯
        total_items = sum(len(v) for v in collected_data.values())
        
        # åˆ›å»ºMarkdownå†…å®¹
        content = f"""---
title: "æ¯æ—¥AIåŠ¨æ€ - {date_str}"
date: {today.isoformat()}+08:00
draft: false
categories: ["daily_ai"]
tags: ["AIåŠ¨æ€", "æŠ€æœ¯æ›´æ–°", "è¡Œä¸šè¶‹åŠ¿"]
description: "{date_str}çš„AIæŠ€æœ¯åŠ¨æ€æ±‡æ€»"
readingTime: "{max(3, total_items // 3)} min"
totalItems: {total_items}
---

# æ¯æ—¥AIåŠ¨æ€ - {date_str}

> ğŸ“… **æ—¶é—´èŒƒå›´**: {time_range} (åŒ—äº¬æ—¶é—´)  
> ğŸ“Š **å†…å®¹ç»Ÿè®¡**: å…± {total_items} æ¡åŠ¨æ€  
> â±ï¸ **é¢„è®¡é˜…è¯»**: {max(3, total_items // 3)} åˆ†é’Ÿ

---

{ai_summary}

---

## ğŸ“Š æ•°æ®æ¥æº

æœ¬æŠ¥å‘Šæ•°æ®æ¥æºäºï¼š
- ğŸ” **Perplexity AI**: å®æ—¶AIæ–°é—»æœç´¢
- ğŸ’» **GitHub**: AIç›¸å…³å¼€æºé¡¹ç›®
- ğŸ¤— **Hugging Face**: æ–°æ¨¡å‹å‘å¸ƒ
- ğŸ“„ **arXiv**: æœ€æ–°å­¦æœ¯è®ºæ–‡

æ‰€æœ‰å†…å®¹ç»è¿‡**è´¨é‡è¯„åˆ†**ã€**å»é‡**å’Œ**æ™ºèƒ½æ’åº**ï¼Œç¡®ä¿ä¿¡æ¯çš„ä»·å€¼å’Œæ—¶æ•ˆæ€§ã€‚

---

> ğŸ’¡ **æç¤º**: æœ¬å†…å®¹ç”± AI è‡ªåŠ¨ç”Ÿæˆï¼Œæ¯æ—¥åŒ—äº¬æ—¶é—´ 08:00 æ›´æ–°ã€‚  
> å¦‚æœ‰é—æ¼æˆ–é”™è¯¯ï¼Œæ¬¢è¿é€šè¿‡ [Issues](https://github.com/hobbytp/hobbytp.github.io/issues) åé¦ˆã€‚
"""
        
        return content
    
    def save_daily_content(self):
        """ä¿å­˜æ¯æ—¥å†…å®¹"""
        content = self.create_daily_content()
        date_str = datetime.datetime.now().strftime('%Y-%m-%d')
        file_path = self.content_dir / f"{date_str}.md"
        
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(content)
        
        print("=" * 60)
        print(f"âœ… æ¯æ—¥AIåŠ¨æ€å·²ä¿å­˜åˆ°: {file_path}")
        print("=" * 60)
        return file_path

def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("æ¯æ—¥AIåŠ¨æ€æ”¶é›†å™¨ V2.0 - ä¸“ä¸šç‰ˆ")
    print("=" * 60)
    
    collector = DailyAICollectorV2()
    file_path = collector.save_daily_content()
    
    print(f"\nâœ… æ”¶é›†å®Œæˆ: {file_path}")
    print("\næ–°åŠŸèƒ½ï¼š")
    print("  âœ… Perplexity AI æ–°é—»æœç´¢")
    print("  âœ… 24å°æ—¶æ—¶é—´çª—å£ï¼ˆçœŸæ­£çš„æ¯æ—¥åŠ¨æ€ï¼‰")
    print("  âœ… æ™ºèƒ½å»é‡")
    print("  âœ… å†…å®¹è´¨é‡è¯„åˆ†")
    print("  âœ… æ–°çš„åˆ†ç±»ä½“ç³»")
    print("  âœ… æ”¹è¿›çš„å±•ç°æ ¼å¼")

if __name__ == "__main__":
    main()
