#!/usr/bin/env python3
"""
æ¯æ—¥AIåŠ¨æ€æ”¶é›†è„šæœ¬
è‡ªåŠ¨æ”¶é›†AIé¢†åŸŸçš„æœ€æ–°åŠ¨æ€ï¼ŒåŒ…æ‹¬æ–°æ¨¡å‹ã€æ–°æ¡†æ¶ã€æ–°åº”ç”¨ç­‰
"""

import os
import json
import requests
import datetime
from typing import List, Dict, Any
import openai
from pathlib import Path
import yaml

class DailyAICollector:
    def __init__(self):
        #self.openai_client = openai.OpenAI(api_key=os.getenv('OPENAI_API_KEY'))
        self.openai_client = openai.OpenAI(
            api_key=os.getenv('GEMINI_API_KEY'),
            base_url="https://generativelanguage.googleapis.com/v1beta/openai/"
        )

        self.github_token = os.getenv('GITHUB_TOKEN')
        self.hf_token = os.getenv('HUGGINGFACE_API_KEY')
        self.content_dir = Path("content/zh/daily_ai")
        self.content_dir.mkdir(exist_ok=True)
        
    def get_date_range(self) -> tuple:
        """è·å–æ—¶é—´èŒƒå›´ï¼ˆæ˜¨å¤©8ç‚¹åˆ°ä»Šå¤©8ç‚¹ï¼‰"""
        now = datetime.datetime.now()
        today_8am = now.replace(hour=8, minute=0, second=0, microsecond=0)
        yesterday_8am = today_8am - datetime.timedelta(days=1)
        return yesterday_8am, today_8am
    
    def search_github_trending(self) -> List[Dict]:
        """æœç´¢GitHubçƒ­é—¨AIé¡¹ç›®"""
        if not self.github_token:
            print("WARNING GitHub tokenæœªè®¾ç½®ï¼Œè·³è¿‡GitHubæœç´¢")
            return []
            
        headers = {'Authorization': f'Bearer {self.github_token}'}
        url = 'https://api.github.com/search/repositories'
        
        # æœç´¢è¿‡å»7å¤©åˆ›å»ºçš„AIç›¸å…³é¡¹ç›®ï¼ˆæ”¾å®½æ—¶é—´èŒƒå›´ï¼‰
        yesterday, today = self.get_date_range()
        date_str = (yesterday - datetime.timedelta(days=6)).strftime('%Y-%m-%d')
        
        params = {
            'q': f'AI machine-learning deep-learning created:>{date_str} language:python language:javascript',
            'sort': 'stars',
            'order': 'desc',
            'per_page': 20
        }
        
        try:
            print(f"æœç´¢GitHubé¡¹ç›®: {params['q']}")
            response = requests.get(url, headers=headers, params=params)
            print(f"GitHub APIå“åº”çŠ¶æ€: {response.status_code}")
            
            if response.status_code == 200:
                data = response.json()
                items = data.get('items', [])
                print(f"æ‰¾åˆ° {len(items)} ä¸ªGitHubé¡¹ç›®")
                return items[:10]  # è¿”å›å‰10ä¸ª
            else:
                print(f"GitHub APIé”™è¯¯: {response.status_code} - {response.text}")
        except Exception as e:
            print(f"GitHubæœç´¢é”™è¯¯: {e}")
            
        return []
    
    def search_huggingface_models(self) -> List[Dict]:
        """æœç´¢Hugging Faceæ–°æ¨¡å‹"""
        if not self.hf_token:
            print("WARNING Hugging Face tokenæœªè®¾ç½®ï¼Œè·³è¿‡HFæœç´¢")
            return []
            
        headers = {'Authorization': f'Bearer {self.hf_token}'}
        url = 'https://huggingface.co/api/models'
        
        # æœç´¢è¿‡å»7å¤©åˆ›å»ºçš„æ¨¡å‹ï¼ˆæ”¾å®½æ—¶é—´èŒƒå›´ï¼‰
        yesterday, today = self.get_date_range()
        date_str = (yesterday - datetime.timedelta(days=6)).strftime('%Y-%m-%d')
        
        params = {
            'filter': 'pytorch',
            'sort': 'downloads',
            'direction': -1,
            'limit': 50
        }
        
        try:
            print(f"æœç´¢Hugging Faceæ¨¡å‹")
            response = requests.get(url, headers=headers, params=params)
            print(f"HF APIå“åº”çŠ¶æ€: {response.status_code}")
            
            if response.status_code == 200:
                models = response.json()
                print(f"è·å–åˆ° {len(models)} ä¸ªæ¨¡å‹")
                
                # è¿‡æ»¤æœ€è¿‘åˆ›å»ºçš„æ¨¡å‹
                recent_models = []
                for model in models:
                    created_at = model.get('created_at', '')
                    if created_at and created_at >= date_str:
                        recent_models.append(model)
                
                print(f"æ‰¾åˆ° {len(recent_models)} ä¸ªæœ€è¿‘åˆ›å»ºçš„æ¨¡å‹")
                return recent_models[:5]
            else:
                print(f"HF APIé”™è¯¯: {response.status_code} - {response.text}")
        except Exception as e:
            print(f"Hugging Faceæœç´¢é”™è¯¯: {e}")
            
        return []
    
    def search_arxiv_papers(self) -> List[Dict]:
        """æœç´¢ArXivæœ€æ–°AIè®ºæ–‡"""
        url = 'http://export.arxiv.org/api/query'
        
        # æœç´¢è¿‡å»7å¤©çš„è®ºæ–‡ï¼ˆæ”¾å®½æ—¶é—´èŒƒå›´ï¼‰
        yesterday, today = self.get_date_range()
        start_date = (yesterday - datetime.timedelta(days=6)).strftime('%Y%m%d')
        end_date = today.strftime('%Y%m%d')
        
        params = {
            'search_query': f'cat:cs.AI OR cat:cs.LG OR cat:cs.CL AND submittedDate:[{start_date}0000 TO {end_date}2359]',
            'start': 0,
            'max_results': 20,
            'sortBy': 'submittedDate',
            'sortOrder': 'descending'
        }
        
        try:
            print(f"æœç´¢ArXivè®ºæ–‡: {params['search_query']}")
            response = requests.get(url, params=params)
            print(f"ArXiv APIå“åº”çŠ¶æ€: {response.status_code}")
            
            if response.status_code == 200:
                # è§£æXMLå“åº”
                import xml.etree.ElementTree as ET
                root = ET.fromstring(response.content)
                
                papers = []
                for entry in root.findall('{http://www.w3.org/2005/Atom}entry'):
                    title_elem = entry.find('{http://www.w3.org/2005/Atom}title')
                    summary_elem = entry.find('{http://www.w3.org/2005/Atom}summary')
                    link_elem = entry.find('{http://www.w3.org/2005/Atom}id')
                    
                    if title_elem is not None and summary_elem is not None and link_elem is not None:
                        paper = {
                            'title': title_elem.text.strip() if title_elem.text else 'æ— æ ‡é¢˜',
                            'authors': [author.find('{http://www.w3.org/2005/Atom}name').text 
                                      for author in entry.findall('{http://www.w3.org/2005/Atom}author')
                                      if author.find('{http://www.w3.org/2005/Atom}name') is not None],
                            'summary': summary_elem.text.strip() if summary_elem.text else 'æ— æ‘˜è¦',
                            'link': link_elem.text.strip() if link_elem.text else ''
                        }
                        papers.append(paper)
                
                print(f"æ‰¾åˆ° {len(papers)} ç¯‡ArXivè®ºæ–‡")
                return papers[:5]
            else:
                print(f"ArXiv APIé”™è¯¯: {response.status_code}")
        except Exception as e:
            print(f"ArXivæœç´¢é”™è¯¯: {e}")
            
        return []
    
    def generate_ai_summary(self, collected_data: Dict) -> str:
        """ä½¿ç”¨AIç”Ÿæˆæ¯æ—¥åŠ¨æ€æ‘˜è¦"""
        # å…ˆæ‰“å°æ”¶é›†åˆ°çš„æ•°æ®ç»Ÿè®¡
        github_count = len(collected_data.get('github_projects', []))
        hf_count = len(collected_data.get('hf_models', []))
        arxiv_count = len(collected_data.get('arxiv_papers', []))
        
        print(f"æ•°æ®æ”¶é›†ç»Ÿè®¡:")
        print(f"   GitHubé¡¹ç›®: {github_count}")
        print(f"   HFæ¨¡å‹: {hf_count}")
        print(f"   ArXivè®ºæ–‡: {arxiv_count}")
        
        prompt = f"""
        åŸºäºä»¥ä¸‹æ”¶é›†çš„AIæŠ€æœ¯åŠ¨æ€æ•°æ®ï¼Œç”Ÿæˆä¸€ä»½ç»“æ„åŒ–çš„æ¯æ—¥AIåŠ¨æ€æŠ¥å‘Šï¼š

        æ•°æ®ï¼š
        {json.dumps(collected_data, ensure_ascii=False, indent=2)}

        è¯·æŒ‰ç…§ä»¥ä¸‹æ ¼å¼ç”Ÿæˆå†…å®¹ï¼š
        1. æ–°æ¨¡å‹å‘å¸ƒ
        2. æ–°æ¡†æ¶å·¥å…·  
        3. æ–°åº”ç”¨äº§å“
        4. æ–°æ ‡å‡†è§„èŒƒ
        5. æ–°å¼€æºé¡¹ç›®
        6. æ–°è®ºæ–‡å‘å¸ƒ
        7. ç§‘æŠ€è®¿è°ˆ
        8. æŠ€æœ¯æŠ¥å‘Š
        9. è®ºå›ä¼šè®®
        10. è¡Œä¸šè¶‹åŠ¿

        æ¯ä¸ªåˆ†ç±»ä¸‹åŒ…å«2-3ä¸ªé‡è¦åŠ¨æ€ï¼Œæ¯ä¸ªåŠ¨æ€åŒ…å«æ ‡é¢˜ã€ç®€è¦æè¿°å’Œé“¾æ¥ã€‚
        ä½¿ç”¨ä¸­æ–‡ï¼Œå†…å®¹è¦å‡†ç¡®ã€ç®€æ´ã€æœ‰ä»·å€¼ã€‚
        å¦‚æœæŸä¸ªåˆ†ç±»æ²¡æœ‰æ•°æ®ï¼Œè¯·æ˜ç¡®è¯´æ˜"ä»Šæ—¥æ— æ–°å¢åŠ¨æ€"ã€‚
        """
        
        try:
            print("å¼€å§‹AIç”Ÿæˆæ‘˜è¦...")
            response = self.openai_client.chat.completions.create(
                model="gemini-2.5-flash",
                messages=[{"role": "user", "content": prompt}],
                max_tokens=2000,
                temperature=0.7
            )
            print("AIæ‘˜è¦ç”Ÿæˆå®Œæˆ")
            return response.choices[0].message.content
        except Exception as e:
            print(f"AIç”Ÿæˆæ‘˜è¦é”™è¯¯: {e}")
            return self.generate_fallback_summary(collected_data)
    
    def generate_fallback_summary(self, collected_data: Dict) -> str:
        """ç”Ÿæˆå¤‡ç”¨æ‘˜è¦"""
        summary = "# æ¯æ—¥AIåŠ¨æ€\n\n"
        
        if collected_data.get('github_projects'):
            summary += "## æ–°å¼€æºé¡¹ç›®\n"
            for project in collected_data['github_projects'][:3]:
                summary += f"- **{project['name']}**: {project.get('description', 'æ— æè¿°')}\n"
            summary += "\n"
        
        if collected_data.get('hf_models'):
            summary += "## æ–°æ¨¡å‹å‘å¸ƒ\n"
            for model in collected_data['hf_models'][:3]:
                summary += f"- **{model['modelId']}**: {model.get('pipeline_tag', 'æœªçŸ¥ç±»å‹')}\n"
            summary += "\n"
        
        if collected_data.get('arxiv_papers'):
            summary += "## æ–°è®ºæ–‡å‘å¸ƒ\n"
            for paper in collected_data['arxiv_papers'][:3]:
                summary += f"- **{paper['title']}**: {paper['summary'][:100]}...\n"
            summary += "\n"
        
        return summary
    
    def create_daily_content(self) -> str:
        """åˆ›å»ºæ¯æ—¥å†…å®¹æ–‡ä»¶"""
        yesterday, today = self.get_date_range()
        date_str = today.strftime('%Y-%m-%d')
        time_range = f"{yesterday.strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M')} - {today.strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M')}"
        
        print(f"æ—¶é—´èŒƒå›´: {time_range}")
        
        # æ”¶é›†æ•°æ®
        print("å¼€å§‹æ”¶é›†æ•°æ®...")
        collected_data = {
            'github_projects': self.search_github_trending(),
            'hf_models': self.search_huggingface_models(),
            'arxiv_papers': self.search_arxiv_papers()
        }
        
        # ç”ŸæˆAIæ‘˜è¦
        ai_summary = self.generate_ai_summary(collected_data)
        
        # åˆ›å»ºMarkdownå†…å®¹
        content = f"""---
title: "æ¯æ—¥AIåŠ¨æ€ - {date_str}"
date: {today.isoformat()}+08:00
draft: false
categories: ["daily_ai"]
tags: ["AIåŠ¨æ€", "æŠ€æœ¯æ›´æ–°", "è¡Œä¸šè¶‹åŠ¿"]
description: "{date_str}çš„AIæŠ€æœ¯åŠ¨æ€æ±‡æ€»"
---

# æ¯æ—¥AIåŠ¨æ€ - {date_str}

> ğŸ“… **æ—¶é—´èŒƒå›´**: {time_range} (åŒ—äº¬æ—¶é—´)

{ai_summary}

---

> ğŸ’¡ **æç¤º**: æœ¬å†…å®¹ç”±GitHub Actionè‡ªåŠ¨ç”Ÿæˆï¼Œæ¯æ—¥æ›´æ–°ã€‚å¦‚æœ‰é—æ¼æˆ–é”™è¯¯ï¼Œæ¬¢è¿é€šè¿‡Issuesåé¦ˆã€‚
"""
        
        return content
    
    def save_daily_content(self):
        """ä¿å­˜æ¯æ—¥å†…å®¹"""
        content = self.create_daily_content()
        date_str = datetime.datetime.now().strftime('%Y-%m-%d')
        file_path = self.content_dir / f"{date_str}.md"
        
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(content)
        
        print(f"æ¯æ—¥AIåŠ¨æ€å·²ä¿å­˜åˆ°: {file_path}")
        return file_path

def main():
    """ä¸»å‡½æ•°"""
    print("å¼€å§‹æ”¶é›†æ¯æ—¥AIåŠ¨æ€...")
    
    collector = DailyAICollector()
    file_path = collector.save_daily_content()
    
    print(f"æ¯æ—¥AIåŠ¨æ€æ”¶é›†å®Œæˆ: {file_path}")

if __name__ == "__main__":
    main()
