#!/usr/bin/env python3
"""
æ›´æ–°åšå®¢æ–‡ç« çš„å­—æ•°å’Œé˜…è¯»æ—¶é—´ç»Ÿè®¡
è‡ªåŠ¨æ‰«æcontentç›®å½•ä¸‹çš„æ‰€æœ‰markdownæ–‡ä»¶ï¼Œè®¡ç®—ä¸­æ–‡å­—ç¬¦æ•°å¹¶æ›´æ–°front matter
"""

import os
import re
import sys
from pathlib import Path
from typing import Optional, Tuple

def count_chinese_chars(text: str) -> int:
    """ç»Ÿè®¡ä¸­æ–‡å­—ç¬¦æ•°ï¼ˆåŒ…æ‹¬ä¸­æ–‡æ ‡ç‚¹ç¬¦å·ï¼‰"""
    # åŒ¹é…ä¸­æ–‡å­—ç¬¦
    # \u4e00-\u9fa5: ä¸­æ–‡æ±‰å­—
    # \u3000-\u303f: CJKç¬¦å·å’Œæ ‡ç‚¹
    # \uff00-\uffef: å…¨è§’ç¬¦å·
    chinese_pattern = re.compile(r'[\u4e00-\u9fa5\u3000-\u303f\uff00-\uffef]')
    matches = chinese_pattern.findall(text)
    return len(matches)

def extract_frontmatter(content: str) -> Tuple[Optional[str], str]:
    """æå–front matterå’Œæ­£æ–‡å†…å®¹"""
    if not content.startswith('---'):
        return None, content
    
    # æŸ¥æ‰¾ç¬¬äºŒä¸ª ---
    parts = content.split('---', 2)
    if len(parts) < 3:
        return None, content
    
    frontmatter = parts[1].strip()
    body = parts[2]
    
    return frontmatter, body

def parse_frontmatter(frontmatter: str) -> dict:
    """è§£æYAML front matter"""
    import yaml
    try:
        return yaml.safe_load(frontmatter) or {}
    except Exception as e:
        print(f"è­¦å‘Š: è§£æfront matterå¤±è´¥: {e}")
        return {}

def update_frontmatter(data: dict, word_count: int, reading_time: int) -> str:
    """æ›´æ–°front matteræ•°æ®å¹¶è¿”å›YAMLå­—ç¬¦ä¸²"""
    import yaml
    data['wordCount'] = word_count
    data['readingTime'] = reading_time
    return yaml.dump(data, allow_unicode=True, default_flow_style=False, sort_keys=False)

def process_markdown_file(file_path: Path, update: bool = False) -> Tuple[int, int, bool]:
    """
    å¤„ç†å•ä¸ªmarkdownæ–‡ä»¶
    è¿”å›: (å­—æ•°, é˜…è¯»æ—¶é—´, æ˜¯å¦å·²æ›´æ–°)
    """
    try:
        content = file_path.read_text(encoding='utf-8')
    except Exception as e:
        print(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
        return 0, 0, False
    
    frontmatter_str, body = extract_frontmatter(content)
    
    if frontmatter_str is None:
        print(f"âš ï¸  è·³è¿‡ï¼ˆæ— front matterï¼‰: {file_path}")
        return 0, 0, False
    
    # ç»Ÿè®¡æ­£æ–‡ä¸­çš„ä¸­æ–‡å­—ç¬¦æ•°
    word_count = count_chinese_chars(body)
    
    # è®¡ç®—é˜…è¯»æ—¶é—´ï¼š250å­—/åˆ†é’Ÿ
    reading_speed = 250
    reading_time = max(1, (word_count + reading_speed - 1) // reading_speed)
    
    # è§£æfront matter
    fm_data = parse_frontmatter(frontmatter_str)
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°
    current_word_count = fm_data.get('wordCount', 0)
    current_reading_time = fm_data.get('readingTime', 0)
    
    if not update:
        # ä»…æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        status = "âœ…" if current_word_count == word_count and current_reading_time == reading_time else "âš ï¸ "
        print(f"{status} {file_path.relative_to(Path.cwd())}")
        print(f"   å½“å‰: {current_word_count} å­—, {current_reading_time} åˆ†é’Ÿ")
        print(f"   å®é™…: {word_count} å­—, {reading_time} åˆ†é’Ÿ")
        return word_count, reading_time, False
    
    # éœ€è¦æ›´æ–°
    if current_word_count != word_count or current_reading_time != reading_time:
        # æ›´æ–°front matter
        new_frontmatter = update_frontmatter(fm_data, word_count, reading_time)
        
        # é‡æ–°ç»„è£…æ–‡ä»¶å†…å®¹
        new_content = f"---\n{new_frontmatter}---{body}"
        
        # å†™å›æ–‡ä»¶
        try:
            file_path.write_text(new_content, encoding='utf-8')
            print(f"âœ… å·²æ›´æ–°: {file_path.relative_to(Path.cwd())} ({word_count} å­—, {reading_time} åˆ†é’Ÿ)")
            return word_count, reading_time, True
        except Exception as e:
            print(f"âŒ å†™å…¥æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
            return word_count, reading_time, False
    else:
        print(f"âœ“ æ— éœ€æ›´æ–°: {file_path.relative_to(Path.cwd())}")
        return word_count, reading_time, False

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(
        description='æ›´æ–°åšå®¢æ–‡ç« çš„å­—æ•°å’Œé˜…è¯»æ—¶é—´ç»Ÿè®¡',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # æ£€æŸ¥æ‰€æœ‰æ–‡ä»¶ï¼ˆä¸ä¿®æ”¹ï¼‰
  python scripts/update_word_count.py
  
  # æ›´æ–°æ‰€æœ‰æ–‡ä»¶
  python scripts/update_word_count.py --update
  
  # æ›´æ–°ç‰¹å®šæ–‡ä»¶
  python scripts/update_word_count.py --update content/zh/projects/mcp/skill_seeker.md
  
  # ä»…å¤„ç†ç‰¹å®šç›®å½•
  python scripts/update_word_count.py --update --dir content/zh/daily_ai
        """
    )
    parser.add_argument(
        'files',
        nargs='*',
        help='è¦å¤„ç†çš„markdownæ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤ï¼šæ‰«æcontentç›®å½•ä¸‹æ‰€æœ‰.mdæ–‡ä»¶ï¼‰'
    )
    parser.add_argument(
        '--update',
        action='store_true',
        help='å®é™…æ›´æ–°æ–‡ä»¶ï¼ˆé»˜è®¤ä»…æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯ï¼‰'
    )
    parser.add_argument(
        '--dir',
        type=str,
        help='ä»…å¤„ç†æŒ‡å®šç›®å½•ä¸‹çš„æ–‡ä»¶'
    )
    parser.add_argument(
        '--content-dir',
        type=str,
        default='content',
        help='å†…å®¹ç›®å½•è·¯å¾„ï¼ˆé»˜è®¤: contentï¼‰'
    )
    
    args = parser.parse_args()
    
    # ç¡®å®šè¦å¤„ç†çš„æ–‡ä»¶åˆ—è¡¨
    if args.files:
        files = [Path(f) for f in args.files if f.endswith('.md')]
    elif args.dir:
        dir_path = Path(args.dir)
        if not dir_path.exists():
            print(f"âŒ ç›®å½•ä¸å­˜åœ¨: {dir_path}")
            sys.exit(1)
        files = list(dir_path.rglob('*.md'))
    else:
        content_dir = Path(args.content_dir)
        if not content_dir.exists():
            print(f"âŒ å†…å®¹ç›®å½•ä¸å­˜åœ¨: {content_dir}")
            sys.exit(1)
        files = list(content_dir.rglob('*.md'))
    
    if not files:
        print("âš ï¸  æœªæ‰¾åˆ°ä»»ä½•markdownæ–‡ä»¶")
        sys.exit(0)
    
    print(f"{'=' * 60}")
    if args.update:
        print(f"ğŸ”„ æ›´æ–°æ¨¡å¼: å°†æ›´æ–° {len(files)} ä¸ªæ–‡ä»¶")
    else:
        print(f"ğŸ“Š æ£€æŸ¥æ¨¡å¼: å°†æ£€æŸ¥ {len(files)} ä¸ªæ–‡ä»¶ï¼ˆä½¿ç”¨ --update å®é™…æ›´æ–°ï¼‰")
    print(f"{'=' * 60}\n")
    
    total_word_count = 0
    total_reading_time = 0
    updated_count = 0
    
    for file_path in sorted(files):
        word_count, reading_time, updated = process_markdown_file(file_path, args.update)
        total_word_count += word_count
        total_reading_time += reading_time
        if updated:
            updated_count += 1
    
    print(f"\n{'=' * 60}")
    print(f"ğŸ“ˆ ç»Ÿè®¡æ±‡æ€»:")
    print(f"   æ€»æ–‡ä»¶æ•°: {len(files)}")
    if args.update:
        print(f"   å·²æ›´æ–°: {updated_count}")
        print(f"   æ— éœ€æ›´æ–°: {len(files) - updated_count}")
    print(f"   æ€»å­—æ•°: {total_word_count:,} å­—")
    print(f"   æ€»é˜…è¯»æ—¶é—´: {total_reading_time} åˆ†é’Ÿ")
    print(f"{'=' * 60}")

if __name__ == '__main__':
    try:
        import yaml
    except ImportError:
        print("âŒ é”™è¯¯: éœ€è¦å®‰è£… PyYAML")
        print("   å®‰è£…å‘½ä»¤: pip install pyyaml")
        sys.exit(1)
    
    main()

