#!/usr/bin/env python3
"""
æ•°æ®æ‘„å–ç®¡é“ (Data Ingestion Pipeline)
è§£æ Hugo Markdown æ–‡ä»¶ï¼Œå‘é‡åŒ–å¹¶é€šè¿‡ REST API å­˜å…¥ Cloudflare Vectorize
"""

import os
import re
import json
import hashlib
from pathlib import Path
from typing import Dict, List, Optional
from urllib.parse import urljoin

import requests
import frontmatter

from dotenv import load_dotenv
load_dotenv()

class BlogIngester:
    """åšå®¢å†…å®¹æ‘„å–å™¨"""
    
    # Cloudflare Workers AI Embedding æ¨¡å‹
    EMBEDDING_MODEL = "@cf/baai/bge-base-en-v1.5"
    
    # æ–‡æœ¬åˆ‡ç‰‡é…ç½®
    CHUNK_SIZE = 500  # æ¯ä¸ªchunkçº¦500å­—ç¬¦
    CHUNK_OVERLAP = 50  # é‡å 50å­—ç¬¦
    
    def __init__(self, content_dir: str = "content", base_url: str = "https://hobbytp.github.io", state_file: str = ".ingest_state.json"):
        self.content_dir = Path(content_dir)
        self.base_url = base_url.rstrip('/')
        self.state_file = Path(state_file)
        self.state = self.load_state()
        
        # ä»ç¯å¢ƒå˜é‡è¯»å–Cloudflareé…ç½®
        # æ³¨æ„ï¼šCF_ å‰ç¼€å·²å¼ƒç”¨ï¼Œä½¿ç”¨ CLOUDFLARE_ å‰ç¼€ï¼ˆCloudflareå®˜æ–¹è¦æ±‚ï¼‰
        self.account_id = os.getenv('CLOUDFLARE_ACCOUNT_ID')
        self.api_token = os.getenv('CLOUDFLARE_API_TOKEN')
        self.index_name = os.getenv('CLOUDFLARE_VECTORIZE_INDEX_NAME', 'blog-index')
        
        # è°ƒè¯•ä¿¡æ¯ï¼šæ˜¾ç¤ºç¯å¢ƒå˜é‡æ˜¯å¦åŠ è½½ï¼ˆä¸æ˜¾ç¤ºå®Œæ•´tokenï¼‰
        print("\nğŸ”§ Cloudflare é…ç½®æ£€æŸ¥:")
        if self.account_id:
            print(f"   âœ… CLOUDFLARE_ACCOUNT_ID: {self.account_id[:8]}...{self.account_id[-4:]}")
        else:
            print("   âŒ CLOUDFLARE_ACCOUNT_ID: æœªè®¾ç½®")
        
        if self.api_token:
            token_preview = f"{self.api_token[:8]}...{self.api_token[-4:]}" if len(self.api_token) > 12 else "***"
            print(f"   âœ… CLOUDFLARE_API_TOKEN: {token_preview} (é•¿åº¦: {len(self.api_token)})")
        else:
            print("   âŒ CLOUDFLARE_API_TOKEN: æœªè®¾ç½®")
        
        print(f"   ğŸ“¦ Vectorizeç´¢å¼•åç§°: {self.index_name}\n")
        
        if not self.account_id or not self.api_token:
            raise ValueError(
                "è¯·è®¾ç½®ç¯å¢ƒå˜é‡ CLOUDFLARE_ACCOUNT_ID å’Œ CLOUDFLARE_API_TOKEN\n"
                "æ³¨æ„ï¼šCF_ å‰ç¼€å·²å¼ƒç”¨ï¼Œè¯·ä½¿ç”¨ CLOUDFLARE_ å‰ç¼€\n"
                "è·å–æ–¹å¼ï¼š\n"
                "1. CLOUDFLARE_ACCOUNT_ID: Cloudflare Dashboard -> å³ä¾§æ æ˜¾ç¤º\n"
                "2. CLOUDFLARE_API_TOKEN: https://dash.cloudflare.com/profile/api-tokens\n"
                "   éœ€è¦æƒé™ï¼šAccount: Cloudflare Workers AI:Edit, Account: Vectorize:Edit\n"
                "å‚è€ƒæ–‡æ¡£: https://developers.cloudflare.com/workers/wrangler/system-environment-variables/"
            )
        
        # æ¸…ç†API Tokenï¼ˆç§»é™¤å¼•å·ã€ç©ºç™½å­—ç¬¦ã€æ¢è¡Œç¬¦ï¼‰
        self.api_token = self.api_token.strip()
        # ç§»é™¤å¯èƒ½çš„å¼•å·åŒ…è£¹
        if (self.api_token.startswith('"') and self.api_token.endswith('"')) or \
           (self.api_token.startswith("'") and self.api_token.endswith("'")):
            self.api_token = self.api_token[1:-1].strip()
        # ç§»é™¤æ‰€æœ‰ç©ºç™½å­—ç¬¦å’Œæ¢è¡Œç¬¦ï¼ˆtokenåº”è¯¥æ˜¯è¿ç»­çš„å­—ç¬¦ä¸²ï¼‰
        self.api_token = ''.join(self.api_token.split())
        
        # Cloudflare API ç«¯ç‚¹
        self.ai_api_base = f"https://api.cloudflare.com/client/v4/accounts/{self.account_id}/ai/run"
        # Vectorize v2 REST API åŸºç¡€è·¯å¾„ï¼ˆv1å·²å¼ƒç”¨ï¼Œé¿å… incorrect_api_version é”™è¯¯ï¼‰
        self.vectorize_api_base_v2 = f"https://api.cloudflare.com/client/v4/accounts/{self.account_id}/vectorize/v2/indexes/{self.index_name}"
        
        # è¯·æ±‚å¤´
        self.headers = {
            "Authorization": f"Bearer {self.api_token}",
            "Content-Type": "application/json"
        }
        
        # é…ç½®ä»£ç†ï¼ˆå¦‚æœè®¾ç½®äº†ä»£ç†ç¯å¢ƒå˜é‡ï¼‰
        self.proxies = None
        http_proxy = os.getenv('HTTP_PROXY') or os.getenv('http_proxy')
        https_proxy = os.getenv('HTTPS_PROXY') or os.getenv('https_proxy')
        if http_proxy or https_proxy:
            self.proxies = {
                'http': http_proxy,
                'https': https_proxy or http_proxy
            }
            print(f"ğŸŒ æ£€æµ‹åˆ°ä»£ç†è®¾ç½®: {https_proxy or http_proxy}\n")
            
    def load_state(self) -> Dict[str, str]:
        """åŠ è½½æ‘„å–çŠ¶æ€"""
        if self.state_file.exists():
            try:
                with open(self.state_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                print(f"âš ï¸  åŠ è½½çŠ¶æ€æ–‡ä»¶å¤±è´¥: {e}")
        return {}

    def save_state(self):
        """ä¿å­˜æ‘„å–çŠ¶æ€"""
        try:
            with open(self.state_file, 'w', encoding='utf-8') as f:
                json.dump(self.state, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"âš ï¸  ä¿å­˜çŠ¶æ€æ–‡ä»¶å¤±è´¥: {e}")

    def calculate_file_hash(self, file_path: Path) -> str:
        """è®¡ç®—æ–‡ä»¶å†…å®¹çš„MD5å“ˆå¸Œ"""
        try:
            with open(file_path, 'rb') as f:
                return hashlib.md5(f.read()).hexdigest()
        except Exception as e:
            print(f"âš ï¸  è®¡ç®—å“ˆå¸Œå¤±è´¥ {file_path}: {e}")
            return ""
    
    def extract_frontmatter(self, file_path: Path) -> Dict:
        """æå–Markdownæ–‡ä»¶çš„frontmatter"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                post = frontmatter.load(f)
                return {
                    'title': post.metadata.get('title', ''),
                    'date': post.metadata.get('date', ''),
                    'content': post.content,
                    # é¢å¤–ä¿ç•™å¯èƒ½çš„åˆ†ç±»å­—æ®µï¼Œä¾›åç»­è§£æè¦†ç›–æˆ–å›é€€
                    'category': post.metadata.get('category', None),
                    'categories': post.metadata.get('categories', None),
                }
        except Exception as e:
            print(f"âŒ è§£æfrontmatterå¤±è´¥ {file_path}: {e}")
            return {}
    
    def clean_markdown(self, text: str) -> str:
        """æ¸…æ´—Markdownç¬¦å·ï¼Œä¿ç•™çº¯æ–‡æœ¬"""
        # ç§»é™¤å›¾ç‰‡æ ‡è®° ![alt](url)
        text = re.sub(r'!\[.*?\]\(.*?\)', '', text)
        # ç§»é™¤é“¾æ¥ä½†ä¿ç•™æ–‡æœ¬ [text](url) -> text
        text = re.sub(r'\[([^\]]+)\]\([^\)]+\)', r'\1', text)
        # ç§»é™¤ç²—ä½“/æ–œä½“æ ‡è®°
        text = re.sub(r'\*\*([^\*]+)\*\*', r'\1', text)
        text = re.sub(r'\*([^\*]+)\*', r'\1', text)
        text = re.sub(r'__([^_]+)__', r'\1', text)
        text = re.sub(r'_([^_]+)_', r'\1', text)
        # ç§»é™¤ä»£ç å—
        text = re.sub(r'```[\s\S]*?```', '', text)
        text = re.sub(r'`[^`]+`', '', text)
        # ç§»é™¤æ ‡é¢˜æ ‡è®°
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)
        # ç§»é™¤æ°´å¹³çº¿
        text = re.sub(r'^---+$', '', text, flags=re.MULTILINE)
        # ç§»é™¤å¤šä½™ç©ºç™½
        text = re.sub(r'\n\s*\n', '\n\n', text)
        text = text.strip()
        return text
    
    def chunk_text(self, text: str) -> List[str]:
        """å°†æ–‡æœ¬åˆ‡ç‰‡ï¼Œçº¦500å­—ç¬¦/chunkï¼Œé‡å 50å­—ç¬¦"""
        if not text:
            return []
        
        chunks = []
        start = 0
        text_length = len(text)
        
        while start < text_length:
            # è®¡ç®—chunkç»“æŸä½ç½®
            end = min(start + self.CHUNK_SIZE, text_length)
            
            # å¦‚æœä¸æ˜¯æœ€åä¸€å—ï¼Œå°è¯•åœ¨å¥å·ã€æ¢è¡Œç¬¦å¤„æ–­å¼€
            if end < text_length:
                # å‘åæŸ¥æ‰¾åˆé€‚çš„æ–­ç‚¹ï¼ˆå¥å·ã€æ¢è¡Œç¬¦ï¼‰
                for i in range(end, max(start + self.CHUNK_SIZE - 100, start), -1):
                    if text[i] in 'ã€‚\n':
                        end = i + 1
                        break
            
            chunk = text[start:end].strip()
            if chunk:
                chunks.append(chunk)
            
            # å¦‚æœå·²ç»åˆ°è¾¾æ–‡æ¡£æœ«å°¾ï¼Œé€€å‡ºå¾ªç¯
            if end >= text_length:
                break
            
            # ä¸‹ä¸€ä¸ªchunkçš„èµ·å§‹ä½ç½®ï¼ˆè€ƒè™‘é‡å ï¼‰
            start = end - self.CHUNK_OVERLAP
            # é˜²æ­¢æ— é™å¾ªç¯ï¼šå¦‚æœstartæ²¡æœ‰å‰è¿›ï¼Œå¼ºåˆ¶é€€å‡º
            if start >= end:
                break
        
        return chunks
    
    def generate_url(self, file_path: Path) -> str:
        """æ ¹æ®æ–‡ä»¶è·¯å¾„ç”Ÿæˆæ–‡ç« URL"""
        # è·å–ç›¸å¯¹äºcontentç›®å½•çš„è·¯å¾„
        try:
            rel_path = file_path.relative_to(self.content_dir)
        except ValueError:
            # å¦‚æœä¸åœ¨contentç›®å½•ä¸‹ï¼Œä½¿ç”¨æ–‡ä»¶å
            rel_path = file_path.name
        
        # è½¬æ¢ä¸ºURLè·¯å¾„ï¼šcontent/zh/posts/article.md -> /zh/posts/article/
        url_path = str(rel_path).replace('\\', '/')
        if url_path.endswith('.md'):
            url_path = url_path[:-3]  # ç§»é™¤.md
        
        # ç¡®ä¿ä»¥/å¼€å¤´
        if not url_path.startswith('/'):
            url_path = '/' + url_path
        
        # å¦‚æœä¸æ˜¯ä»¥/ç»“å°¾ï¼Œæ·»åŠ /
        if not url_path.endswith('/'):
            url_path += '/'
        
        return urljoin(self.base_url, url_path)

    def extract_lang_and_category(self, file_path: Path, fm: Dict) -> Dict[str, Optional[str]]:
        """ä»è·¯å¾„å’Œfront matterè§£æè¯­è¨€å’Œåˆ†ç±»ã€‚
        ä¼˜å…ˆä½¿ç”¨front matterä¸­çš„categoryï¼Œå…¶æ¬¡ä»è·¯å¾„ content/<lang>/<category>/... æ¨æ–­ã€‚
        """
        lang: Optional[str] = None
        category_from_path: Optional[str] = None
        # åŸºäºç›¸å¯¹è·¯å¾„è§£æ
        try:
            rel = file_path.relative_to(self.content_dir)
            parts = list(rel.parts)
            if len(parts) >= 2:
                lang = parts[0]
                category_from_path = parts[1]
        except ValueError:
            # å°è¯•åŸºäºç»å¯¹è·¯å¾„å®šä½ content æ®µ
            parts = list(file_path.parts)
            if 'content' in parts:
                idx = parts.index('content')
                if len(parts) > idx + 2:
                    lang = parts[idx + 1]
                    category_from_path = parts[idx + 2]

        # front matter ä¼˜å…ˆ
        fm_cat: Optional[str] = None
        if fm:
            raw_cat = fm.get('category')
            raw_cats = fm.get('categories')
            if isinstance(raw_cat, str) and raw_cat.strip():
                fm_cat = raw_cat.strip()
            elif isinstance(raw_cats, list) and len(raw_cats) > 0:
                fm_cat = str(raw_cats[0]).strip()
            elif isinstance(raw_cats, str) and raw_cats.strip():
                fm_cat = raw_cats.strip()

        category = fm_cat or category_from_path
        return {"lang": lang, "category": category}
    
    def generate_chunk_id(self, url: str, chunk_index: int) -> str:
        """ç”Ÿæˆç¡®å®šæ€§ID: md5(full_url + chunk_index)"""
        content = f"{url}{chunk_index}"
        return hashlib.md5(content.encode('utf-8')).hexdigest()
    
    def generate_embeddings(self, texts: List[str]) -> List[List[float]]:
        """è°ƒç”¨Cloudflare Workers AIç”Ÿæˆå‘é‡"""
        if not texts:
            return []
        
        url = f"{self.ai_api_base}/{self.EMBEDDING_MODEL}"
        
        # å‡†å¤‡è¯·æ±‚æ•°æ®ï¼šCloudflare Workers AI APIæœŸæœ›å•ä¸ªæ–‡æœ¬æˆ–æ–‡æœ¬æ•°ç»„
        if len(texts) == 1:
            payload = {"text": texts[0]}
        else:
            payload = {"text": texts}
        
        try:
            # æ”¯æŒæ‰¹é‡å¤„ç†
            response = requests.post(
                url,
                headers=self.headers,
                json=payload,
                proxies=self.proxies,
                timeout=60
            )
            
            # è¯¦ç»†çš„é”™è¯¯å¤„ç†
            if response.status_code == 401:
                error_data = response.json() if response.content else {}
                error_msg = error_data.get('errors', [{}])[0].get('message', 'Unknown error')
                raise ValueError(
                    f"è®¤è¯å¤±è´¥ (401): {error_msg}\n"
                    f"è¯·æ£€æŸ¥ï¼š\n"
                    f"1. CLOUDFLARE_API_TOKEN æ˜¯å¦æ­£ç¡®\n"
                    f"2. API Token æ˜¯å¦å·²è¿‡æœŸ\n"
                    f"3. API Token æ˜¯å¦æœ‰ Workers AI å’Œ Vectorize æƒé™\n"
                    f"4. ç¯å¢ƒå˜é‡æ˜¯å¦æ­£ç¡®åŠ è½½ï¼ˆæ£€æŸ¥.envæ–‡ä»¶ï¼‰\n"
                    f"å“åº”è¯¦æƒ…: {error_data}"
                )
            elif response.status_code == 403:
                error_data = response.json() if response.content else {}
                error_msg = error_data.get('errors', [{}])[0].get('message', 'Unknown error')
                raise ValueError(
                    f"æƒé™ä¸è¶³ (403): {error_msg}\n"
                    f"è¯·ç¡®ä¿ API Token æœ‰ä»¥ä¸‹æƒé™ï¼š\n"
                    f"- Account: Cloudflare Workers AI:Edit\n"
                    f"- Account: Vectorize:Edit"
                )
            
            response.raise_for_status()
            
            # Cloudflare Workers AI å¯èƒ½è¿”å›ä¸¤ç§ç»“æ„ï¼š
            # 1) ç›´æ¥åœ¨æ ¹ä¸ŠåŒ…å« data / embedding
            # 2) å°†å®é™…ç»“æœæ”¾åœ¨ result å­—æ®µä¸­ï¼Œå½¢å¦‚ï¼š
            #    {"result": {"data": [[...]], "shape": [N, 768], ...}, "success": true, ...}
            result_outer = response.json()
            result = result_outer.get('result', result_outer)
            
            # å¤„ç†è¿”å›æ ¼å¼ï¼šå¯èƒ½æ˜¯ {data: [[...]], shape: [...]} æˆ– {data: [{embedding: [...]}]}
            if 'data' in result:
                if isinstance(result['data'], list) and len(result['data']) > 0:
                    if isinstance(result['data'][0], dict) and 'embedding' in result['data'][0]:
                        # OpenAIå…¼å®¹æ ¼å¼
                        return [item['embedding'] for item in result['data']]
                    elif isinstance(result['data'][0], list):
                        # ç›´æ¥æ˜¯å‘é‡æ•°ç»„
                        return result['data']
            
            # å¦‚æœæ ¼å¼ä¸ç¬¦åˆé¢„æœŸï¼Œå°è¯•å…¶ä»–å¯èƒ½çš„ç»“æ„
            if 'embedding' in result:
                return [result['embedding']]
            
            raise ValueError(f"æ— æ³•è§£æembeddingå“åº”æ ¼å¼: {result}")
            
        except requests.exceptions.ProxyError as e:
            raise ConnectionError(
                f"ä»£ç†è¿æ¥å¤±è´¥: {e}\n"
                f"è¯·æ£€æŸ¥ä»£ç†è®¾ç½®æˆ–ä¸´æ—¶ç¦ç”¨ä»£ç†ç¯å¢ƒå˜é‡ï¼ˆHTTP_PROXY, HTTPS_PROXYï¼‰"
            )
        except requests.exceptions.ConnectionError as e:
            raise ConnectionError(
                f"ç½‘ç»œè¿æ¥å¤±è´¥: {e}\n"
                f"è¯·æ£€æŸ¥ï¼š\n"
                f"1. ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸\n"
                f"2. ä»£ç†è®¾ç½®æ˜¯å¦æ­£ç¡®ï¼ˆå¦‚æœä½¿ç”¨ä»£ç†ï¼‰\n"
                f"3. Cloudflare API æ˜¯å¦å¯è®¿é—®"
            )
        except requests.exceptions.RequestException as e:
            print(f"âŒ ç”Ÿæˆembeddingå¤±è´¥: {e}")
            if hasattr(e.response, 'text'):
                print(f"å“åº”å†…å®¹: {e.response.text}")
            raise
    
    def upload_to_vectorize(self, vectors: List[Dict]) -> bool:
        """æ‰¹é‡ä¸Šä¼ å‘é‡åˆ°Cloudflare Vectorize (ndjsonæ ¼å¼)"""
        if not vectors:
            return True
        
        url = f"{self.vectorize_api_base_v2}/upsert"
        
        # å‡†å¤‡ndjsonæ ¼å¼æ•°æ®
        ndjson_lines = []
        for vec in vectors:
            ndjson_lines.append(json.dumps(vec))
        
        ndjson_data = '\n'.join(ndjson_lines)
        
        try:
            response = requests.post(
                url,
                headers={
                    "Authorization": f"Bearer {self.api_token}",
                    "Content-Type": "application/x-ndjson"
                },
                data=ndjson_data,
                proxies=self.proxies,
                timeout=120
            )
            
            # è¯¦ç»†çš„é”™è¯¯å¤„ç†
            if response.status_code == 401:
                error_data = response.json() if response.content else {}
                error_msg = error_data.get('errors', [{}])[0].get('message', 'Unknown error')
                raise ValueError(
                    f"è®¤è¯å¤±è´¥ (401): {error_msg}\n"
                    f"è¯·æ£€æŸ¥ CLOUDFLARE_API_TOKEN æ˜¯å¦æ­£ç¡®ä¸”æœ‰ Vectorize æƒé™"
                )
            elif response.status_code == 404:
                raise ValueError(
                    f"ç´¢å¼•ä¸å­˜åœ¨ (404): {self.index_name}\n"
                    f"è¯·å…ˆåˆ›å»º Vectorize ç´¢å¼•ï¼š\n"
                    f"npx wrangler vectorize create {self.index_name} --dimensions=768 --metric=cosine"
                )
            
            response.raise_for_status()
            return True
            
        except requests.exceptions.ProxyError as e:
            raise ConnectionError(
                f"ä»£ç†è¿æ¥å¤±è´¥: {e}\n"
                f"è¯·æ£€æŸ¥ä»£ç†è®¾ç½®æˆ–ä¸´æ—¶ç¦ç”¨ä»£ç†ç¯å¢ƒå˜é‡ï¼ˆHTTP_PROXY, HTTPS_PROXYï¼‰"
            )
        except requests.exceptions.ConnectionError as e:
            raise ConnectionError(
                f"ç½‘ç»œè¿æ¥å¤±è´¥: {e}\n"
                f"è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥å’Œä»£ç†è®¾ç½®"
            )
        except requests.exceptions.RequestException as e:
            print(f"âŒ ä¸Šä¼ åˆ°Vectorizeå¤±è´¥: {e}")
            if hasattr(e, 'response') and e.response is not None:
                try:
                    error_data = e.response.json()
                    print(f"å“åº”å†…å®¹: {error_data}")
                except:
                    print(f"å“åº”å†…å®¹: {e.response.text}")
            raise
    
    def process_file(self, file_path: Path) -> int:
        """å¤„ç†å•ä¸ªMarkdownæ–‡ä»¶ï¼Œè¿”å›å¤„ç†çš„chunkæ•°é‡"""
        print(f"\nğŸ“„ å¤„ç†æ–‡ä»¶: {file_path}")
        
        # æå–frontmatter
        metadata = self.extract_frontmatter(file_path)
        if not metadata:
            print(f"âš ï¸  è·³è¿‡ï¼ˆæ— æ³•è§£æfrontmatterï¼‰")
            return 0
        
        title = metadata.get('title', '')
        content = metadata.get('content', '')
        
        if not content:
            print(f"âš ï¸  è·³è¿‡ï¼ˆå†…å®¹ä¸ºç©ºï¼‰")
            return 0
        
        # ç”ŸæˆURL
        url = self.generate_url(file_path)
        print(f"   URL: {url}")
        print(f"   æ ‡é¢˜: {title}")
        
        # æ¸…æ´—Markdown
        clean_text = self.clean_markdown(content)
        
        # æ–‡æœ¬åˆ‡ç‰‡
        chunks = self.chunk_text(clean_text)
        print(f"   ç”Ÿæˆ {len(chunks)} ä¸ªchunks")
        
        if not chunks:
            print(f"âš ï¸  è·³è¿‡ï¼ˆæ— æœ‰æ•ˆchunksï¼‰")
            return 0
        
        # æ‰¹é‡ç”Ÿæˆembeddings
        print(f"   ğŸ”„ ç”Ÿæˆembeddings...")
        try:
            embeddings = self.generate_embeddings(chunks)
        except Exception as e:
            print(f"âŒ ç”Ÿæˆembeddingså¤±è´¥: {e}")
            return 0
        
        if len(embeddings) != len(chunks):
            print(f"âš ï¸  embeddingsæ•°é‡({len(embeddings)})ä¸chunksæ•°é‡({len(chunks)})ä¸åŒ¹é…")
            return 0
        
        # å‡†å¤‡å‘é‡æ•°æ®
        vectors = []
        lc = self.extract_lang_and_category(file_path, metadata)
        for i, (chunk, embedding) in enumerate(zip(chunks, embeddings)):
            chunk_id = self.generate_chunk_id(url, i)
            vectors.append({
                "id": chunk_id,
                "values": embedding,
                "metadata": {
                    "url": url,
                    "title": title,
                    "text": chunk,
                    "chunk_index": i,
                    # å¯ç­›é€‰å­—æ®µï¼šcategoryï¼ˆå¯é€šè¿‡ wrangler åˆ›å»ºmetadataç´¢å¼•ï¼‰
                    "category": lc.get("category")
                }
            })
        
        # æ‰¹é‡ä¸Šä¼ åˆ°Vectorize
        print(f"   ğŸ“¤ ä¸Šä¼ åˆ°Vectorize...")
        if self.upload_to_vectorize(vectors):
            print(f"   âœ… æˆåŠŸä¸Šä¼  {len(vectors)} ä¸ªå‘é‡")
            return len(vectors)
        else:
            print(f"   âŒ ä¸Šä¼ å¤±è´¥")
            return 0
    
    def ingest_all(self, pattern: str = "**/*.md", force: bool = False) -> Dict[str, int]:
        """å¤„ç†æ‰€æœ‰Markdownæ–‡ä»¶"""
        stats = {
            'total_files': 0,
            'processed_files': 0,
            'skipped_files': 0,
            'total_chunks': 0,
            'failed_files': 0
        }
        
        # æŸ¥æ‰¾æ‰€æœ‰Markdownæ–‡ä»¶
        md_files = list(self.content_dir.rglob(pattern))
        # æ’é™¤_index.mdæ–‡ä»¶
        md_files = [f for f in md_files if f.name != '_index.md']
        
        stats['total_files'] = len(md_files)
        print(f"\nğŸ“š æ‰¾åˆ° {stats['total_files']} ä¸ªMarkdownæ–‡ä»¶")
        
        state_changed = False
        
        for file_path in md_files:
            try:
                # è®¡ç®—ç›¸å¯¹è·¯å¾„ä½œä¸ºkey
                rel_path = str(file_path.relative_to(self.content_dir))
                current_hash = self.calculate_file_hash(file_path)
                
                # æ£€æŸ¥æ˜¯å¦éœ€è¦è·³è¿‡
                if not force and rel_path in self.state and self.state[rel_path] == current_hash:
                    # print(f"â© è·³è¿‡æœªä¿®æ”¹æ–‡ä»¶: {rel_path}")
                    stats['skipped_files'] += 1
                    continue
                
                chunks_count = self.process_file(file_path)
                if chunks_count > 0:
                    stats['processed_files'] += 1
                    stats['total_chunks'] += chunks_count
                    # æ›´æ–°çŠ¶æ€
                    self.state[rel_path] = current_hash
                    state_changed = True
                else:
                    stats['failed_files'] += 1
            except Exception as e:
                print(f"âŒ å¤„ç†æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
                stats['failed_files'] += 1
        
        # ä¿å­˜çŠ¶æ€
        if state_changed:
            self.save_state()
            print(f"ğŸ’¾ çŠ¶æ€å·²ä¿å­˜åˆ° {self.state_file}")
        
        return stats


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='åšå®¢å†…å®¹æ‘„å–åˆ°Cloudflare Vectorize')
    parser.add_argument(
        '--content-dir',
        default='content',
        help='å†…å®¹ç›®å½•è·¯å¾„ (é»˜è®¤: content)'
    )
    parser.add_argument(
        '--base-url',
        default='https://hobbytp.github.io',
        help='åšå®¢åŸºç¡€URL (é»˜è®¤: https://hobbytp.github.io)'
    )
    parser.add_argument(
        '--file',
        help='å¤„ç†å•ä¸ªæ–‡ä»¶ï¼ˆç›¸å¯¹äºcontentç›®å½•çš„è·¯å¾„ï¼‰'
    )
    parser.add_argument(
        '--force',
        action='store_true',
        help='å¼ºåˆ¶é‡æ–°å¤„ç†æ‰€æœ‰æ–‡ä»¶ï¼Œå¿½ç•¥ç¼“å­˜çŠ¶æ€'
    )
    
    args = parser.parse_args()
    
    try:
        ingester = BlogIngester(
            content_dir=args.content_dir,
            base_url=args.base_url
        )
        
        if args.file:
            # å¤„ç†å•ä¸ªæ–‡ä»¶
            file_path = Path(args.content_dir) / args.file
            if not file_path.exists():
                print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
                return 1
            
            chunks_count = ingester.process_file(file_path)
            print(f"\nâœ… å®Œæˆï¼å¤„ç†äº† {chunks_count} ä¸ªchunks")
        else:
            # å¤„ç†æ‰€æœ‰æ–‡ä»¶
            stats = ingester.ingest_all(force=args.force)
            print(f"\n{'='*60}")
            print(f"ğŸ“Š å¤„ç†ç»Ÿè®¡:")
            print(f"   æ€»æ–‡ä»¶æ•°: {stats['total_files']}")
            print(f"   æˆåŠŸå¤„ç†: {stats['processed_files']}")
            print(f"   è·³è¿‡æ–‡ä»¶: {stats['skipped_files']}")
            print(f"   å¤±è´¥æ–‡ä»¶: {stats['failed_files']}")
            print(f"   æ€»chunks: {stats['total_chunks']}")
            print(f"{'='*60}")
        
        return 0
        
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
        return 1


if __name__ == '__main__':
    exit(main())
