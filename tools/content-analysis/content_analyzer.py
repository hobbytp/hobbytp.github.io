#!/usr/bin/env python3
"""
Hugo Blog Content Analyzer

åˆ†æHugoåšå®¢å†…å®¹è´¨é‡å’ŒSEOä¼˜åŒ–ï¼š
- å¯è¯»æ€§è¯„åˆ† (Flesch-Kincaid, SMOG, Coleman-Liau)
- å…³é”®è¯æå–å’Œåˆ†æ
- SEOä¼˜åŒ–å»ºè®®
- å†…å®¹è´¨é‡è¯„ä¼°
- ç»“æ„åˆ†æ

ä½¿ç”¨æ–¹æ³•ï¼š
python content_analyzer.py [é€‰é¡¹]

é€‰é¡¹ï¼š
--input-dir DIR    è¾“å…¥ç›®å½•ï¼ˆé»˜è®¤: content/ï¼‰
--output-file FILE è¾“å‡ºæ–‡ä»¶ï¼ˆé»˜è®¤: content-analysis-report.mdï¼‰
--analyze-single FILE åˆ†æå•ä¸ªæ–‡ä»¶
--keywords         å¯ç”¨å…³é”®è¯æå–
--seo-check        å¯ç”¨SEOæ£€æŸ¥
--readability      å¯ç”¨å¯è¯»æ€§åˆ†æ
--all             æ‰§è¡Œæ‰€æœ‰åˆ†æï¼ˆé»˜è®¤ï¼‰
"""

import os
import sys
import re
import json
import statistics
import argparse
from pathlib import Path
from typing import Dict, List, Any, Tuple, Optional
from collections import Counter
import math
from datetime import datetime

# å¯é€‰å¯¼å…¥OpenAIå’Œdotenv
try:
    import openai
except ImportError:
    openai = None

try:
    from dotenv import load_dotenv
    DOTENV_AVAILABLE = True
except ImportError:
    DOTENV_AVAILABLE = False

try:
    from rich.console import Console
    from rich.json import JSON
    from rich.panel import Panel
    from rich.text import Text
    RICH_AVAILABLE = True
except ImportError:
    RICH_AVAILABLE = False
    load_dotenv = None

# ä¸­æ–‡åœç”¨è¯åˆ—è¡¨
STOP_WORDS = {
    'çš„', 'äº†', 'å’Œ', 'æ˜¯', 'å°±', 'éƒ½', 'è€Œ', 'åŠ', 'ä¸', 'ç€', 'æˆ–', 'ä¸€ä¸ª', 'æ²¡æœ‰', 'æˆ‘ä»¬', 'ä½ ä»¬', 'ä»–ä»¬',
    'è¿™ä¸ª', 'é‚£ä¸ª', 'è¿™äº›', 'é‚£äº›', 'è¿™é‡Œ', 'é‚£é‡Œ', 'ä»€ä¹ˆ', 'æ€ä¹ˆ', 'ä¸ºä»€ä¹ˆ', 'æ€ä¹ˆæ ·', 'å¯ä»¥', 'èƒ½å¤Ÿ',
    'å¦‚æœ', 'å› ä¸º', 'æ‰€ä»¥', 'ä½†æ˜¯', 'ä¸è¿‡', 'è™½ç„¶', 'ä½†æ˜¯', 'è€Œä¸”', 'å› æ­¤', 'ç„¶å', 'å°±æ˜¯', 'å°±æ˜¯',
    'å°±æ˜¯è¯´', 'ä¹Ÿå°±æ˜¯è¯´', 'æ¯”å¦‚', 'ä¾‹å¦‚', 'ä¸€èˆ¬', 'é€šå¸¸', 'å¯èƒ½', 'ä¹Ÿè®¸', 'å¤§æ¦‚', 'å¤§çº¦', 'å·¦å³',
    'ç°åœ¨', 'ä»Šå¤©', 'æ˜¨å¤©', 'æ˜å¤©', 'æ—¶é—´', 'æ—¶å€™', 'ç›®å‰', 'æœ€è¿‘', 'å·²ç»', 'è¿˜æ˜¯', 'è¿˜æ˜¯', 'è¿˜æ˜¯',
    'è¿˜æ˜¯', 'è¿˜æ˜¯', 'è¿˜æ˜¯', 'è¿˜æ˜¯', 'è¿˜æ˜¯', 'è¿˜æ˜¯', 'è¿˜æ˜¯', 'è¿˜æ˜¯', 'è¿˜æ˜¯', 'è¿˜æ˜¯', 'è¿˜æ˜¯', 'è¿˜æ˜¯'
}

class ContentAnalyzer:
    """Hugoåšå®¢å†…å®¹åˆ†æå™¨"""

    def __init__(self, input_dir: str = "content"):
        self.input_dir = Path(input_dir)
        self.analysis_results = {}

        # åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯
        self.openai_client = None

        if openai:
            try:
                # ä¼˜å…ˆåŠ è½½.envæ–‡ä»¶ï¼Œç¡®ä¿.envæ–‡ä»¶çš„é…ç½®ä¼˜å…ˆçº§æœ€é«˜
                if DOTENV_AVAILABLE:
                    load_dotenv(override=True)  # override=True ç¡®ä¿.envæ–‡ä»¶è¦†ç›–ç°æœ‰ç¯å¢ƒå˜é‡
                    print("ğŸ“„ å·²åŠ è½½ .env æ–‡ä»¶é…ç½®")
                else:
                    print("âš ï¸  æœªæ‰¾åˆ° python-dotenvï¼Œæ— æ³•åŠ è½½ .env æ–‡ä»¶")

                # ä»ç¯å¢ƒå˜é‡è·å–é…ç½®ï¼ˆæ­¤æ—¶.envæ–‡ä»¶çš„å€¼å·²ç»è¢«åŠ è½½ï¼‰
                api_key = os.getenv('OPENAI_API_KEY')
                base_url = os.getenv('OPENAI_BASE_URL')
                model = os.getenv('OPENAI_MODEL', 'gpt-3.5-turbo')

                # æ‰“å°APIé…ç½®ä¿¡æ¯ï¼ˆå®‰å…¨æ˜¾ç¤ºï¼‰
                print("\nğŸ”§ OpenAI é…ç½®ä¿¡æ¯:")
                if api_key:
                    print(f"ğŸ”‘ OPENAI_API_KEY: {api_key[:8]}...{api_key[-4:] if len(api_key) > 12 else api_key}")
                else:
                    print("âŒ OPENAI_API_KEY: æœªè®¾ç½®")

                if base_url:
                    print(f"ğŸŒ OPENAI_BASE_URL: {base_url}")
                else:
                    print("ğŸŒ OPENAI_BASE_URL: ä½¿ç”¨é»˜è®¤ (https://api.openai.com/v1)")

                print(f"ğŸ¤– OPENAI_MODEL: {model}")
                print()

                self.openai_model = model

                if not api_key:
                    print("âš ï¸ è­¦å‘Š: æœªæ‰¾åˆ°OPENAI_API_KEYç¯å¢ƒå˜é‡")
                    print("AIå¢å¼ºåˆ†æåŠŸèƒ½å°†è¢«ç¦ç”¨")
                    return

                client_kwargs = {'api_key': api_key}
                if base_url:
                    client_kwargs['base_url'] = base_url

                self.openai_client = openai.OpenAI(**client_kwargs)
                print(f"âœ… OpenAIå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ (æ¨¡å‹: {self.openai_model})")

            except Exception as e:
                print(f"è­¦å‘Š: OpenAIå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
                print("AIå¢å¼ºåˆ†æåŠŸèƒ½å°†è¢«ç¦ç”¨")

        if not self.input_dir.exists():
            raise FileNotFoundError(f"å†…å®¹ç›®å½•ä¸å­˜åœ¨: {self.input_dir}")

    def extract_frontmatter(self, content: str) -> Dict[str, Any]:
        """æå–Hugo frontmatter"""
        frontmatter = {}
        lines = content.split('\n')

        if not lines[0].strip() == '---':
            return frontmatter

        # æŸ¥æ‰¾frontmatterç»“æŸæ ‡è®°
        end_idx = -1
        for i, line in enumerate(lines[1:], 1):
            if line.strip() == '---':
                end_idx = i
                break

        if end_idx == -1:
            return frontmatter

        # è§£æfrontmatter
        frontmatter_lines = lines[1:end_idx]
        frontmatter_text = '\n'.join(frontmatter_lines)

        # ç®€å•è§£æYAMLæ ¼å¼
        for line in frontmatter_lines:
            line = line.strip()
            if ':' in line and not line.startswith('#'):
                key, value = line.split(':', 1)
                key = key.strip()
                value = value.strip().strip('"').strip("'")

                # å¤„ç†æ•°ç»„
                if value.startswith('[') and value.endswith(']'):
                    value = [item.strip().strip('"').strip("'") for item in value[1:-1].split(',') if item.strip()]

                frontmatter[key] = value

        return frontmatter

    def extract_content_body(self, content: str) -> str:
        """æå–å†…å®¹ä¸»ä½“ï¼ˆå»é™¤frontmatterï¼‰"""
        lines = content.split('\n')

        if not lines[0].strip() == '---':
            return content

        # æŸ¥æ‰¾frontmatterç»“æŸæ ‡è®°
        end_idx = -1
        for i, line in enumerate(lines[1:], 1):
            if line.strip() == '---':
                end_idx = i + 1
                break

        if end_idx == -1:
            return content

        return '\n'.join(lines[end_idx:])

    def clean_markdown(self, text: str) -> str:
        """æ¸…ç†Markdownæ ‡è®°ï¼Œè¿”å›çº¯æ–‡æœ¬"""
        # ç§»é™¤frontmatter
        text = self.extract_content_body(text)

        # ç§»é™¤ä»£ç å—
        text = re.sub(r'```[\s\S]*?```', '', text)
        text = re.sub(r'`[^`]*`', '', text)

        # ç§»é™¤é“¾æ¥
        text = re.sub(r'\[([^\]]*)\]\([^\)]*\)', r'\1', text)
        text = re.sub(r'https?://[^\s]+', '', text)

        # ç§»é™¤å›¾ç‰‡
        text = re.sub(r'!\[([^\]]*)\]\([^\)]*\)', '', text)

        # ç§»é™¤æ ‡é¢˜æ ‡è®°
        text = re.sub(r'^#+\s*', '', text, flags=re.MULTILINE)

        # ç§»é™¤åˆ—è¡¨æ ‡è®°
        text = re.sub(r'^[\s]*[-\*\+]\s+', '', text, flags=re.MULTILINE)
        text = re.sub(r'^\s*\d+\.\s+', '', text, flags=re.MULTILINE)

        # ç§»é™¤ç²—ä½“å’Œæ–œä½“
        text = re.sub(r'\*\*([^\*]*)\*\*', r'\1', text)
        text = re.sub(r'\*([^\*]*)\*', r'\1', text)
        text = re.sub(r'_([^_]*)_', r'\1', text)

        # ç§»é™¤å¤šä½™ç©ºç™½
        text = re.sub(r'\s+', ' ', text)
        text = text.strip()

        return text

    def calculate_readability_scores(self, text: str) -> Dict[str, float]:
        """è®¡ç®—å¯è¯»æ€§åˆ†æ•°"""
        if not text:
            return {'flesch_kincaid': 0, 'smog': 0, 'coleman_liau': 0, 'avg_score': 0}

        # åŸºç¡€ç»Ÿè®¡
        sentences = re.split(r'[.!?]+', text)
        sentences = [s.strip() for s in sentences if s.strip()]
        words = re.findall(r'\b\w+\b', text)
        chars = len(text.replace(' ', ''))

        total_sentences = len(sentences)
        total_words = len(words)
        total_chars = chars

        if total_sentences == 0 or total_words == 0:
            return {'flesch_kincaid': 0, 'smog': 0, 'coleman_liau': 0, 'avg_score': 0}

        # Flesch-Kincaid Grade Level
        avg_words_per_sentence = total_words / total_sentences
        avg_syllables_per_word = sum(self.count_syllables(word) for word in words) / total_words
        flesch_kincaid = 0.39 * avg_words_per_sentence + 11.8 * avg_syllables_per_word - 15.59

        # SMOG Index
        complex_words = sum(1 for word in words if self.count_syllables(word) >= 3)
        smog = 1.043 * math.sqrt(complex_words * (30 / total_sentences)) + 3.1291

        # Coleman-Liau Index
        avg_chars_per_word = total_chars / total_words
        avg_sentences_per_100_words = (total_sentences / total_words) * 100
        coleman_liau = 0.0588 * avg_chars_per_word - 0.296 * avg_sentences_per_100_words - 15.8

        # è®¡ç®—å¹³å‡åˆ†æ•°
        avg_score = (flesch_kincaid + smog + coleman_liau) / 3

        return {
            'flesch_kincaid': round(flesch_kincaid, 2),
            'smog': round(smog, 2),
            'coleman_liau': round(coleman_liau, 2),
            'avg_score': round(avg_score, 2),
            'stats': {
                'sentences': total_sentences,
                'words': total_words,
                'chars': total_chars,
                'avg_words_per_sentence': round(avg_words_per_sentence, 1),
                'avg_chars_per_word': round(avg_chars_per_word, 1)
            }
        }

    def count_syllables(self, word: str) -> int:
        """è®¡ç®—å•è¯éŸ³èŠ‚æ•°ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        word = word.lower()
        count = 0
        vowels = "aeiouy"

        if word[0] in vowels:
            count += 1

        for i in range(1, len(word)):
            if word[i] in vowels and word[i - 1] not in vowels:
                count += 1

        if word.endswith("e"):
            count -= 1

        if count == 0:
            count += 1

        return count

    def extract_keywords(self, text: str, top_n: int = 10) -> List[Tuple[str, int]]:
        """æå–å…³é”®è¯"""
        # æ¸…ç†æ–‡æœ¬
        clean_text = self.clean_markdown(text).lower()

        # åˆ†è¯ï¼ˆç®€å•æŒ‰ç©ºæ ¼å’Œæ ‡ç‚¹åˆ†å‰²ï¼‰
        words = re.findall(r'\b\w+\b', clean_text)

        # è¿‡æ»¤åœç”¨è¯å’ŒçŸ­è¯
        filtered_words = [
            word for word in words
            if len(word) > 2 and word not in STOP_WORDS and not word.isdigit()
        ]

        # ç»Ÿè®¡è¯é¢‘
        word_freq = Counter(filtered_words)

        # è¿”å›æœ€å¸¸è§çš„å…³é”®è¯
        return word_freq.most_common(top_n)

    def analyze_seo(self, content: str, filename: str) -> Dict[str, Any]:
        """åˆ†æSEOç›¸å…³æŒ‡æ ‡"""
        frontmatter = self.extract_frontmatter(content)
        body = self.extract_content_body(content)

        seo_analysis = {
            'title': frontmatter.get('title', ''),
            'description': frontmatter.get('description', ''),
            'slug': filename.replace('.md', ''),
            'issues': [],
            'suggestions': []
        }

        # æ£€æŸ¥æ ‡é¢˜
        if not seo_analysis['title']:
            seo_analysis['issues'].append('ç¼ºå°‘æ ‡é¢˜ (title)')
        elif len(seo_analysis['title']) < 30:
            seo_analysis['issues'].append('æ ‡é¢˜è¿‡çŸ­ (å»ºè®®30-60å­—ç¬¦)')
        elif len(seo_analysis['title']) > 60:
            seo_analysis['issues'].append('æ ‡é¢˜è¿‡é•¿ (å»ºè®®30-60å­—ç¬¦)')

        # æ£€æŸ¥æè¿°
        if not seo_analysis['description']:
            seo_analysis['issues'].append('ç¼ºå°‘æè¿° (description)')
        elif len(seo_analysis['description']) < 120:
            seo_analysis['issues'].append('æè¿°è¿‡çŸ­ (å»ºè®®120-160å­—ç¬¦)')
        elif len(seo_analysis['description']) > 160:
            seo_analysis['issues'].append('æè¿°è¿‡é•¿ (å»ºè®®120-160å­—ç¬¦)')

        # æ£€æŸ¥å…³é”®è¯å¯†åº¦
        clean_text = self.clean_markdown(body)
        words = re.findall(r'\b\w+\b', clean_text.lower())
        total_words = len(words)

        if total_words > 0:
            # è®¡ç®—å…³é”®è¯å¯†åº¦
            keywords = self.extract_keywords(body, 5)
            for keyword, count in keywords:
                density = (count / total_words) * 100
                if density > 5:
                    seo_analysis['issues'].append(f'å…³é”®è¯"{keyword}"å¯†åº¦è¿‡é«˜ ({density:.1f}%)')
                elif density < 0.5:
                    seo_analysis['suggestions'].append(f'è€ƒè™‘å¢åŠ å…³é”®è¯"{keyword}"çš„ä½¿ç”¨')

        # æ£€æŸ¥URLç»“æ„
        if len(seo_analysis['slug']) > 100:
            seo_analysis['issues'].append('URLè¿‡é•¿')

        # æ£€æŸ¥frontmatter
        if 'date' not in frontmatter:
            seo_analysis['issues'].append('ç¼ºå°‘å‘å¸ƒæ—¥æœŸ (date)')
        if 'draft' not in frontmatter or frontmatter['draft']:
            seo_analysis['issues'].append('æ–‡ç« ä»ä¸ºè‰ç¨¿çŠ¶æ€')

        return seo_analysis

    def analyze_content_quality_ai(self, content: str) -> Dict[str, Any]:
        """ä½¿ç”¨AIè¿›è¡Œæ™ºèƒ½å†…å®¹è´¨é‡åˆ†æ"""
        if not self.openai_client:
            return {
                'ai_analysis': None,
                'error': 'OpenAI APIæœªé…ç½®ï¼Œè¯·è®¾ç½®OPENAI_API_KEYç¯å¢ƒå˜é‡'
            }

        try:
            frontmatter = self.extract_frontmatter(content)
            body = self.extract_content_body(content)
            clean_text = self.clean_markdown(content)

            # é™åˆ¶æ–‡æœ¬é•¿åº¦ï¼Œé¿å…APIé™åˆ¶
            analysis_text = clean_text[:16000] + "..." if len(clean_text) > 16000 else clean_text

            prompt = f"""è¯·å¯¹ä»¥ä¸‹åšå®¢æ–‡ç« è¿›è¡Œæ™ºèƒ½åˆ†æï¼Œé‡ç‚¹å…³æ³¨å†…å®¹è´¨é‡ã€ç»“æ„å®Œæ•´æ€§ã€è¯»è€…ä»·å€¼ç­‰æ–¹é¢ã€‚

æ–‡ç« æ ‡é¢˜: {frontmatter.get('title', 'æ— æ ‡é¢˜')}
æ–‡ç« æè¿°: {frontmatter.get('description', 'æ— æè¿°')}

æ–‡ç« å†…å®¹:
{analysis_text}

å…ˆåˆ†ææ–‡ç« ç±»å‹ï¼Œç„¶åä»¥ä¸“ä¸šçš„è§’åº¦ï¼Œä»ä»¥ä¸‹ç»´åº¦è¿›è¡Œåˆ†æï¼ˆæ¯é¡¹ç»™å‡º1-10åˆ†ï¼Œå¹¶ç®€è¦è¯´æ˜ç†ç”±ï¼‰ï¼š

1. å†…å®¹æ·±åº¦ (1-10åˆ†): å†…å®¹çš„ä¸“ä¸šæ·±åº¦å’ŒçŸ¥è¯†ä»·å€¼
2. ç»“æ„å®Œæ•´æ€§ (1-10åˆ†): æ–‡ç« ç»“æ„æ˜¯å¦æ¸…æ™°åˆç†
3. è¯»è€…ä»·å€¼ (1-10åˆ†): å¯¹è¯»è€…æ˜¯å¦æœ‰å®é™…å¸®åŠ©å’Œå¯å‘
4. å†™ä½œè´¨é‡ (1-10åˆ†): è¯­è¨€è¡¨è¾¾ã€é€»è¾‘æ€§å’Œå¯è¯»æ€§
5. åˆ›æ–°æ€§ (1-10åˆ†): æ˜¯å¦æœ‰æ–°é¢–çš„è§‚ç‚¹æˆ–è§è§£

ç»¼åˆè¯„åˆ† (1-100åˆ†): åŸºäºä»¥ä¸Šç»´åº¦ç»™å‡ºçš„æ€»ä½“è¯„åˆ†

æ”¹è¿›å»ºè®®: åˆ—å‡º2-3ä¸ªæœ€é‡è¦çš„æ”¹è¿›ç‚¹

æ€»ä½“è¯„ä»·: ç”¨ä¸€å¥è¯æ€»ç»“æ–‡ç« è´¨é‡"""

            response = self.openai_client.chat.completions.create(
                model=self.openai_model,
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å†…å®¹è´¨é‡åˆ†æå¸ˆï¼Œæ“…é•¿åˆ†ææŠ€æœ¯åšå®¢æ–‡ç« çš„è´¨é‡ã€‚è¯·æä¾›å®¢è§‚ã€å»ºè®¾æ€§çš„åˆ†æã€‚"},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=800,
                temperature=0.3
            )

            ai_response = response.choices[0].message.content

            # è§£æAIå“åº”
            analysis = self._parse_ai_analysis(ai_response)

            return {
                'ai_analysis': analysis,
                'raw_response': ai_response,
                'model': self.openai_model
            }

        except Exception as e:
            return {
                'ai_analysis': None,
                'error': f'AIåˆ†æå¤±è´¥: {str(e)}'
            }

    def _parse_ai_analysis(self, ai_response: str) -> Dict[str, Any]:
        """è§£æAIåˆ†æç»“æœ"""
        analysis = {
            'dimensions': {},
            'overall_score': 0,
            'improvement_suggestions': [],
            'overall_assessment': '',
            'confidence': 0.8  # é»˜è®¤ç½®ä¿¡åº¦
        }

        lines = ai_response.split('\n')

        for line in lines:
            line = line.strip()
            if not line:
                continue

            # è§£æå„ç»´åº¦è¯„åˆ†
            for dimension in ['å†…å®¹æ·±åº¦', 'ç»“æ„å®Œæ•´æ€§', 'è¯»è€…ä»·å€¼', 'å†™ä½œè´¨é‡', 'åˆ›æ–°æ€§']:
                if dimension in line and ('åˆ†' in line or '/10' in line):
                    try:
                        # æå–æ•°å­—åˆ†æ•°
                        import re
                        score_match = re.search(r'(\d+(?:\.\d+)?)', line)
                        if score_match:
                            score = float(score_match.group(1))
                            if score <= 10:  # ç¡®ä¿æ˜¯æœ‰æ•ˆåˆ†æ•°
                                analysis['dimensions'][dimension] = score
                    except:
                        pass

            # è§£æç»¼åˆè¯„åˆ†
            if 'ç»¼åˆè¯„åˆ†' in line or 'æ€»ä½“è¯„åˆ†' in line:
                try:
                    import re
                    score_match = re.search(r'(\d+(?:\.\d+)?)', line)
                    if score_match:
                        score = float(score_match.group(1))
                        if score <= 100:  # ç¡®ä¿æ˜¯æœ‰æ•ˆåˆ†æ•°
                            analysis['overall_score'] = score
                except:
                    pass

            # è§£ææ”¹è¿›å»ºè®®
            if 'æ”¹è¿›å»ºè®®' in line or any(word in line for word in ['å»ºè®®', 'å¯ä»¥', 'åº”è¯¥', 'éœ€è¦']):
                if ':' in line:
                    suggestion = line.split(':', 1)[1].strip()
                    if suggestion and len(suggestion) > 5:
                        analysis['improvement_suggestions'].append(suggestion)

            # è§£ææ€»ä½“è¯„ä»·
            if 'æ€»ä½“è¯„ä»·' in line or 'ä¸€å¥è¯æ€»ç»“' in line:
                if ':' in line:
                    assessment = line.split(':', 1)[1].strip()
                    if assessment:
                        analysis['overall_assessment'] = assessment

        # è®¡ç®—å¹³å‡ç»´åº¦åˆ†æ•°ä½œä¸ºå¤‡é€‰ç»¼åˆè¯„åˆ†
        if not analysis['overall_score'] and analysis['dimensions']:
            avg_dimension_score = sum(analysis['dimensions'].values()) / len(analysis['dimensions'])
            analysis['overall_score'] = avg_dimension_score * 10  # è½¬æ¢ä¸º0-100åˆ†åˆ¶

        return analysis

    def analyze_content_quality(self, content: str, enable_ai: bool = False) -> Dict[str, Any]:
        """åˆ†æå†…å®¹è´¨é‡ï¼ˆä¼ ç»Ÿæ–¹æ³• + å¯é€‰AIå¢å¼ºï¼‰"""
        frontmatter = self.extract_frontmatter(content)
        body = self.extract_content_body(content)

        quality = {
            'score': 0,
            'max_score': 100,
            'checks': {},
            'issues': [],
            'strengths': [],
            'ai_enhanced': enable_ai
        }

        # AIå¢å¼ºåˆ†æ
        if enable_ai:
            ai_analysis = self.analyze_content_quality_ai(content)
            quality['ai_analysis'] = ai_analysis

        # æ£€æŸ¥frontmatterå®Œæ•´æ€§ (20åˆ†)
        fm_score = 0
        required_fields = ['title', 'date', 'description']
        for field in required_fields:
            if field in frontmatter:
                fm_score += 7  # 20/3 â‰ˆ 6.67

        quality['checks']['frontmatter'] = fm_score
        if fm_score >= 15:
            quality['strengths'].append('Frontmatterä¿¡æ¯å®Œæ•´')
        else:
            quality['issues'].append('Frontmatterä¿¡æ¯ä¸å®Œæ•´')

        # æ£€æŸ¥å†…å®¹é•¿åº¦ (25åˆ†)
        clean_text = self.clean_markdown(body)
        word_count = len(re.findall(r'\b\w+\b', clean_text))

        if word_count >= 1000:
            content_score = 25
            quality['strengths'].append('å†…å®¹ä¸°å¯Œè¯¦ç»†')
        elif word_count >= 500:
            content_score = 20
            quality['strengths'].append('å†…å®¹é€‚ä¸­')
        elif word_count >= 200:
            content_score = 15
        else:
            content_score = 5
            quality['issues'].append('å†…å®¹è¿‡çŸ­')

        quality['checks']['content_length'] = content_score

        # æ£€æŸ¥ç»“æ„ (20åˆ†)
        headings = re.findall(r'^#{1,6}\s+', body, re.MULTILINE)
        lists = len(re.findall(r'^[\s]*[-\*\+]\s+', body, re.MULTILINE))
        code_blocks = len(re.findall(r'```', body))

        structure_score = min(20, len(headings) * 2 + lists + code_blocks * 3)
        quality['checks']['structure'] = structure_score

        if structure_score >= 15:
            quality['strengths'].append('ç»“æ„å±‚æ¬¡æ¸…æ™°')
        elif structure_score < 5:
            quality['issues'].append('ç»“æ„å±‚æ¬¡ä¸è¶³')

        # æ£€æŸ¥å¯è¯»æ€§ (20åˆ†)
        readability = self.calculate_readability_scores(clean_text)
        avg_score = readability.get('avg_score', 0)

        if 6 <= avg_score <= 8:
            readability_score = 20
            quality['strengths'].append('å¯è¯»æ€§é€‚ä¸­')
        elif 4 <= avg_score <= 10:
            readability_score = 15
        elif 2 <= avg_score <= 12:
            readability_score = 10
        else:
            readability_score = 5
            quality['issues'].append('å¯è¯»æ€§éœ€è¦æ”¹è¿›')

        quality['checks']['readability'] = readability_score

        # æ£€æŸ¥å…³é”®è¯ä½¿ç”¨ (15åˆ†)
        keywords = self.extract_keywords(body, 3)
        if keywords and keywords[0][1] >= 3:
            keyword_score = 15
            quality['strengths'].append('å…³é”®è¯ä½¿ç”¨å¾—å½“')
        elif keywords:
            keyword_score = 10
        else:
            keyword_score = 0
            quality['issues'].append('ç¼ºå°‘æ ¸å¿ƒå…³é”®è¯')

        quality['checks']['keywords'] = keyword_score

        # è®¡ç®—æ€»åˆ†
        quality['score'] = sum(quality['checks'].values())

        return quality

    def analyze_file(self, file_path: Path, enable_ai: bool = False) -> Dict[str, Any]:
        """åˆ†æå•ä¸ªæ–‡ä»¶"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()

            analysis = {
                'file': str(file_path.relative_to(self.input_dir)),
                'frontmatter': self.extract_frontmatter(content),
                'stats': {},
                'readability': {},
                'keywords': [],
                'seo': {},
                'quality': {},
                'ai_enhanced': enable_ai
            }

            # åŸºç¡€ç»Ÿè®¡
            body = self.extract_content_body(content)
            clean_text = self.clean_markdown(content)

            analysis['stats'] = {
                'total_chars': len(content),
                'body_chars': len(body),
                'clean_chars': len(clean_text),
                'words': len(re.findall(r'\b\w+\b', clean_text)),
                'sentences': len(re.split(r'[.!?]+', clean_text)),
                'paragraphs': len([p for p in body.split('\n\n') if p.strip()]),
                'headings': len(re.findall(r'^#{1,6}\s+', body, re.MULTILINE)),
                'links': len(re.findall(r'\[([^\]]*)\]\([^\)]*\)', body)),
                'images': len(re.findall(r'!\[([^\]]*)\]\([^\)]*\)', body)),
                'code_blocks': len(re.findall(r'```', body))
            }

            # å¯è¯»æ€§åˆ†æ
            analysis['readability'] = self.calculate_readability_scores(clean_text)

            # å…³é”®è¯æå–
            analysis['keywords'] = self.extract_keywords(content, 10)

            # SEOåˆ†æ
            analysis['seo'] = self.analyze_seo(content, file_path.name)

            # è´¨é‡åˆ†æ
            analysis['quality'] = self.analyze_content_quality(content, enable_ai)

            # å¥åº·åº¦åˆ†æ
            analysis['health'] = self.analyze_content_health(content, file_path)

            return analysis

        except Exception as e:
            return {
                'file': str(file_path.relative_to(self.input_dir)),
                'error': str(e)
            }

    def analyze_content_health(self, content: str, file_path: Path) -> Dict[str, Any]:
        """åˆ†æå†…å®¹å¥åº·åº¦ï¼ˆå€Ÿé‰´Jimmy Songçš„å®ç°ï¼‰"""
        frontmatter = self.extract_frontmatter(content)
        body = self.extract_content_body(content)

        health = {
            'score': 100,
            'issues': [],
            'warnings': [],
            'suggestions': []
        }

        # æ£€æŸ¥å¿…éœ€çš„frontmatterå­—æ®µ
        required_fields = ['title', 'date', 'description']
        for field in required_fields:
            if field not in frontmatter or not frontmatter[field]:
                health['issues'].append(f'ç¼ºå¤±å¿…éœ€å­—æ®µ: {field}')
                health['score'] -= 20

        # æ£€æŸ¥æè¿°é•¿åº¦
        description = frontmatter.get('description', '')
        if len(description) < 50:
            health['warnings'].append('æè¿°è¿‡çŸ­ï¼Œå»ºè®®120-160å­—ç¬¦')
            health['score'] -= 5
        elif len(description) > 160:
            health['warnings'].append('æè¿°è¿‡é•¿ï¼Œå¯èƒ½å½±å“SEO')
            health['score'] -= 5

        # æ£€æŸ¥æ ‡ç­¾å’Œåˆ†ç±»
        tags = frontmatter.get('tags', [])
        categories = frontmatter.get('categories', [])
        if not tags and not categories:
            health['warnings'].append('ç¼ºå°‘æ ‡ç­¾æˆ–åˆ†ç±»ï¼Œå½±å“å†…å®¹ç»„ç»‡')
            health['score'] -= 10

        # æ£€æŸ¥å›¾ç‰‡altæ–‡æœ¬
        images = re.findall(r'!\[([^\]]*)\]\([^\)]*\)', body)
        missing_alt = [img for img in images if not img.strip()]
        if missing_alt:
            health['warnings'].append(f'å‘ç°{len(missing_alt)}å¼ å›¾ç‰‡ç¼ºå°‘altæ–‡æœ¬')
            health['score'] -= len(missing_alt) * 2

        # æ£€æŸ¥æ ‡é¢˜å±‚çº§
        headings = re.findall(r'^(#{1,6})\s+', body, re.MULTILINE)
        heading_levels = [len(h) for h in headings]
        if heading_levels and min(heading_levels) > 1:
            health['warnings'].append('ç¼ºå°‘ä¸€çº§æ ‡é¢˜(H1)')
            health['score'] -= 10

        # æ£€æŸ¥æ ‡é¢˜å±‚çº§è·³è·ƒ
        for i in range(1, len(heading_levels)):
            if heading_levels[i] > heading_levels[i-1] + 1:
                health['warnings'].append('æ ‡é¢˜å±‚çº§è·³è·ƒï¼Œå¯èƒ½å½±å“å¯è¯»æ€§')
                health['score'] -= 5
                break

        # æ£€æŸ¥å†…å®¹é•¿åº¦
        word_count = len(re.findall(r'\b\w+\b', body))
        if word_count < 300:
            health['suggestions'].append('å†…å®¹è¾ƒçŸ­ï¼Œå»ºè®®å¢åŠ æ›´å¤šç»†èŠ‚')
        elif word_count > 3000:
            health['suggestions'].append('å†…å®¹è¿‡é•¿ï¼Œå»ºè®®è€ƒè™‘æ‹†åˆ†ä¸ºå¤šç¯‡æ–‡ç« ')

        health['score'] = max(0, min(100, health['score']))
        return health

    def analyze_content_distribution(self, files_data: List[Dict[str, Any]]) -> Dict[str, Any]:
        """åˆ†æå†…å®¹åˆ†å¸ƒï¼ˆå€Ÿé‰´Jimmy Songçš„å®ç°ï¼‰"""
        distribution = {
            'by_category': {},
            'by_tag': {},
            'by_content_type': {},
            'by_date': {},
            'by_reading_time': {'short': 0, 'medium': 0, 'long': 0}
        }

        for file_data in files_data:
            if 'error' in file_data:
                continue

            frontmatter = file_data.get('frontmatter', {})

            # æŒ‰åˆ†ç±»ç»Ÿè®¡
            categories = frontmatter.get('categories', [])
            for category in categories:
                distribution['by_category'][category] = distribution['by_category'].get(category, 0) + 1

            # æŒ‰æ ‡ç­¾ç»Ÿè®¡
            tags = frontmatter.get('tags', [])
            for tag in tags:
                distribution['by_tag'][tag] = distribution['by_tag'].get(tag, 0) + 1

            # æŒ‰å†…å®¹ç±»å‹ç»Ÿè®¡ï¼ˆåŸºäºæ–‡ä»¶è·¯å¾„ï¼‰
            file_path = file_data.get('file', '')
            if 'posts' in file_path or 'blog' in file_path:
                content_type = 'blog_posts'
            elif 'pages' in file_path:
                content_type = 'pages'
            elif 'docs' in file_path:
                content_type = 'documentation'
            else:
                content_type = 'other'

            distribution['by_content_type'][content_type] = distribution['by_content_type'].get(content_type, 0) + 1

            # æŒ‰æ—¥æœŸç»Ÿè®¡
            date_str = frontmatter.get('date', '')
            if date_str:
                try:
                    # æå–å¹´æœˆ
                    if isinstance(date_str, str) and len(date_str) >= 7:
                        year_month = date_str[:7]  # YYYY-MM
                        distribution['by_date'][year_month] = distribution['by_date'].get(year_month, 0) + 1
                except:
                    pass

            # æŒ‰é˜…è¯»æ—¶é—´ç»Ÿè®¡
            stats = file_data.get('stats', {})
            word_count = stats.get('words', 0)
            reading_time = word_count / 200  # å‡è®¾æ¯åˆ†é’Ÿé˜…è¯»200å­—

            if reading_time < 5:
                distribution['by_reading_time']['short'] += 1
            elif reading_time < 15:
                distribution['by_reading_time']['medium'] += 1
            else:
                distribution['by_reading_time']['long'] += 1

        return distribution

    def analyze_growth_trends(self, files_data: List[Dict[str, Any]]) -> Dict[str, Any]:
        """åˆ†æå¢é•¿è¶‹åŠ¿ï¼ˆå€Ÿé‰´Jimmy Songçš„å®ç°ï¼‰"""
        trends = {
            'monthly_creation': {},
            'content_velocity': [],
            'quality_trends': {},
            'word_count_trends': {}
        }

        # æ”¶é›†æ¯æœˆåˆ›å»ºçš„å†…å®¹æ•°é‡
        for file_data in files_data:
            if 'error' in file_data:
                continue

            frontmatter = file_data.get('frontmatter', {})
            date_str = frontmatter.get('date', '')

            if date_str and isinstance(date_str, str) and len(date_str) >= 7:
                try:
                    year_month = date_str[:7]  # YYYY-MM
                    trends['monthly_creation'][year_month] = trends['monthly_creation'].get(year_month, 0) + 1

                    # æ”¶é›†è´¨é‡å’Œå­—æ•°è¶‹åŠ¿
                    quality_score = file_data.get('quality', {}).get('score', 0)
                    word_count = file_data.get('stats', {}).get('words', 0)

                    if year_month not in trends['quality_trends']:
                        trends['quality_trends'][year_month] = []
                        trends['word_count_trends'][year_month] = []

                    trends['quality_trends'][year_month].append(quality_score)
                    trends['word_count_trends'][year_month].append(word_count)

                except:
                    pass

        # è®¡ç®—å†…å®¹åˆ›å»ºé€Ÿåº¦ï¼ˆæœˆç¯æ¯”å¢é•¿ï¼‰
        sorted_months = sorted(trends['monthly_creation'].keys())
        for i in range(1, len(sorted_months)):
            current_month = sorted_months[i]
            prev_month = sorted_months[i-1]

            current_count = trends['monthly_creation'][current_month]
            prev_count = trends['monthly_creation'].get(prev_month, 0)

            if prev_count > 0:
                velocity = {
                    'month': current_month,
                    'posts': current_count,
                    'change': current_count - prev_count,
                    'change_percent': ((current_count - prev_count) / prev_count) * 100
                }
                trends['content_velocity'].append(velocity)

        # è®¡ç®—å¹³å‡è´¨é‡å’Œå­—æ•°è¶‹åŠ¿
        for month in trends['quality_trends']:
            if trends['quality_trends'][month]:
                trends['quality_trends'][month] = sum(trends['quality_trends'][month]) / len(trends['quality_trends'][month])

        for month in trends['word_count_trends']:
            if trends['word_count_trends'][month]:
                trends['word_count_trends'][month] = sum(trends['word_count_trends'][month]) / len(trends['word_count_trends'][month])

        return trends

    def analyze_directory(self, enable_ai: bool = False) -> Dict[str, Any]:
        """åˆ†ææ•´ä¸ªç›®å½•"""
        results = {
            'summary': {
                'total_files': 0,
                'analyzed_files': 0,
                'error_files': 0,
                'avg_quality_score': 0,
                'avg_readability': 0,
                'total_words': 0,
                'health_score': 0
            },
            'files': [],
            'quality_distribution': {},
            'readability_distribution': {},
            'top_keywords': [],
            'seo_issues': [],
            'distribution': {},
            'trends': {},
            'health_analysis': [],
            'ai_enhanced': enable_ai
        }

        all_keywords = Counter()
        quality_scores = []
        readability_scores = []
        seo_issues = []

        # åˆ†ææ‰€æœ‰Markdownæ–‡ä»¶
        for md_file in self.input_dir.rglob('*.md'):
            results['summary']['total_files'] += 1

            analysis = self.analyze_file(md_file, enable_ai)

            if 'error' in analysis:
                results['summary']['error_files'] += 1
            else:
                results['summary']['analyzed_files'] += 1
                results['files'].append(analysis)
                results['ai_enhanced'] = enable_ai

                # æ”¶é›†ç»Ÿè®¡æ•°æ®
                quality_scores.append(analysis['quality']['score'])
                readability_scores.append(analysis['readability']['avg_score'])
                results['summary']['total_words'] += analysis['stats']['words']

                # æ”¶é›†å…³é”®è¯
                for keyword, count in analysis['keywords']:
                    all_keywords[keyword] += count

                # æ”¶é›†SEOé—®é¢˜
                seo_issues.extend(analysis['seo']['issues'])

        # è®¡ç®—æ±‡æ€»ç»Ÿè®¡
        if quality_scores:
            results['summary']['avg_quality_score'] = round(statistics.mean(quality_scores), 1)

        if readability_scores:
            results['summary']['avg_readability'] = round(statistics.mean(readability_scores), 1)

        # è´¨é‡åˆ†å¸ƒ
        for score in quality_scores:
            level = 'ä¼˜ç§€' if score >= 80 else 'è‰¯å¥½' if score >= 60 else 'ä¸€èˆ¬' if score >= 40 else 'éœ€æ”¹è¿›'
            results['quality_distribution'][level] = results['quality_distribution'].get(level, 0) + 1

        # å¯è¯»æ€§åˆ†å¸ƒ
        for score in readability_scores:
            level = 'å®¹æ˜“' if score <= 6 else 'é€‚ä¸­' if score <= 8 else 'è¾ƒéš¾' if score <= 10 else 'å›°éš¾'
            results['readability_distribution'][level] = results['readability_distribution'].get(level, 0) + 1

        # çƒ­é—¨å…³é”®è¯
        results['top_keywords'] = all_keywords.most_common(20)

        # SEOé—®é¢˜æ±‡æ€»
        results['seo_issues'] = list(set(seo_issues))

        # å†…å®¹åˆ†å¸ƒåˆ†æï¼ˆå€Ÿé‰´Jimmy Songï¼‰
        results['distribution'] = self.analyze_content_distribution(results['files'])

        # å¢é•¿è¶‹åŠ¿åˆ†æï¼ˆå€Ÿé‰´Jimmy Songï¼‰
        results['trends'] = self.analyze_growth_trends(results['files'])

        # å¥åº·åº¦åˆ†ææ±‡æ€»
        health_scores = []
        for file_data in results['files']:
            if 'health' in file_data:
                health_scores.append(file_data['health']['score'])

        if health_scores:
            results['summary']['health_score'] = round(statistics.mean(health_scores), 1)

        self.analysis_results = results
        return results

    def generate_json_data(self, output_file: str = "content-analysis-data.json", enable_ai: bool = False):
        """ç”ŸæˆJSONæ ¼å¼çš„åˆ†ææ•°æ®ï¼ˆä¾›å‰ç«¯ä»ªè¡¨æ¿ä½¿ç”¨ï¼‰"""
        if not self.analysis_results:
            self.analyze_directory(enable_ai)

        results = self.analysis_results

        # æ¸…ç†ä¸éœ€è¦çš„æ•°æ®ï¼Œä¼˜åŒ–JSONå¤§å°
        json_data = {
            'generated_at': datetime.now().isoformat(),
            'summary': results['summary'],
            'distribution': results.get('distribution', {}),
            'trends': results.get('trends', {}),
            'seo_issues': results['seo_issues'],
            'top_keywords': results['top_keywords'][:20],  # åªä¿ç•™å‰20ä¸ªå…³é”®è¯
            'ai_enhanced': results.get('ai_enhanced', False),
            'files_count': len(results['files'])
        }

        # ä¿å­˜JSONæ•°æ®
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(json_data, f, ensure_ascii=False, indent=2)

        print(f"ğŸ“„ JSONæ•°æ®å·²ä¿å­˜åˆ°: {output_file}")
        return json_data

    def generate_report(self, output_file: str = "content-analysis-report.md", enable_ai: bool = False):
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        if not self.analysis_results:
            self.analyze_directory(enable_ai)

        results = self.analysis_results

        ai_indicator = " ğŸ¤– AIå¢å¼ºç‰ˆ" if results.get('ai_enhanced') else ""

        report = f"""# Hugoåšå®¢å†…å®¹åˆ†ææŠ¥å‘Š{ai_indicator}

ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## ğŸ“Š æ€»ä½“ç»Ÿè®¡

- **æ€»æ–‡ä»¶æ•°**: {results['summary']['total_files']}
- **æˆåŠŸåˆ†æ**: {results['summary']['analyzed_files']}
- **åˆ†æå¤±è´¥**: {results['summary']['error_files']}
- **æ€»å­—æ•°**: {results['summary']['total_words']:,}
- **å¹³å‡è´¨é‡åˆ†æ•°**: {results['summary']['avg_quality_score']}/100
- **å¹³å‡å¯è¯»æ€§**: {results['summary']['avg_readability']}

## ğŸ“ˆ è´¨é‡åˆ†å¸ƒ

"""

        for level, count in results['quality_distribution'].items():
            percentage = (count / results['summary']['analyzed_files']) * 100
            report += f"- **{level}**: {count} ç¯‡ ({percentage:.1f}%)\n"

        report += "\n## ğŸ“– å¯è¯»æ€§åˆ†å¸ƒ\n\n"

        for level, count in results['readability_distribution'].items():
            percentage = (count / results['summary']['analyzed_files']) * 100
            report += f"- **{level}**: {count} ç¯‡ ({percentage:.1f}%)\n"

        report += "\n## ğŸ”¥ çƒ­é—¨å…³é”®è¯\n\n"

        for keyword, count in results['top_keywords'][:10]:
            report += f"- **{keyword}**: {count} æ¬¡\n"

        report += "\n## âš ï¸ SEOé—®é¢˜æ±‡æ€»\n\n"

        if results['seo_issues']:
            for issue in results['seo_issues']:
                report += f"- {issue}\n"
        else:
            report += "æš‚æ— æ˜æ˜¾SEOé—®é¢˜\n"

        # å†…å®¹åˆ†å¸ƒåˆ†æï¼ˆå€Ÿé‰´Jimmy Songï¼‰
        if results.get('distribution'):
            report += "\n## ğŸ“Š å†…å®¹åˆ†å¸ƒåˆ†æ\n\n"

            dist = results['distribution']

            # æŒ‰åˆ†ç±»åˆ†å¸ƒ
            if dist['by_category']:
                report += "### æŒ‰åˆ†ç±»åˆ†å¸ƒ\n\n"
                sorted_categories = sorted(dist['by_category'].items(), key=lambda x: x[1], reverse=True)
                for category, count in sorted_categories[:10]:
                    report += f"- **{category}**: {count} ç¯‡\n"
                report += "\n"

            # æŒ‰æ ‡ç­¾åˆ†å¸ƒ
            if dist['by_tag']:
                report += "### æŒ‰æ ‡ç­¾åˆ†å¸ƒ\n\n"
                sorted_tags = sorted(dist['by_tag'].items(), key=lambda x: x[1], reverse=True)
                for tag, count in sorted_tags[:15]:
                    report += f"- **{tag}**: {count} æ¬¡\n"
                report += "\n"

            # æŒ‰é˜…è¯»æ—¶é—´åˆ†å¸ƒ
            report += "### æŒ‰é˜…è¯»æ—¶é—´åˆ†å¸ƒ\n\n"
            reading_dist = dist['by_reading_time']
            total = sum(reading_dist.values())
            if total > 0:
                for time_range, count in reading_dist.items():
                    percentage = (count / total) * 100
                    time_label = {'short': 'çŸ­æ–‡ (<5min)', 'medium': 'ä¸­ç­‰ (5-15min)', 'long': 'é•¿æ–‡ (>15min)'}[time_range]
                    report += f"- **{time_label}**: {count} ç¯‡ ({percentage:.1f}%)\n"
            report += "\n"

        # å¢é•¿è¶‹åŠ¿åˆ†æï¼ˆå€Ÿé‰´Jimmy Songï¼‰
        if results.get('trends'):
            report += "\n## ğŸ“ˆ å¢é•¿è¶‹åŠ¿åˆ†æ\n\n"

            trends = results['trends']

            # æœˆåº¦å†…å®¹åˆ›å»ºè¶‹åŠ¿
            if trends['monthly_creation']:
                report += "### æœˆåº¦å†…å®¹åˆ›å»ºè¶‹åŠ¿\n\n"
                sorted_months = sorted(trends['monthly_creation'].items())
                for month, count in sorted_months[-6:]:  # æœ€è¿‘6ä¸ªæœˆ
                    report += f"- **{month}**: {count} ç¯‡\n"
                report += "\n"

            # å†…å®¹åˆ›å»ºé€Ÿåº¦
            if trends['content_velocity']:
                report += "### å†…å®¹åˆ›å»ºé€Ÿåº¦\n\n"
                for velocity in trends['content_velocity'][-3:]:  # æœ€è¿‘3ä¸ªæœˆçš„é€Ÿåº¦
                    change_symbol = "ğŸ“ˆ" if velocity['change'] > 0 else "ğŸ“‰" if velocity['change'] < 0 else "â¡ï¸"
                    report += f"- **{velocity['month']}**: {velocity['posts']} ç¯‡ {change_symbol} {velocity['change']:+d} ({velocity['change_percent']:+.1f}%)\n"
                report += "\n"

        # å†…å®¹å¥åº·åº¦åˆ†æ
        if results['summary'].get('health_score'):
            report += "\n## ğŸ¥ å†…å®¹å¥åº·åº¦åˆ†æ\n\n"

            health_score = results['summary']['health_score']
            health_level = "ä¼˜ç§€" if health_score >= 90 else "è‰¯å¥½" if health_score >= 75 else "ä¸€èˆ¬" if health_score >= 60 else "éœ€æ”¹è¿›"

            report += f"- **æ•´ä½“å¥åº·åº¦**: {health_score}/100 ({health_level})\n"
            report += f"- **å¥åº·æ–‡ä»¶æ•°**: {len([f for f in results['files'] if f.get('health', {}).get('score', 0) >= 80])}/{results['summary']['analyzed_files']}\n\n"

            # æ±‡æ€»å¸¸è§å¥åº·é—®é¢˜
            all_issues = []
            all_warnings = []
            for file_data in results['files']:
                if 'health' in file_data:
                    all_issues.extend(file_data['health']['issues'])
                    all_warnings.extend(file_data['health']['warnings'])

            if all_issues:
                report += "### å¸¸è§é—®é¢˜\n\n"
                issue_counts = {}
                for issue in all_issues:
                    issue_counts[issue] = issue_counts.get(issue, 0) + 1
                for issue, count in sorted(issue_counts.items(), key=lambda x: x[1], reverse=True)[:5]:
                    report += f"- {issue} ({count} æ¬¡)\n"
                report += "\n"

            if all_warnings:
                report += "### ä¼˜åŒ–å»ºè®®\n\n"
                warning_counts = {}
                for warning in all_warnings:
                    warning_counts[warning] = warning_counts.get(warning, 0) + 1
                for warning, count in sorted(warning_counts.items(), key=lambda x: x[1], reverse=True)[:5]:
                    report += f"- {warning} ({count} æ¬¡)\n"
                report += "\n"

        report += "\n## ğŸ“‹ è¯¦ç»†æ–‡ä»¶åˆ†æ\n\n"

        for file_analysis in results['files'][:5]:  # åªæ˜¾ç¤ºå‰5ä¸ªæ–‡ä»¶çš„è¯¦ç»†åˆ†æ
            report += f"### {file_analysis['file']}\n\n"
            report += f"- **è´¨é‡åˆ†æ•°**: {file_analysis['quality']['score']}/100\n"
            report += f"- **å¯è¯»æ€§**: {file_analysis['readability']['avg_score']}\n"
            report += f"- **å­—æ•°**: {file_analysis['stats']['words']}\n"

            if file_analysis['seo']['issues']:
                report += f"- **SEOé—®é¢˜**: {', '.join(file_analysis['seo']['issues'][:3])}\n"

            if file_analysis['keywords']:
                top_keywords = [kw for kw, _ in file_analysis['keywords'][:3]]
                report += f"- **å…³é”®è¯**: {', '.join(top_keywords)}\n"

            report += "\n"

        # AIåˆ†æç»“æœ
        if results.get('ai_enhanced'):
            report += "\n## ğŸ¤– AIæ™ºèƒ½åˆ†æç»“æœ\n\n"

            ai_analyses = []
            for file_result in results['files']:
                if file_result.get('quality', {}).get('ai_analysis', {}).get('ai_analysis'):
                    ai_analysis = file_result['quality']['ai_analysis']['ai_analysis']
                    if ai_analysis.get('overall_score', 0) > 0:
                        ai_analyses.append({
                            'file': file_result['file'],
                            'score': ai_analysis['overall_score'],
                            'assessment': ai_analysis.get('overall_assessment', ''),
                            'dimensions': ai_analysis.get('dimensions', {})
                        })

            if ai_analyses:
                # æŒ‰AIè¯„åˆ†æ’åº
                ai_analyses.sort(key=lambda x: x['score'], reverse=True)

                report += "### ğŸ¯ AIè¯„åˆ†Top 5\n\n"
                for i, analysis in enumerate(ai_analyses[:5], 1):
                    report += f"{i}. **{Path(analysis['file']).name}**\n"
                    report += f"   - AIè¯„åˆ†: {analysis['score']:.1f}/100\n"
                    if analysis['assessment']:
                        report += f"   - è¯„ä»·: {analysis['assessment']}\n"
                    if analysis['dimensions']:
                        dims = [f"{k}:{v:.1f}" for k, v in analysis['dimensions'].items()]
                        report += f"   - ç»´åº¦: {', '.join(dims)}\n"
                    report += "\n"

                # AIæ”¹è¿›å»ºè®®æ±‡æ€»
                all_suggestions = []
                for file_result in results['files']:
                    ai_data = file_result.get('quality', {}).get('ai_analysis', {}).get('ai_analysis', {})
                    suggestions = ai_data.get('improvement_suggestions', [])
                    all_suggestions.extend(suggestions)

                if all_suggestions:
                    report += "### ğŸ’¡ AIæ”¹è¿›å»ºè®®æ±‡æ€»\n\n"
                    # ç»Ÿè®¡å»ºè®®é¢‘ç‡
                    from collections import Counter
                    suggestion_counts = Counter(all_suggestions)
                    for suggestion, count in suggestion_counts.most_common(5):
                        report += f"- {suggestion} ({count}æ¬¡æåˆ°)\n"
                    report += "\n"

        # ä¿å­˜æŠ¥å‘Š
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(report)

        print(f"ğŸ“„ åˆ†ææŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_file}")
        return report

def print_analysis_result(analysis: Dict[str, Any]) -> None:
    """ç¾åŒ–è¾“å‡ºåˆ†æç»“æœ"""
    if RICH_AVAILABLE:
        console = Console()

        # åˆ›å»ºæ ‡é¢˜
        title = Text("ğŸ“Š å†…å®¹åˆ†æç»“æœ", style="bold blue")
        console.print(Panel(title, expand=False))

        # ç¾åŒ–JSONè¾“å‡º
        json_str = json.dumps(analysis, ensure_ascii=False, indent=2)
        json_obj = JSON(json_str)
        console.print(json_obj)
    else:
        # å›é€€åˆ°æ™®é€šJSONè¾“å‡º
        print(json.dumps(analysis, ensure_ascii=False, indent=2))

def main():
    parser = argparse.ArgumentParser(
        description="Hugoåšå®¢å†…å®¹åˆ†æå·¥å…·",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=__doc__
    )

    parser.add_argument(
        '--input-dir',
        default='content',
        help='è¾“å…¥ç›®å½• (é»˜è®¤: content)'
    )

    parser.add_argument(
        '--output-file',
        default='content-analysis-report.md',
        help='è¾“å‡ºæ–‡ä»¶ (é»˜è®¤: content-analysis-report.md)'
    )

    parser.add_argument(
        '--json-data',
        action='store_true',
        help='ç”ŸæˆJSONæ•°æ®æ–‡ä»¶ä¾›å‰ç«¯ä»ªè¡¨æ¿ä½¿ç”¨'
    )

    parser.add_argument(
        '--json-output',
        default='content-analysis-data.json',
        help='JSONæ•°æ®è¾“å‡ºæ–‡ä»¶ (é»˜è®¤: content-analysis-data.json)'
    )

    parser.add_argument(
        '--analyze-single',
        help='åˆ†æå•ä¸ªæ–‡ä»¶'
    )

    parser.add_argument(
        '--keywords',
        action='store_true',
        help='å¯ç”¨å…³é”®è¯æå–'
    )

    parser.add_argument(
        '--seo-check',
        action='store_true',
        help='å¯ç”¨SEOæ£€æŸ¥'
    )

    parser.add_argument(
        '--readability',
        action='store_true',
        help='å¯ç”¨å¯è¯»æ€§åˆ†æ'
    )

    parser.add_argument(
        '--all',
        action='store_true',
        help='æ‰§è¡Œæ‰€æœ‰åˆ†æ (é»˜è®¤)'
    )

    parser.add_argument(
        '--ai-enhance',
        action='store_true',
        help='å¯ç”¨AIå¢å¼ºåˆ†æ (éœ€è¦OpenAI APIå¯†é’¥)'
    )

    args = parser.parse_args()

    # å¦‚æœæ²¡æœ‰æŒ‡å®šä»»ä½•é€‰é¡¹ï¼Œé»˜è®¤æ‰§è¡Œæ‰€æœ‰åˆ†æ
    if not any([args.keywords, args.seo_check, args.readability]):
        args.all = True

    try:
        if args.analyze_single:
            # åˆ†æå•ä¸ªæ–‡ä»¶ - ä¸éœ€è¦åˆå§‹åŒ–æ•´ä¸ªç›®å½•åˆ†æå™¨
            import os
            file_path_str = args.analyze_single

            # å¦‚æœæ˜¯ç›¸å¯¹è·¯å¾„ï¼Œä»é¡¹ç›®æ ¹ç›®å½•å¼€å§‹è§£æ
            if not os.path.isabs(file_path_str):
                # è·å–å½“å‰è„šæœ¬æ‰€åœ¨ç›®å½•ï¼Œç„¶åå‘ä¸Šæ‰¾åˆ°é¡¹ç›®æ ¹ç›®å½•
                script_dir = Path(__file__).parent
                project_root = script_dir.parent.parent  # tools/content-analysis -> tools -> project_root
                file_path_str = str(project_root / file_path_str)

            file_path = Path(file_path_str).resolve()
            if file_path.exists():
                # ä½¿ç”¨æ–‡ä»¶æ‰€åœ¨ç›®å½•çš„çˆ¶ç›®å½•ä½œä¸ºcontentæ ¹ç›®å½•
                content_root = file_path.parent.parent
                analyzer = ContentAnalyzer(str(content_root))
                analysis = analyzer.analyze_file(file_path, args.ai_enhance)
                print_analysis_result(analysis)
            else:
                print(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path} (åŸå§‹è·¯å¾„: {args.analyze_single})")
        else:
            # åˆ†ææ•´ä¸ªç›®å½•
            analyzer = ContentAnalyzer(args.input_dir)
            analyzer.analyze_directory(args.ai_enhance)

            # æ ¹æ®å‚æ•°ç”Ÿæˆç›¸åº”æ ¼å¼
            if args.json_data:
                analyzer.generate_json_data(args.json_output, args.ai_enhance)
            else:
                analyzer.generate_report(args.output_file, args.ai_enhance)

    except Exception as e:
        print(f"é”™è¯¯: {e}")
        sys.exit(1)

if __name__ == '__main__':
    main()
