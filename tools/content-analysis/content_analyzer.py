#!/usr/bin/env python3
"""
Hugo Blog Content Analyzer

åˆ†æHugoåšå®¢å†…å®¹è´¨é‡å’ŒSEOä¼˜åŒ–ï¼š
- å¯è¯»æ€§è¯„åˆ† (Flesch-Kincaid, SMOG, Coleman-Liau)
- å…³é”®è¯æå–å’Œåˆ†æ
- SEOä¼˜åŒ–å»ºè®®
- å†…å®¹è´¨é‡è¯„ä¼°
- ç»“æ„åˆ†æ

ä½¿ç”¨æ–¹æ³•ï¼š
python content_analyzer.py [é€‰é¡¹]

é€‰é¡¹ï¼š
--input-dir DIR    è¾“å…¥ç›®å½•ï¼ˆé»˜è®¤: content/ï¼‰
--output-file FILE è¾“å‡ºæ–‡ä»¶ï¼ˆé»˜è®¤: content-analysis-report.mdï¼‰
--analyze-single FILE åˆ†æå•ä¸ªæ–‡ä»¶
--keywords         å¯ç”¨å…³é”®è¯æå–
--seo-check        å¯ç”¨SEOæ£€æŸ¥
--readability      å¯ç”¨å¯è¯»æ€§åˆ†æ
--all             æ‰§è¡Œæ‰€æœ‰åˆ†æï¼ˆé»˜è®¤ï¼‰
"""

import os
import sys
import re
import json
import statistics
import argparse
from pathlib import Path
from typing import Dict, List, Any, Tuple, Optional
from collections import Counter
import math
from datetime import datetime

# ä¸­æ–‡åœç”¨è¯åˆ—è¡¨
STOP_WORDS = {
    'çš„', 'äº†', 'å’Œ', 'æ˜¯', 'å°±', 'éƒ½', 'è€Œ', 'åŠ', 'ä¸', 'ç€', 'æˆ–', 'ä¸€ä¸ª', 'æ²¡æœ‰', 'æˆ‘ä»¬', 'ä½ ä»¬', 'ä»–ä»¬',
    'è¿™ä¸ª', 'é‚£ä¸ª', 'è¿™äº›', 'é‚£äº›', 'è¿™é‡Œ', 'é‚£é‡Œ', 'ä»€ä¹ˆ', 'æ€ä¹ˆ', 'ä¸ºä»€ä¹ˆ', 'æ€ä¹ˆæ ·', 'å¯ä»¥', 'èƒ½å¤Ÿ',
    'å¦‚æœ', 'å› ä¸º', 'æ‰€ä»¥', 'ä½†æ˜¯', 'ä¸è¿‡', 'è™½ç„¶', 'ä½†æ˜¯', 'è€Œä¸”', 'å› æ­¤', 'ç„¶å', 'å°±æ˜¯', 'å°±æ˜¯',
    'å°±æ˜¯è¯´', 'ä¹Ÿå°±æ˜¯è¯´', 'æ¯”å¦‚', 'ä¾‹å¦‚', 'ä¸€èˆ¬', 'é€šå¸¸', 'å¯èƒ½', 'ä¹Ÿè®¸', 'å¤§æ¦‚', 'å¤§çº¦', 'å·¦å³',
    'ç°åœ¨', 'ä»Šå¤©', 'æ˜¨å¤©', 'æ˜å¤©', 'æ—¶é—´', 'æ—¶å€™', 'ç›®å‰', 'æœ€è¿‘', 'å·²ç»', 'è¿˜æ˜¯', 'è¿˜æ˜¯', 'è¿˜æ˜¯',
    'è¿˜æ˜¯', 'è¿˜æ˜¯', 'è¿˜æ˜¯', 'è¿˜æ˜¯', 'è¿˜æ˜¯', 'è¿˜æ˜¯', 'è¿˜æ˜¯', 'è¿˜æ˜¯', 'è¿˜æ˜¯', 'è¿˜æ˜¯', 'è¿˜æ˜¯', 'è¿˜æ˜¯'
}

class ContentAnalyzer:
    """Hugoåšå®¢å†…å®¹åˆ†æå™¨"""

    def __init__(self, input_dir: str = "content"):
        self.input_dir = Path(input_dir)
        self.analysis_results = {}

        if not self.input_dir.exists():
            raise FileNotFoundError(f"å†…å®¹ç›®å½•ä¸å­˜åœ¨: {self.input_dir}")

    def extract_frontmatter(self, content: str) -> Dict[str, Any]:
        """æå–Hugo frontmatter"""
        frontmatter = {}
        lines = content.split('\n')

        if not lines[0].strip() == '---':
            return frontmatter

        # æŸ¥æ‰¾frontmatterç»“æŸæ ‡è®°
        end_idx = -1
        for i, line in enumerate(lines[1:], 1):
            if line.strip() == '---':
                end_idx = i
                break

        if end_idx == -1:
            return frontmatter

        # è§£æfrontmatter
        frontmatter_lines = lines[1:end_idx]
        frontmatter_text = '\n'.join(frontmatter_lines)

        # ç®€å•è§£æYAMLæ ¼å¼
        for line in frontmatter_lines:
            line = line.strip()
            if ':' in line and not line.startswith('#'):
                key, value = line.split(':', 1)
                key = key.strip()
                value = value.strip().strip('"').strip("'")

                # å¤„ç†æ•°ç»„
                if value.startswith('[') and value.endswith(']'):
                    value = [item.strip().strip('"').strip("'") for item in value[1:-1].split(',') if item.strip()]

                frontmatter[key] = value

        return frontmatter

    def extract_content_body(self, content: str) -> str:
        """æå–å†…å®¹ä¸»ä½“ï¼ˆå»é™¤frontmatterï¼‰"""
        lines = content.split('\n')

        if not lines[0].strip() == '---':
            return content

        # æŸ¥æ‰¾frontmatterç»“æŸæ ‡è®°
        end_idx = -1
        for i, line in enumerate(lines[1:], 1):
            if line.strip() == '---':
                end_idx = i + 1
                break

        if end_idx == -1:
            return content

        return '\n'.join(lines[end_idx:])

    def clean_markdown(self, text: str) -> str:
        """æ¸…ç†Markdownæ ‡è®°ï¼Œè¿”å›çº¯æ–‡æœ¬"""
        # ç§»é™¤frontmatter
        text = self.extract_content_body(text)

        # ç§»é™¤ä»£ç å—
        text = re.sub(r'```[\s\S]*?```', '', text)
        text = re.sub(r'`[^`]*`', '', text)

        # ç§»é™¤é“¾æ¥
        text = re.sub(r'\[([^\]]*)\]\([^\)]*\)', r'\1', text)
        text = re.sub(r'https?://[^\s]+', '', text)

        # ç§»é™¤å›¾ç‰‡
        text = re.sub(r'!\[([^\]]*)\]\([^\)]*\)', '', text)

        # ç§»é™¤æ ‡é¢˜æ ‡è®°
        text = re.sub(r'^#+\s*', '', text, flags=re.MULTILINE)

        # ç§»é™¤åˆ—è¡¨æ ‡è®°
        text = re.sub(r'^[\s]*[-\*\+]\s+', '', text, flags=re.MULTILINE)
        text = re.sub(r'^\s*\d+\.\s+', '', text, flags=re.MULTILINE)

        # ç§»é™¤ç²—ä½“å’Œæ–œä½“
        text = re.sub(r'\*\*([^\*]*)\*\*', r'\1', text)
        text = re.sub(r'\*([^\*]*)\*', r'\1', text)
        text = re.sub(r'_([^_]*)_', r'\1', text)

        # ç§»é™¤å¤šä½™ç©ºç™½
        text = re.sub(r'\s+', ' ', text)
        text = text.strip()

        return text

    def calculate_readability_scores(self, text: str) -> Dict[str, float]:
        """è®¡ç®—å¯è¯»æ€§åˆ†æ•°"""
        if not text:
            return {'flesch_kincaid': 0, 'smog': 0, 'coleman_liau': 0, 'avg_score': 0}

        # åŸºç¡€ç»Ÿè®¡
        sentences = re.split(r'[.!?]+', text)
        sentences = [s.strip() for s in sentences if s.strip()]
        words = re.findall(r'\b\w+\b', text)
        chars = len(text.replace(' ', ''))

        total_sentences = len(sentences)
        total_words = len(words)
        total_chars = chars

        if total_sentences == 0 or total_words == 0:
            return {'flesch_kincaid': 0, 'smog': 0, 'coleman_liau': 0, 'avg_score': 0}

        # Flesch-Kincaid Grade Level
        avg_words_per_sentence = total_words / total_sentences
        avg_syllables_per_word = sum(self.count_syllables(word) for word in words) / total_words
        flesch_kincaid = 0.39 * avg_words_per_sentence + 11.8 * avg_syllables_per_word - 15.59

        # SMOG Index
        complex_words = sum(1 for word in words if self.count_syllables(word) >= 3)
        smog = 1.043 * math.sqrt(complex_words * (30 / total_sentences)) + 3.1291

        # Coleman-Liau Index
        avg_chars_per_word = total_chars / total_words
        avg_sentences_per_100_words = (total_sentences / total_words) * 100
        coleman_liau = 0.0588 * avg_chars_per_word - 0.296 * avg_sentences_per_100_words - 15.8

        # è®¡ç®—å¹³å‡åˆ†æ•°
        avg_score = (flesch_kincaid + smog + coleman_liau) / 3

        return {
            'flesch_kincaid': round(flesch_kincaid, 2),
            'smog': round(smog, 2),
            'coleman_liau': round(coleman_liau, 2),
            'avg_score': round(avg_score, 2),
            'stats': {
                'sentences': total_sentences,
                'words': total_words,
                'chars': total_chars,
                'avg_words_per_sentence': round(avg_words_per_sentence, 1),
                'avg_chars_per_word': round(avg_chars_per_word, 1)
            }
        }

    def count_syllables(self, word: str) -> int:
        """è®¡ç®—å•è¯éŸ³èŠ‚æ•°ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        word = word.lower()
        count = 0
        vowels = "aeiouy"

        if word[0] in vowels:
            count += 1

        for i in range(1, len(word)):
            if word[i] in vowels and word[i - 1] not in vowels:
                count += 1

        if word.endswith("e"):
            count -= 1

        if count == 0:
            count += 1

        return count

    def extract_keywords(self, text: str, top_n: int = 10) -> List[Tuple[str, int]]:
        """æå–å…³é”®è¯"""
        # æ¸…ç†æ–‡æœ¬
        clean_text = self.clean_markdown(text).lower()

        # åˆ†è¯ï¼ˆç®€å•æŒ‰ç©ºæ ¼å’Œæ ‡ç‚¹åˆ†å‰²ï¼‰
        words = re.findall(r'\b\w+\b', clean_text)

        # è¿‡æ»¤åœç”¨è¯å’ŒçŸ­è¯
        filtered_words = [
            word for word in words
            if len(word) > 2 and word not in STOP_WORDS and not word.isdigit()
        ]

        # ç»Ÿè®¡è¯é¢‘
        word_freq = Counter(filtered_words)

        # è¿”å›æœ€å¸¸è§çš„å…³é”®è¯
        return word_freq.most_common(top_n)

    def analyze_seo(self, content: str, filename: str) -> Dict[str, Any]:
        """åˆ†æSEOç›¸å…³æŒ‡æ ‡"""
        frontmatter = self.extract_frontmatter(content)
        body = self.extract_content_body(content)

        seo_analysis = {
            'title': frontmatter.get('title', ''),
            'description': frontmatter.get('description', ''),
            'slug': filename.replace('.md', ''),
            'issues': [],
            'suggestions': []
        }

        # æ£€æŸ¥æ ‡é¢˜
        if not seo_analysis['title']:
            seo_analysis['issues'].append('ç¼ºå°‘æ ‡é¢˜ (title)')
        elif len(seo_analysis['title']) < 30:
            seo_analysis['issues'].append('æ ‡é¢˜è¿‡çŸ­ (å»ºè®®30-60å­—ç¬¦)')
        elif len(seo_analysis['title']) > 60:
            seo_analysis['issues'].append('æ ‡é¢˜è¿‡é•¿ (å»ºè®®30-60å­—ç¬¦)')

        # æ£€æŸ¥æè¿°
        if not seo_analysis['description']:
            seo_analysis['issues'].append('ç¼ºå°‘æè¿° (description)')
        elif len(seo_analysis['description']) < 120:
            seo_analysis['issues'].append('æè¿°è¿‡çŸ­ (å»ºè®®120-160å­—ç¬¦)')
        elif len(seo_analysis['description']) > 160:
            seo_analysis['issues'].append('æè¿°è¿‡é•¿ (å»ºè®®120-160å­—ç¬¦)')

        # æ£€æŸ¥å…³é”®è¯å¯†åº¦
        clean_text = self.clean_markdown(body)
        words = re.findall(r'\b\w+\b', clean_text.lower())
        total_words = len(words)

        if total_words > 0:
            # è®¡ç®—å…³é”®è¯å¯†åº¦
            keywords = self.extract_keywords(body, 5)
            for keyword, count in keywords:
                density = (count / total_words) * 100
                if density > 5:
                    seo_analysis['issues'].append(f'å…³é”®è¯"{keyword}"å¯†åº¦è¿‡é«˜ ({density:.1f}%)')
                elif density < 0.5:
                    seo_analysis['suggestions'].append(f'è€ƒè™‘å¢åŠ å…³é”®è¯"{keyword}"çš„ä½¿ç”¨')

        # æ£€æŸ¥URLç»“æ„
        if len(seo_analysis['slug']) > 100:
            seo_analysis['issues'].append('URLè¿‡é•¿')

        # æ£€æŸ¥frontmatter
        if 'date' not in frontmatter:
            seo_analysis['issues'].append('ç¼ºå°‘å‘å¸ƒæ—¥æœŸ (date)')
        if 'draft' not in frontmatter or frontmatter['draft']:
            seo_analysis['issues'].append('æ–‡ç« ä»ä¸ºè‰ç¨¿çŠ¶æ€')

        return seo_analysis

    def analyze_content_quality(self, content: str) -> Dict[str, Any]:
        """åˆ†æå†…å®¹è´¨é‡"""
        frontmatter = self.extract_frontmatter(content)
        body = self.extract_content_body(content)

        quality = {
            'score': 0,
            'max_score': 100,
            'checks': {},
            'issues': [],
            'strengths': []
        }

        # æ£€æŸ¥frontmatterå®Œæ•´æ€§ (20åˆ†)
        fm_score = 0
        required_fields = ['title', 'date', 'description']
        for field in required_fields:
            if field in frontmatter:
                fm_score += 7  # 20/3 â‰ˆ 6.67

        quality['checks']['frontmatter'] = fm_score
        if fm_score >= 15:
            quality['strengths'].append('Frontmatterä¿¡æ¯å®Œæ•´')
        else:
            quality['issues'].append('Frontmatterä¿¡æ¯ä¸å®Œæ•´')

        # æ£€æŸ¥å†…å®¹é•¿åº¦ (25åˆ†)
        clean_text = self.clean_markdown(body)
        word_count = len(re.findall(r'\b\w+\b', clean_text))

        if word_count >= 1000:
            content_score = 25
            quality['strengths'].append('å†…å®¹ä¸°å¯Œè¯¦ç»†')
        elif word_count >= 500:
            content_score = 20
            quality['strengths'].append('å†…å®¹é€‚ä¸­')
        elif word_count >= 200:
            content_score = 15
        else:
            content_score = 5
            quality['issues'].append('å†…å®¹è¿‡çŸ­')

        quality['checks']['content_length'] = content_score

        # æ£€æŸ¥ç»“æ„ (20åˆ†)
        headings = re.findall(r'^#{1,6}\s+', body, re.MULTILINE)
        lists = len(re.findall(r'^[\s]*[-\*\+]\s+', body, re.MULTILINE))
        code_blocks = len(re.findall(r'```', body))

        structure_score = min(20, len(headings) * 2 + lists + code_blocks * 3)
        quality['checks']['structure'] = structure_score

        if structure_score >= 15:
            quality['strengths'].append('ç»“æ„å±‚æ¬¡æ¸…æ™°')
        elif structure_score < 5:
            quality['issues'].append('ç»“æ„å±‚æ¬¡ä¸è¶³')

        # æ£€æŸ¥å¯è¯»æ€§ (20åˆ†)
        readability = self.calculate_readability_scores(clean_text)
        avg_score = readability.get('avg_score', 0)

        if 6 <= avg_score <= 8:
            readability_score = 20
            quality['strengths'].append('å¯è¯»æ€§é€‚ä¸­')
        elif 4 <= avg_score <= 10:
            readability_score = 15
        elif 2 <= avg_score <= 12:
            readability_score = 10
        else:
            readability_score = 5
            quality['issues'].append('å¯è¯»æ€§éœ€è¦æ”¹è¿›')

        quality['checks']['readability'] = readability_score

        # æ£€æŸ¥å…³é”®è¯ä½¿ç”¨ (15åˆ†)
        keywords = self.extract_keywords(body, 3)
        if keywords and keywords[0][1] >= 3:
            keyword_score = 15
            quality['strengths'].append('å…³é”®è¯ä½¿ç”¨å¾—å½“')
        elif keywords:
            keyword_score = 10
        else:
            keyword_score = 0
            quality['issues'].append('ç¼ºå°‘æ ¸å¿ƒå…³é”®è¯')

        quality['checks']['keywords'] = keyword_score

        # è®¡ç®—æ€»åˆ†
        quality['score'] = sum(quality['checks'].values())

        return quality

    def analyze_file(self, file_path: Path) -> Dict[str, Any]:
        """åˆ†æå•ä¸ªæ–‡ä»¶"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()

            analysis = {
                'file': str(file_path.relative_to(self.input_dir)),
                'frontmatter': self.extract_frontmatter(content),
                'stats': {},
                'readability': {},
                'keywords': [],
                'seo': {},
                'quality': {}
            }

            # åŸºç¡€ç»Ÿè®¡
            body = self.extract_content_body(content)
            clean_text = self.clean_markdown(content)

            analysis['stats'] = {
                'total_chars': len(content),
                'body_chars': len(body),
                'clean_chars': len(clean_text),
                'words': len(re.findall(r'\b\w+\b', clean_text)),
                'sentences': len(re.split(r'[.!?]+', clean_text)),
                'paragraphs': len([p for p in body.split('\n\n') if p.strip()]),
                'headings': len(re.findall(r'^#{1,6}\s+', body, re.MULTILINE)),
                'links': len(re.findall(r'\[([^\]]*)\]\([^\)]*\)', body)),
                'images': len(re.findall(r'!\[([^\]]*)\]\([^\)]*\)', body)),
                'code_blocks': len(re.findall(r'```', body))
            }

            # å¯è¯»æ€§åˆ†æ
            analysis['readability'] = self.calculate_readability_scores(clean_text)

            # å…³é”®è¯æå–
            analysis['keywords'] = self.extract_keywords(content, 10)

            # SEOåˆ†æ
            analysis['seo'] = self.analyze_seo(content, file_path.name)

            # è´¨é‡åˆ†æ
            analysis['quality'] = self.analyze_content_quality(content)

            return analysis

        except Exception as e:
            return {
                'file': str(file_path.relative_to(self.input_dir)),
                'error': str(e)
            }

    def analyze_directory(self) -> Dict[str, Any]:
        """åˆ†ææ•´ä¸ªç›®å½•"""
        results = {
            'summary': {
                'total_files': 0,
                'analyzed_files': 0,
                'error_files': 0,
                'avg_quality_score': 0,
                'avg_readability': 0,
                'total_words': 0
            },
            'files': [],
            'quality_distribution': {},
            'readability_distribution': {},
            'top_keywords': [],
            'seo_issues': []
        }

        all_keywords = Counter()
        quality_scores = []
        readability_scores = []
        seo_issues = []

        # åˆ†ææ‰€æœ‰Markdownæ–‡ä»¶
        for md_file in self.input_dir.rglob('*.md'):
            results['summary']['total_files'] += 1

            analysis = self.analyze_file(md_file)

            if 'error' in analysis:
                results['summary']['error_files'] += 1
            else:
                results['summary']['analyzed_files'] += 1
                results['files'].append(analysis)

                # æ”¶é›†ç»Ÿè®¡æ•°æ®
                quality_scores.append(analysis['quality']['score'])
                readability_scores.append(analysis['readability']['avg_score'])
                results['summary']['total_words'] += analysis['stats']['words']

                # æ”¶é›†å…³é”®è¯
                for keyword, count in analysis['keywords']:
                    all_keywords[keyword] += count

                # æ”¶é›†SEOé—®é¢˜
                seo_issues.extend(analysis['seo']['issues'])

        # è®¡ç®—æ±‡æ€»ç»Ÿè®¡
        if quality_scores:
            results['summary']['avg_quality_score'] = round(statistics.mean(quality_scores), 1)

        if readability_scores:
            results['summary']['avg_readability'] = round(statistics.mean(readability_scores), 1)

        # è´¨é‡åˆ†å¸ƒ
        for score in quality_scores:
            level = 'ä¼˜ç§€' if score >= 80 else 'è‰¯å¥½' if score >= 60 else 'ä¸€èˆ¬' if score >= 40 else 'éœ€æ”¹è¿›'
            results['quality_distribution'][level] = results['quality_distribution'].get(level, 0) + 1

        # å¯è¯»æ€§åˆ†å¸ƒ
        for score in readability_scores:
            level = 'å®¹æ˜“' if score <= 6 else 'é€‚ä¸­' if score <= 8 else 'è¾ƒéš¾' if score <= 10 else 'å›°éš¾'
            results['readability_distribution'][level] = results['readability_distribution'].get(level, 0) + 1

        # çƒ­é—¨å…³é”®è¯
        results['top_keywords'] = all_keywords.most_common(20)

        # SEOé—®é¢˜æ±‡æ€»
        results['seo_issues'] = list(set(seo_issues))

        self.analysis_results = results
        return results

    def generate_report(self, output_file: str = "content-analysis-report.md"):
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        if not self.analysis_results:
            self.analyze_directory()

        results = self.analysis_results

        report = f"""# Hugoåšå®¢å†…å®¹åˆ†ææŠ¥å‘Š

ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## ğŸ“Š æ€»ä½“ç»Ÿè®¡

- **æ€»æ–‡ä»¶æ•°**: {results['summary']['total_files']}
- **æˆåŠŸåˆ†æ**: {results['summary']['analyzed_files']}
- **åˆ†æå¤±è´¥**: {results['summary']['error_files']}
- **æ€»å­—æ•°**: {results['summary']['total_words']:,}
- **å¹³å‡è´¨é‡åˆ†æ•°**: {results['summary']['avg_quality_score']}/100
- **å¹³å‡å¯è¯»æ€§**: {results['summary']['avg_readability']}

## ğŸ“ˆ è´¨é‡åˆ†å¸ƒ

"""

        for level, count in results['quality_distribution'].items():
            percentage = (count / results['summary']['analyzed_files']) * 100
            report += f"- **{level}**: {count} ç¯‡ ({percentage:.1f}%)\n"

        report += "\n## ğŸ“– å¯è¯»æ€§åˆ†å¸ƒ\n\n"

        for level, count in results['readability_distribution'].items():
            percentage = (count / results['summary']['analyzed_files']) * 100
            report += f"- **{level}**: {count} ç¯‡ ({percentage:.1f}%)\n"

        report += "\n## ğŸ”¥ çƒ­é—¨å…³é”®è¯\n\n"

        for keyword, count in results['top_keywords'][:10]:
            report += f"- **{keyword}**: {count} æ¬¡\n"

        report += "\n## âš ï¸ SEOé—®é¢˜æ±‡æ€»\n\n"

        if results['seo_issues']:
            for issue in results['seo_issues']:
                report += f"- {issue}\n"
        else:
            report += "æš‚æ— æ˜æ˜¾SEOé—®é¢˜\n"

        report += "\n## ğŸ“‹ è¯¦ç»†æ–‡ä»¶åˆ†æ\n\n"

        for file_analysis in results['files'][:5]:  # åªæ˜¾ç¤ºå‰5ä¸ªæ–‡ä»¶çš„è¯¦ç»†åˆ†æ
            report += f"### {file_analysis['file']}\n\n"
            report += f"- **è´¨é‡åˆ†æ•°**: {file_analysis['quality']['score']}/100\n"
            report += f"- **å¯è¯»æ€§**: {file_analysis['readability']['avg_score']}\n"
            report += f"- **å­—æ•°**: {file_analysis['stats']['words']}\n"

            if file_analysis['seo']['issues']:
                report += f"- **SEOé—®é¢˜**: {', '.join(file_analysis['seo']['issues'][:3])}\n"

            if file_analysis['keywords']:
                top_keywords = [kw for kw, _ in file_analysis['keywords'][:3]]
                report += f"- **å…³é”®è¯**: {', '.join(top_keywords)}\n"

            report += "\n"

        # ä¿å­˜æŠ¥å‘Š
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(report)

        print(f"ğŸ“„ åˆ†ææŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_file}")
        return report

def main():
    parser = argparse.ArgumentParser(
        description="Hugoåšå®¢å†…å®¹åˆ†æå·¥å…·",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=__doc__
    )

    parser.add_argument(
        '--input-dir',
        default='content',
        help='è¾“å…¥ç›®å½• (é»˜è®¤: content)'
    )

    parser.add_argument(
        '--output-file',
        default='content-analysis-report.md',
        help='è¾“å‡ºæ–‡ä»¶ (é»˜è®¤: content-analysis-report.md)'
    )

    parser.add_argument(
        '--analyze-single',
        help='åˆ†æå•ä¸ªæ–‡ä»¶'
    )

    parser.add_argument(
        '--keywords',
        action='store_true',
        help='å¯ç”¨å…³é”®è¯æå–'
    )

    parser.add_argument(
        '--seo-check',
        action='store_true',
        help='å¯ç”¨SEOæ£€æŸ¥'
    )

    parser.add_argument(
        '--readability',
        action='store_true',
        help='å¯ç”¨å¯è¯»æ€§åˆ†æ'
    )

    parser.add_argument(
        '--all',
        action='store_true',
        help='æ‰§è¡Œæ‰€æœ‰åˆ†æ (é»˜è®¤)'
    )

    args = parser.parse_args()

    # å¦‚æœæ²¡æœ‰æŒ‡å®šä»»ä½•é€‰é¡¹ï¼Œé»˜è®¤æ‰§è¡Œæ‰€æœ‰åˆ†æ
    if not any([args.keywords, args.seo_check, args.readability]):
        args.all = True

    try:
        analyzer = ContentAnalyzer(args.input_dir)

        if args.analyze_single:
            # åˆ†æå•ä¸ªæ–‡ä»¶
            file_path = Path(args.analyze_single)
            if file_path.exists():
                analysis = analyzer.analyze_file(file_path)
                print(json.dumps(analysis, ensure_ascii=False, indent=2))
            else:
                print(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        else:
            # åˆ†ææ•´ä¸ªç›®å½•
            analyzer.analyze_directory()
            analyzer.generate_report(args.output_file)

    except Exception as e:
        print(f"é”™è¯¯: {e}")
        sys.exit(1)

if __name__ == '__main__':
    main()
