#!/usr/bin/env python3
"""
Hugo Blog RAG Chatbot Engine

åŸºäºRetrieval-Augmented Generationçš„æ™ºèƒ½é—®ç­”ç³»ç»Ÿï¼š
- è‡ªåŠ¨ç´¢å¼•Hugoåšå®¢å†…å®¹
- åŸºäºå‘é‡æ£€ç´¢çš„ç›¸å…³å†…å®¹æœç´¢
- ç»“åˆä¸Šä¸‹æ–‡çš„æ™ºèƒ½å›ç­”ç”Ÿæˆ
- æ”¯æŒå¤šè¯­è¨€å†…å®¹ç†è§£
- å®æ—¶å†…å®¹æ›´æ–°å’Œç´¢å¼•ç»´æŠ¤

ä½¿ç”¨æ–¹æ³•ï¼š
python rag_engine.py [é€‰é¡¹]

é€‰é¡¹ï¼š
--rebuild-index    é‡å»ºå‘é‡ç´¢å¼•
--query TEXT       æ‰§è¡ŒæŸ¥è¯¢å¹¶ç”Ÿæˆå›ç­”
--serve            å¯åŠ¨WebæœåŠ¡
--update-index     æ›´æ–°ç°æœ‰ç´¢å¼•ï¼ˆå¢é‡ï¼‰
--stats            æ˜¾ç¤ºç´¢å¼•ç»Ÿè®¡ä¿¡æ¯
"""

import os
import sys
import json
import time
import asyncio
import argparse
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime
import hashlib
import pickle

# RAGç›¸å…³ä¾èµ–
try:
    import numpy as np
    from sentence_transformers import SentenceTransformer
    import faiss
    import markdown
    from bs4 import BeautifulSoup
    import openai
    from openai import OpenAI
except ImportError as e:
    print(f"é”™è¯¯: ç¼ºå°‘å¿…è¦çš„ä¾èµ– - {e}")
    print("è¯·å®‰è£…: pip install numpy sentence-transformers faiss-cpu beautifulsoup4 openai markdown")
    sys.exit(1)

class HugoRAGEngine:
    """Hugoåšå®¢RAGå¼•æ“"""

    def __init__(self,
                 content_dir: str = "content",
                 index_dir: str = "rag-index",
                 embedding_model: str = "all-MiniLM-L6-v2",
                 openai_api_key: Optional[str] = None,
                 max_docs: int = 100):  # é™åˆ¶æœ€å¤§æ–‡æ¡£æ•°é‡
        self.content_dir = Path(content_dir)
        self.index_dir = Path(index_dir)
        self.index_dir.mkdir(parents=True, exist_ok=True)

        # æ¨¡å‹å’ŒAPIé…ç½®
        self.embedding_model_name = embedding_model
        self.embedding_model = None
        self.openai_client = None

        if openai_api_key:
            self.openai_client = OpenAI(api_key=openai_api_key)
        else:
            # å°è¯•ä»ç¯å¢ƒå˜é‡è·å–
            api_key = os.getenv('OPENAI_API_KEY')
            if api_key:
                self.openai_client = OpenAI(api_key=api_key)

        # ç´¢å¼•æ–‡ä»¶è·¯å¾„
        self.index_file = self.index_dir / "faiss_index.idx"
        self.metadata_file = self.index_dir / "metadata.pkl"
        self.content_file = self.index_dir / "content.pkl"

        # ç´¢å¼•æ•°æ®
        self.index = None
        self.metadata = []
        self.content_chunks = []

        # é…ç½®
        self.chunk_size = 800  # æ–‡æœ¬å—å¤§å° (å‡å°‘ä»¥èŠ‚çœå†…å­˜)
        self.chunk_overlap = 150  # å—é‡å å¤§å° (å‡å°‘é‡å )
        self.max_docs = max_docs  # æœ€å¤§æ–‡æ¡£æ•°é‡é™åˆ¶

    def load_embedding_model(self):
        """åŠ è½½åµŒå…¥æ¨¡å‹"""
        if self.embedding_model is None:
            print("ğŸ”„ åŠ è½½åµŒå…¥æ¨¡å‹...")
            self.embedding_model = SentenceTransformer(self.embedding_model_name)
            print("âœ… åµŒå…¥æ¨¡å‹åŠ è½½å®Œæˆ")

    def extract_frontmatter(self, content: str) -> Dict[str, Any]:
        """æå–Hugo frontmatter"""
        frontmatter = {}
        lines = content.split('\n')

        if not lines[0].strip() == '---':
            return frontmatter

        end_idx = -1
        for i, line in enumerate(lines[1:], 1):
            if line.strip() == '---':
                end_idx = i
                break

        if end_idx == -1:
            return frontmatter

        try:
            frontmatter_text = '\n'.join(lines[1:end_idx])
            # ç®€å•YAMLè§£æ
            for line in frontmatter_text.split('\n'):
                line = line.strip()
                if ':' in line and not line.startswith('#'):
                    key, value = line.split(':', 1)
                    key = key.strip()
                    value = value.strip().strip('"').strip("'")
                    frontmatter[key] = value
        except:
            pass

        return frontmatter

    def clean_markdown(self, text: str) -> str:
        """æ¸…ç†Markdownæ ‡è®°"""
        # ç§»é™¤frontmatter
        lines = text.split('\n')
        if lines and lines[0].strip() == '---':
            for i, line in enumerate(lines[1:], 1):
                if line.strip() == '---':
                    text = '\n'.join(lines[i+1:])
                    break

        # è½¬æ¢ä¸ºHTMLç„¶åæå–çº¯æ–‡æœ¬
        html = markdown.markdown(text)
        soup = BeautifulSoup(html, 'html.parser')

        # ç§»é™¤ä»£ç å—
        for code in soup.find_all(['code', 'pre']):
            code.decompose()

        # ç§»é™¤é“¾æ¥ä½†ä¿ç•™æ–‡æœ¬
        for a in soup.find_all('a'):
            a.replace_with(a.get_text())

        # ç§»é™¤å›¾ç‰‡
        for img in soup.find_all('img'):
            img.decompose()

        return soup.get_text()

    def split_text(self, text: str) -> List[str]:
        """å°†æ–‡æœ¬åˆ†å‰²æˆå—"""
        chunks = []
        start = 0

        while start < len(text):
            end = start + self.chunk_size

            # å°è¯•åœ¨å¥å­è¾¹ç•Œç»“æŸ
            if end < len(text):
                # æŸ¥æ‰¾æœ€è¿‘çš„å¥å­ç»“æŸç¬¦
                for punct in ['. ', '! ', '? ', '\n\n']:
                    last_punct = text.rfind(punct, start, end)
                    if last_punct != -1:
                        end = last_punct + len(punct)
                        break

            chunk = text[start:end].strip()
            if chunk:
                chunks.append(chunk)

            # ç§»åŠ¨åˆ°ä¸‹ä¸€ä¸ªä½ç½®ï¼Œè€ƒè™‘é‡å 
            start = end - self.chunk_overlap

        return chunks

    def process_markdown_file(self, file_path: Path) -> List[Dict[str, Any]]:
        """å¤„ç†å•ä¸ªMarkdownæ–‡ä»¶"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()

            frontmatter = self.extract_frontmatter(content)
            clean_text = self.clean_markdown(content)

            if not clean_text.strip():
                return []

            # åˆ†å‰²æ–‡æœ¬
            chunks = self.split_text(clean_text)

            # ä¸ºæ¯ä¸ªå—åˆ›å»ºå…ƒæ•°æ®
            documents = []
            for i, chunk in enumerate(chunks):
                doc = {
                    'file_path': str(file_path.relative_to(self.content_dir)),
                    'chunk_id': i,
                    'title': frontmatter.get('title', file_path.stem),
                    'date': frontmatter.get('date', ''),
                    'tags': frontmatter.get('tags', []),
                    'categories': frontmatter.get('categories', []),
                    'content': chunk,
                    'url': self.generate_url(file_path, frontmatter),
                    'hash': hashlib.md5(chunk.encode()).hexdigest()[:8]
                }
                documents.append(doc)

            return documents

        except Exception as e:
            print(f"âš ï¸  å¤„ç†æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
            return []

    def generate_url(self, file_path: Path, frontmatter: Dict) -> str:
        """ç”Ÿæˆæ–‡ç« URL"""
        rel_path = file_path.relative_to(self.content_dir)
        url_path = str(rel_path).replace('.md', '/').replace('\\', '/')

        # ç§»é™¤å¤šä½™çš„è·¯å¾„å‰ç¼€
        if url_path.startswith('./'):
            url_path = url_path[2:]

        return f"/{url_path}"

    def build_index(self, force_rebuild: bool = False):
        """æ„å»ºå‘é‡ç´¢å¼•"""
        print("ğŸ—ï¸  æ„å»ºå‘é‡ç´¢å¼•...")

        self.load_embedding_model()

        # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡å»º
        if self.index_file.exists() and not force_rebuild:
            print("ğŸ“‚ å‘ç°ç°æœ‰ç´¢å¼•ï¼Œä½¿ç”¨ --rebuild-index å¼ºåˆ¶é‡å»º")
            self.load_index()
            return

        all_documents = []

        # å¤„ç†æ‰€æœ‰Markdownæ–‡ä»¶
        md_files = list(self.content_dir.rglob('*.md'))
        print(f"ğŸ“š å‘ç° {len(md_files)} ä¸ªMarkdownæ–‡ä»¶")

        for i, md_file in enumerate(md_files):
            if md_file.name.startswith('_'):  # è·³è¿‡ç‰¹æ®Šæ–‡ä»¶
                continue

            if len(all_documents) >= self.max_docs:
                print(f"âš ï¸  å·²è¾¾åˆ°æœ€å¤§æ–‡æ¡£æ•°é‡é™åˆ¶ ({self.max_docs})ï¼Œåœæ­¢å¤„ç†")
                break

            print(f"ğŸ“„ å¤„ç†: {md_file.name} ({i+1}/{len(md_files)})")
            documents = self.process_markdown_file(md_file)
            all_documents.extend(documents)

            # æ˜¾ç¤ºè¿›åº¦
            if (i + 1) % 10 == 0:
                print(f"ğŸ“Š å·²å¤„ç† {len(all_documents)} ä¸ªæ–‡æ¡£å—...")

        if not all_documents:
            print("âš ï¸  æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆæ–‡æ¡£")
            return

        print(f"ğŸ“Š å¤„ç†äº† {len(all_documents)} ä¸ªæ–‡æ¡£å—")

        # ç”ŸæˆåµŒå…¥å‘é‡ (åˆ†æ‰¹å¤„ç†ä»¥å‡å°‘å†…å­˜ä½¿ç”¨)
        print("ğŸ§® ç”ŸæˆåµŒå…¥å‘é‡...")
        texts = [doc['content'] for doc in all_documents]
        batch_size = 32  # æ‰¹å¤„ç†å¤§å°
        embeddings = []

        for i in range(0, len(texts), batch_size):
            batch_texts = texts[i:i+batch_size]
            print(f"å¤„ç†æ‰¹æ¬¡ {i//batch_size + 1}/{(len(texts) + batch_size - 1)//batch_size} ({len(batch_texts)} ä¸ªæ–‡æ¡£)")
            batch_embeddings = self.embedding_model.encode(batch_texts, show_progress_bar=False)
            embeddings.append(batch_embeddings)

        embeddings = np.vstack(embeddings)

        # æ„å»ºFAISSç´¢å¼•
        print("ğŸ” æ„å»ºFAISSç´¢å¼•...")
        dimension = embeddings.shape[1]
        self.index = faiss.IndexFlatIP(dimension)  # å†…ç§¯ç´¢å¼•
        faiss.normalize_L2(embeddings)  # L2å½’ä¸€åŒ–
        self.index.add(embeddings)

        # ä¿å­˜ç´¢å¼•å’Œå…ƒæ•°æ®
        self.metadata = all_documents
        self.content_chunks = texts

        self.save_index()
        print("âœ… ç´¢å¼•æ„å»ºå®Œæˆ")
        print(f"ğŸ“ˆ ç´¢å¼•ç»Ÿè®¡: {len(all_documents)} ä¸ªæ–‡æ¡£å—ï¼Œç»´åº¦: {dimension}")

    def save_index(self):
        """ä¿å­˜ç´¢å¼•åˆ°ç£ç›˜"""
        print("ğŸ’¾ ä¿å­˜ç´¢å¼•...")

        # ä¿å­˜FAISSç´¢å¼•
        faiss.write_index(self.index, str(self.index_file))

        # ä¿å­˜å…ƒæ•°æ®
        with open(self.metadata_file, 'wb') as f:
            pickle.dump(self.metadata, f)

        # ä¿å­˜å†…å®¹å—
        with open(self.content_file, 'wb') as f:
            pickle.dump(self.content_chunks, f)

    def load_index(self):
        """ä»ç£ç›˜åŠ è½½ç´¢å¼•"""
        print("ğŸ“‚ åŠ è½½ç°æœ‰ç´¢å¼•...")

        if not self.index_file.exists():
            print("âš ï¸  ç´¢å¼•æ–‡ä»¶ä¸å­˜åœ¨")
            return

        try:
            # åŠ è½½FAISSç´¢å¼•
            self.index = faiss.read_index(str(self.index_file))

            # åŠ è½½å…ƒæ•°æ®
            with open(self.metadata_file, 'rb') as f:
                self.metadata = pickle.load(f)

            # åŠ è½½å†…å®¹å—
            with open(self.content_file, 'rb') as f:
                self.content_chunks = pickle.load(f)

            print(f"âœ… ç´¢å¼•åŠ è½½å®Œæˆ: {len(self.metadata)} ä¸ªæ–‡æ¡£å—")

        except Exception as e:
            print(f"âŒ åŠ è½½ç´¢å¼•å¤±è´¥: {e}")
            self.index = None
            self.metadata = []
            self.content_chunks = []

    def search(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:
        """æœç´¢ç›¸å…³æ–‡æ¡£"""
        if self.index is None:
            self.load_index()

        if self.index is None:
            print("âŒ ç´¢å¼•æœªåŠ è½½")
            return []

        self.load_embedding_model()

        # ç”ŸæˆæŸ¥è¯¢å‘é‡
        query_embedding = self.embedding_model.encode([query])[0]
        query_embedding = query_embedding / np.linalg.norm(query_embedding)  # å½’ä¸€åŒ–

        # æœç´¢æœ€ç›¸ä¼¼çš„æ–‡æ¡£
        query_embedding = query_embedding.reshape(1, -1)
        scores, indices = self.index.search(query_embedding, top_k)

        results = []
        for score, idx in zip(scores[0], indices[0]):
            if idx < len(self.metadata):
                result = self.metadata[idx].copy()
                result['score'] = float(score)
                results.append(result)

        return results

    def generate_answer(self, query: str, context_docs: List[Dict[str, Any]]) -> str:
        """åŸºäºæ£€ç´¢ç»“æœç”Ÿæˆå›ç­”"""
        if not self.openai_client:
            # å¦‚æœæ²¡æœ‰OpenAI APIï¼Œä½¿ç”¨ç®€å•çš„æ¨¡æ¿å›ç­”
            return self.generate_simple_answer(query, context_docs)

        try:
            # æ„å»ºä¸Šä¸‹æ–‡
            context = "\n\n".join([
                f"æ ‡é¢˜: {doc['title']}\nå†…å®¹: {doc['content'][:500]}..."
                for doc in context_docs[:3]  # åªä½¿ç”¨å‰3ä¸ªæœ€ç›¸å…³çš„æ–‡æ¡£
            ])

            prompt = f"""åŸºäºä»¥ä¸‹åšå®¢æ–‡ç« å†…å®¹ï¼Œå›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚è¯·æä¾›å‡†ç¡®ã€æœ‰å¸®åŠ©çš„å›ç­”ï¼Œå¹¶å¼•ç”¨ç›¸å…³ä¿¡æ¯æ¥æºã€‚

é—®é¢˜: {query}

ç›¸å…³æ–‡ç« å†…å®¹:
{context}

è¯·ç”¨ä¸­æ–‡å›ç­”ï¼Œå¹¶ä¿æŒç®€æ´æ˜äº†ã€‚"""

            response = self.openai_client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åšå®¢åŠ©æ‰‹ï¼ŒåŸºäºæä¾›çš„æ–‡ç« å†…å®¹å›ç­”é—®é¢˜ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=500,
                temperature=0.7
            )

            return response.choices[0].message.content.strip()

        except Exception as e:
            print(f"âŒ ç”Ÿæˆå›ç­”å¤±è´¥: {e}")
            return self.generate_simple_answer(query, context_docs)

    def generate_simple_answer(self, query: str, context_docs: List[Dict[str, Any]]) -> str:
        """ç”Ÿæˆç®€å•çš„å›ç­”ï¼ˆä¸ä½¿ç”¨AIï¼‰"""
        if not context_docs:
            return "æŠ±æ­‰ï¼Œæˆ‘æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚"

        # æ‰¾åˆ°æœ€ç›¸å…³çš„æ–‡æ¡£
        best_doc = context_docs[0]

        answer = f"æ ¹æ®ã€Š{best_doc['title']}ã€‹ä¸­çš„å†…å®¹ï¼š\n\n"
        answer += best_doc['content'][:300]
        if len(best_doc['content']) > 300:
            answer += "..."

        if best_doc['url']:
            answer += f"\n\næŸ¥çœ‹å®Œæ•´æ–‡ç« : {best_doc['url']}"

        return answer

    def query(self, query_text: str, top_k: int = 5) -> Dict[str, Any]:
        """æ‰§è¡Œå®Œæ•´æŸ¥è¯¢æµç¨‹"""
        print(f"ğŸ” æŸ¥è¯¢: {query_text}")

        # æœç´¢ç›¸å…³æ–‡æ¡£
        relevant_docs = self.search(query_text, top_k)

        if not relevant_docs:
            return {
                'query': query_text,
                'answer': "æŠ±æ­‰ï¼Œæˆ‘æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚",
                'sources': []
            }

        # ç”Ÿæˆå›ç­”
        answer = self.generate_answer(query_text, relevant_docs)

        # æ„å»ºç»“æœ
        sources = []
        for doc in relevant_docs[:3]:  # åªè¿”å›å‰3ä¸ªæ¥æº
            sources.append({
                'title': doc['title'],
                'url': doc['url'],
                'score': doc['score'],
                'snippet': doc['content'][:200] + "..."
            })

        return {
            'query': query_text,
            'answer': answer,
            'sources': sources,
            'timestamp': datetime.now().isoformat()
        }

    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç´¢å¼•ç»Ÿè®¡ä¿¡æ¯"""
        if self.index is None:
            self.load_index()

        stats = {
            'total_documents': len(self.metadata) if self.metadata else 0,
            'index_size': self.index.ntotal if self.index else 0,
            'dimension': self.index.d if self.index else 0,
            'last_updated': None,
            'content_stats': {}
        }

        if self.metadata:
            # ç»Ÿè®¡å†…å®¹ä¿¡æ¯
            total_words = sum(len(doc['content'].split()) for doc in self.metadata)
            stats['content_stats'] = {
                'total_words': total_words,
                'avg_chunk_size': total_words / len(self.metadata),
                'unique_files': len(set(doc['file_path'] for doc in self.metadata))
            }

            # æ£€æŸ¥æœ€åæ›´æ–°æ—¶é—´
            if self.index_file.exists():
                stats['last_updated'] = datetime.fromtimestamp(
                    self.index_file.stat().st_mtime
                ).isoformat()

        return stats

def main():
    parser = argparse.ArgumentParser(
        description="Hugoåšå®¢RAGèŠå¤©æœºå™¨äººå¼•æ“",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=__doc__
    )

    parser.add_argument(
        '--content-dir',
        default='content',
        help='å†…å®¹ç›®å½• (é»˜è®¤: content)'
    )

    parser.add_argument(
        '--index-dir',
        default='rag-index',
        help='ç´¢å¼•ç›®å½• (é»˜è®¤: rag-index)'
    )

    parser.add_argument(
        '--rebuild-index',
        action='store_true',
        help='é‡å»ºå‘é‡ç´¢å¼•'
    )

    parser.add_argument(
        '--update-index',
        action='store_true',
        help='æ›´æ–°ç°æœ‰ç´¢å¼•'
    )

    parser.add_argument(
        '--query',
        help='æ‰§è¡ŒæŸ¥è¯¢'
    )

    parser.add_argument(
        '--serve',
        action='store_true',
        help='å¯åŠ¨WebæœåŠ¡'
    )

    parser.add_argument(
        '--stats',
        action='store_true',
        help='æ˜¾ç¤ºç´¢å¼•ç»Ÿè®¡ä¿¡æ¯'
    )

    parser.add_argument(
        '--openai-api-key',
        help='OpenAI APIå¯†é’¥'
    )

    parser.add_argument(
        '--embedding-model',
        default='all-MiniLM-L6-v2',
        help='åµŒå…¥æ¨¡å‹åç§° (é»˜è®¤: all-MiniLM-L6-v2)'
    )

    parser.add_argument(
        '--max-docs',
        type=int,
        default=50,
        help='æœ€å¤§æ–‡æ¡£æ•°é‡é™åˆ¶ (é»˜è®¤: 50)'
    )

    args = parser.parse_args()

    # åˆå§‹åŒ–RAGå¼•æ“
    engine = HugoRAGEngine(
        content_dir=args.content_dir,
        index_dir=args.index_dir,
        embedding_model=args.embedding_model,
        openai_api_key=args.openai_api_key,
        max_docs=args.max_docs
    )

    try:
        if args.rebuild_index:
            # é‡å»ºç´¢å¼•
            engine.build_index(force_rebuild=True)

        elif args.update_index:
            # æ›´æ–°ç´¢å¼•
            engine.build_index(force_rebuild=False)

        elif args.query:
            # æ‰§è¡ŒæŸ¥è¯¢
            if not engine.index:
                engine.load_index()

            result = engine.query(args.query)
            print("\n" + "="*50)
            print("ğŸ¤– å›ç­”:")
            print(result['answer'])
            print("\nğŸ“š ç›¸å…³æ¥æº:")
            for i, source in enumerate(result['sources'], 1):
                print(f"{i}. {source['title']} (ç›¸å…³åº¦: {source['score']:.3f})")
                print(f"   {source['url']}")
            print("="*50)

        elif args.stats:
            # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
            stats = engine.get_stats()
            print("ğŸ“Š RAGç´¢å¼•ç»Ÿè®¡ä¿¡æ¯:")
            print(f"- æ–‡æ¡£å—æ•°é‡: {stats['total_documents']}")
            print(f"- å‘é‡ç»´åº¦: {stats['dimension']}")
            print(f"- æ€»è¯æ•°: {stats.get('content_stats', {}).get('total_words', 0)}")
            print(f"- å”¯ä¸€æ–‡ä»¶æ•°: {stats.get('content_stats', {}).get('unique_files', 0)}")
            if stats['last_updated']:
                print(f"- æœ€åæ›´æ–°: {stats['last_updated']}")

        elif args.serve:
            # å¯åŠ¨WebæœåŠ¡
            print("ğŸš€ å¯åŠ¨RAGèŠå¤©æœåŠ¡...")
            print("åŠŸèƒ½å¾…å®ç°ï¼Œè¯·ä½¿ç”¨ --query å‚æ•°è¿›è¡Œæµ‹è¯•")

        else:
            # é»˜è®¤æ“ä½œï¼šæ£€æŸ¥ç´¢å¼•çŠ¶æ€
            if engine.index_file.exists():
                print("âœ… RAGç´¢å¼•å·²å­˜åœ¨")
                engine.get_stats()
            else:
                print("âŒ RAGç´¢å¼•ä¸å­˜åœ¨ï¼Œè¯·è¿è¡Œ --rebuild-index æ„å»ºç´¢å¼•")

    except KeyboardInterrupt:
        print("\nâš ï¸  ç”¨æˆ·ä¸­æ–­æ“ä½œ")
    except Exception as e:
        print(f"âŒ æ“ä½œå¤±è´¥: {e}")
        sys.exit(1)

if __name__ == '__main__':
    main()
